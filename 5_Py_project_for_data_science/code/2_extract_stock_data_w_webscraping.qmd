# Extracting Stock Data Using Web Scraping

Not all stock data is available via API in this assignment; you will use web-scraping to obtain financial data. Using beautiful soup we will extract historical share data from a web-page.

## Prepare libs

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
```


## Extract the data

First we must use the request library to download the webpage, and extract the text. We will extract Netflix stock data https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html.

```{python}
#set url
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html"

#request data
data  = requests.get(url).text

#parse the data into text using beautiful soup
soup = BeautifulSoup(data, 'html5lib')
```

Turn the html table into a pandas df

```{python}
#create empty df
netflix_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])

# First we isolate the body of the table which contains all the information
# Then we loop through each row and find all the column values for each row
for row in soup.find("tbody").find_all('tr'):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text
    
    # Finally we append the data of each row to the table
    netflix_data = netflix_data.append({"Date":date, "Open":Open, "High":high, "Low":low, "Close":close, "Adj_Close":adj_close, "Volume":volume}, ignore_index=True)    

#print df
netflix_data.head()
```

We can also use the pandas read_html function using the url

```{python}
read_html_pandas_data = pd.read_html(url)
```

Or we can convert the BeautifulSoup object to a string

```{python}
read_html_pandas_data = pd.read_html(str(soup))
```

Beacause there is only one table on the page, we just take the first table in the list returned

```{python}
netflix_dataframe = read_html_pandas_data[0]
netflix_dataframe.head()
```

## Using Webscraping to Extract Stock Data Exercise

Use the requests library to download the webpage https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html. Save the text of the response as a variable named html_data.


```{python}
#set url
url2 = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html"

#request data
html_data = requests.get(url2).text
```

Parse the html data using beautiful_soup

```{python}
soup2 = BeautifulSoup(html_data, 'html5lib')
```

- What is the content of the title attribute:

```{python}
soup.find("title")
```

Using beautiful soup extract the table with historical share prices and store it into a dataframe named amazon_data. The dataframe should have columns Date, Open, High, Low, Close, Adj Close, and Volume. Fill in each variable with the correct data from the list col.

```{python}
amazon_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])

for row in soup.find("tbody").find_all("tr"):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text
    
    amazon_data = amazon_data.append({"Date":date, "Open":Open, "High":high, "Low":low, "Close":close, "Adj Close":adj_close, "Volume":volume}, ignore_index=True)

#Print out the first five rows of the `amazon_data` dataframe you created.
amazon_data.head(n=5)
```

- What is the name of the columns of the dataframe

```{python}
print(amazon_data.columns)
```

-  What is the Open of the last row of the amazon_data dataframe?

```{python}
amazon_data.tail()
amazon_data['Open'].iloc[-1]
```