{
  "hash": "950934fed874b04f535402ba7037c207",
  "result": {
    "markdown": "# Extracting Stock Data Using Web Scraping\n\nNot all stock data is available via API in this assignment; you will use web-scraping to obtain financial data. Using beautiful soup we will extract historical share data from a web-page.\n\n## Prepare libs\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n```\n:::\n\n\n## Extract the data\n\nFirst we must use the request library to download the webpage, and extract the text. We will extract Netflix stock data https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#set url\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\"\n\n#request data\ndata  = requests.get(url).text\n\n#parse the data into text using beautiful soup\nsoup = BeautifulSoup(data, 'html5lib')\n```\n:::\n\n\nTurn the html table into a pandas df\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#create empty df\nnetflix_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n\n# First we isolate the body of the table which contains all the information\n# Then we loop through each row and find all the column values for each row\nfor row in soup.find(\"tbody\").find_all('tr'):\n    col = row.find_all(\"td\")\n    date = col[0].text\n    Open = col[1].text\n    high = col[2].text\n    low = col[3].text\n    close = col[4].text\n    adj_close = col[5].text\n    volume = col[6].text\n    \n    # Finally we append the data of each row to the table\n    netflix_data = netflix_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj_Close\":adj_close, \"Volume\":volume}, ignore_index=True)    \n\n#print df\nnetflix_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=tex}\n\\begin{tabular}{llllllll}\n\\toprule\n{} &          Date &    Open &    High &     Low &   Close &       Volume & Adj\\_Close \\\\\n\\midrule\n0 &  Jun 01, 2021 &  504.01 &  536.13 &  482.14 &  528.21 &   78,560,600 &    528.21 \\\\\n1 &  May 01, 2021 &  512.65 &  518.95 &  478.54 &  502.81 &   66,927,600 &    502.81 \\\\\n2 &  Apr 01, 2021 &  529.93 &  563.56 &  499.00 &  513.47 &  111,573,300 &    513.47 \\\\\n3 &  Mar 01, 2021 &  545.57 &  556.99 &  492.85 &  521.66 &   90,183,900 &    521.66 \\\\\n4 &  Feb 01, 2021 &  536.79 &  566.65 &  518.28 &  538.85 &   61,902,300 &    538.85 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nWe can also use the pandas read_html function using the url\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nread_html_pandas_data = pd.read_html(url)\n```\n:::\n\n\nOr we can convert the BeautifulSoup object to a string\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nread_html_pandas_data = pd.read_html(str(soup))\n```\n:::\n\n\nBeacause there is only one table on the page, we just take the first table in the list returned\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnetflix_dataframe = read_html_pandas_data[0]\nnetflix_dataframe.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=tex}\n\\begin{tabular}{llllllll}\n\\toprule\n{} &          Date &    Open &    High &     Low &  Close* & Adj Close** &     Volume \\\\\n\\midrule\n0 &  Jun 01, 2021 &  504.01 &  536.13 &  482.14 &  528.21 &      528.21 &   78560600 \\\\\n1 &  May 01, 2021 &  512.65 &  518.95 &  478.54 &  502.81 &      502.81 &   66927600 \\\\\n2 &  Apr 01, 2021 &  529.93 &  563.56 &  499.00 &  513.47 &      513.47 &  111573300 \\\\\n3 &  Mar 01, 2021 &  545.57 &  556.99 &  492.85 &  521.66 &      521.66 &   90183900 \\\\\n4 &  Feb 01, 2021 &  536.79 &  566.65 &  518.28 &  538.85 &      538.85 &   61902300 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n## Using Webscraping to Extract Stock Data Exercise\n\nUse the requests library to download the webpage https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html. Save the text of the response as a variable named html_data.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#set url\nurl2 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n\n#request data\nhtml_data = requests.get(url2).text\n```\n:::\n\n\nParse the html data using beautiful_soup\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsoup2 = BeautifulSoup(html_data, 'html5lib')\n```\n:::\n\n\n- What is the content of the title attribute:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsoup.find(\"title\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<title>Netflix, Inc. (NFLX) Stock Historical Prices &amp; Data - Yahoo Finance</title>\n```\n:::\n:::\n\n\nUsing beautiful soup extract the table with historical share prices and store it into a dataframe named amazon_data. The dataframe should have columns Date, Open, High, Low, Close, Adj Close, and Volume. Fill in each variable with the correct data from the list col.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\namazon_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n\nfor row in soup.find(\"tbody\").find_all(\"tr\"):\n    col = row.find_all(\"td\")\n    date = col[0].text\n    Open = col[1].text\n    high = col[2].text\n    low = col[3].text\n    close = col[4].text\n    adj_close = col[5].text\n    volume = col[6].text\n    \n    amazon_data = amazon_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)\n\n#Print out the first five rows of the `amazon_data` dataframe you created.\namazon_data.head(n=5)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=tex}\n\\begin{tabular}{llllllll}\n\\toprule\n{} &          Date &    Open &    High &     Low &   Close &       Volume & Adj Close \\\\\n\\midrule\n0 &  Jun 01, 2021 &  504.01 &  536.13 &  482.14 &  528.21 &   78,560,600 &    528.21 \\\\\n1 &  May 01, 2021 &  512.65 &  518.95 &  478.54 &  502.81 &   66,927,600 &    502.81 \\\\\n2 &  Apr 01, 2021 &  529.93 &  563.56 &  499.00 &  513.47 &  111,573,300 &    513.47 \\\\\n3 &  Mar 01, 2021 &  545.57 &  556.99 &  492.85 &  521.66 &   90,183,900 &    521.66 \\\\\n4 &  Feb 01, 2021 &  536.79 &  566.65 &  518.28 &  538.85 &   61,902,300 &    538.85 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n- What is the name of the columns of the dataframe\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprint(amazon_data.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], dtype='object')\n```\n:::\n:::\n\n\n-  What is the Open of the last row of the amazon_data dataframe?\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\namazon_data.tail()\namazon_data['Open'].iloc[-1]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n'109.35'\n```\n:::\n:::\n\n\n",
    "supporting": [
      "2_extract_stock_data_w_webscraping_files"
    ],
    "filters": []
  }
}