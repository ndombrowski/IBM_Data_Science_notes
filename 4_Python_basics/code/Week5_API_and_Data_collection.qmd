# APIs and Data collection


```{python}
#|  echo: false

#ensure that plots get rendered, will give a warning, but nothing to worry about
import os
import sys
import matplotlib
import pandas as pd
import plotly
import plotly.graph_objects as go
from plotly.offline import iplot
import requests
from bs4 import BeautifulSoup
```

```{bash}
pwd
```

## Simple Application Programmming Interaces (APIs)

An API lets two pieces of software talk to each other For example you have your program, you have some data, you have other software components. You use the api to communicate with the api via inputs and outputs.

Just like a function, you don’t have to know how the API works, but just its inputs and outputs. Pandas is actually a set of software components, much of which are not even written in Python. You have some data. You have a set of software components. We use the pandas api to process the data by communicating with the other Software Components.

An example:

```{python}
dict_ = {"a":[11,21,31], "b":[12,22,23]}
df = pd.DataFrame(dict_)
print(df.head())
```


When you create a dictionary, and then create a pandas object with the Dataframe constructor, in API lingo, this is an “instance.” The data in the dictionary is passed along to the pandas API. You then use the dataframe to communicate with the API. When you call the method head, the dataframe communicates with the API displaying the first few rows of the dataframe.


## REST APIs

REST APIs are another popular type of API; they allow you to communicate through the internet allowing you to take advantage of resources like storage, access more data, artificial intelligent algorithms, and much more. 

The RE stands for Representational, the S stands for State, the T stand for Transfer. 

In rest API’s your program is called the client. The API communicates with a web service you call through the internet. There is a set of rules regarding Communication, Input or Request, and Output or Response.

You or your code can be thought of as a **client**. The web service is referred to as a **resource**. The client finds the service via an **endpoint**.  The client sends requests to the resource and the response to the client. 

HTTP methods are a way of transmitting data over the internet We tell the Rest API’s what to do by sending a request. The request is usually communicated via an HTTP message. The HTTP message usually contains a JSON file. This contains instructions for what operation we would like the service to perform. This operation is transmitted to the webservice via the internet. The service performs the operation.In the similar manner, the webservice returns a response via an HTTP message, where the information is usually returned via a JSON file. This information is transmitted back to the client.



## Pycoingeckp

Crypto Currency data is excellent to be used in an API because it is being constantly updated and it is vital to CryptoCurrency Trading We will use the Py-Coin-Gecko Python Client/Wrapper for the Coin Gecko API, updated every minute by Coin-Gecko We use the Wrapper/Client because it is easy to use so you can focus on the task of collecting data, we will also introduce pandas time series functions for dealing with time series data

```{python}
#import software
from pycoingecko import CoinGeckoAPI

#create a client
cg = CoinGeckoAPI()

#use a fct to request data
#getting data on bitcoin, in U.S. Dollars, for the past 30 days
#we get a JSON expressed as a python dictionary of nested lists
bitcoin_data = cg.get_coin_market_chart_by_id(id="bitcoin", vs_currency="usd",days=30)

#select only the prices
bitcoin_price_data = bitcoin_data['prices']

bitcoin_price_data = pd.DataFrame(bitcoin_price_data, columns=["TimeStamp", "Price"])
bitcoin_price_data.head()
```

Convert the timestamp to a more readable format usign the pythin function *to_datetime*

```{python}
#convert the timestamp
bitcoin_price_data['Date'] = pd.to_datetime(bitcoin_price_data['TimeStamp'], unit = 'ms')
bitcoin_price_data.head()

#an alternative way to do this
#data['date'] = data['TimeStamp'].apply(lambda d: datetime.date.fromtimestamp(d/1000.0))
```

Now, we want to create a candlestick plot. To get the data for the daily candlesticks we will group by the date to find the minimum, maximum, first, and last price of each day Finally we will use plotly to create the candlestick chart and plot it.

The plotly Python package exists to create, manipulate and render graphical figures (i.e. charts, plots, maps and diagrams) represented by data structures also referred to as figures.

```{python}
#convert the timestamp
candlestick_data = bitcoin_price_data.groupby(bitcoin_price_data.Date.dt.date).agg({'Price': ['min', 'max', 'first', 'last']})
candlestick_data.head()
```

```{python}
fig = go.Figure(data=[go.Candlestick(x=candlestick_data.index,
        open=candlestick_data['Price']['first'],
        high=candlestick_data['Price']['max'],
        low=candlestick_data['Price']['min'],
        close=candlestick_data['Price']['last'])
        ])

fig.update_layout(xaxis_rangeslider_visible=False, xaxis_title="Date",yaxis_title="Price in USD", title='Bitcoin Candlestick Chart over Past 30 days')

fig.show()
```


## Watson Text to Speech API

We will transcribe an audio file using the Watson Text to Speech API. We will then translate the text to a new language using the Watson Language Translator API. In the API call, you will send a copy of the audio file to the API. This is sometimes called a POST request. Then the API will send the text transcription of what the individual is saying. Under the hood, the API is making a GET request. We then send the text we would like to translate into a second language to a second API. The API will translate the text and send the translation back to you. In this case, we translate English to Spanish. We then provide an overview of API keys and endpoints, Watson Speech to Text, and Watson Translate. First, we will review API keys and endpoints. They will give you access to the API.


## REST APIs & HTTP Requests

The HTTP protocol can be thought of as a general protocol of transferring information through the web.

REST API’s function by sending a request, and the request is communicated via HTTP message. The HTTP message usually contains a JSON file. 

When you, the client, use a web page your browser sends an  **HTTP request** to the server where the page is hosted. The server tries to find the desired resource by default "index.html". If your request is successful, the server will send the object to the client in an **HTTP response**; this includes information like the type of the resource, the length of the resource, and other information.

A **Uniform Resource Locator (URL)** is the most popular way to find resources on the web.

We can break the URL into three parts. 

- The **scheme**, i.e. the protocol, for this lab it will always be *http://* 
- The **Internet address** or Base URL which is used to find the location. I.e. www.gitlab.com
- The **route**, the location on the webserver, i.e. /images/images.png

Together this gives something like: http://www.gitlab.com//images/images.png

### Status codes

The prefix indicates the class; for example, the 100s are informational responses; 100 indicates that everything is OK so far. The 200s are Successful responses: For example, 200 The request has succeeded. Anything in the 400s is bad news. 401 means the request is unauthorized. 500’s stands for server errors, like 501 for not Implemented.

### Requests

The process can be broken into the request and response process. The request using the get method is partially illustrated below. In the start line we have the GET method, this is an HTTP method. Also the location of the resource /index.html and the HTTP version. The Request header passes additional information with an HTTP request:

### HTTP methods

When an HTTP request is made, an  HTTP method is sent. This tells the server what action to perform. A list of several HTTP methods is shown below: 

- GET: Retrieve data from the server
- POST: submits data to the server
- PUT: Updates data already on the server
- DELETE: Deletes data from the server

### Response

The response start line contains the version number HTTP/1.0, a status code (200) meaning success, followed by a descriptive phrase (OK). The response header contains useful information. Finally, we have the response body containing the requested file, an  HTML  document. It should be noted that some requests have headers.

### The python requests library

#### Basics

The Requests Library a popular method for dealing with the HTTP protocol in Python. Requests is a python Library that allows you to send HTTP/1.1 requests easily.

````{python}
#import the library 
import requests

#make a get request via the method get
url="https://www.ibm.com/"
r = requests.get(url)
```

We have the response object ’r’ , this has information about the request, like the status of the request. We can view the status code using the attribute status_code, which is 200 for OK

````{python}
#view status of the request
r.status_code
```

````{python}
#view the request headers
r.request.headers
```

We can view the request body in the following line. As there is no body for a GET request, we get a None or empty field.

````{python}
#view the request body
r.request.body
```

````{python}
#check the encoding of the data
r.encoding
```


````{python}
#view the request header
header = r.headers
header
```

As shown above, we can view the HTTP response header using the attribute headers. This returns a python dictionary of HTTP response headers. We can look at the dictionary values. We can obtain the date the request was sent by using the key Date.

````{python}
#obtain the date a request was send
print(header['date'])

#print the type of data obtained
print(header['Content-Type'])
```


As the Content-Type is text/html, we can use the attribute text to display the HTML in the body. We can review the first 100 characters.

````{python}
r.text[0:100]
```

You can load other types of data for non-text requests, like images. Consider the URL of the following image:

```{python}
url='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/IDSNlogo.png'

#make a request
r=requests.get(url)

#look at the response header 
print(r.headers)

#look at the content type
print(r.headers['Content-Type'])
```

An image is a response object that contains the image as a bytes-like object. As a result, we must save it using a file object. First, we specify the file path and name

```{python}
#load some libraries
import os 
from PIL import Image

#specify the file path and name
path=os.path.join(os.getcwd() + '/data/image.png')
path
```

We save the file, in order to access the body of the response we use the attribute content then save it using the open function and write method

```{python}
with open("/Users/ninadombrowski/Desktop/WorkingDir/Notebooks/IBM_Data_Science/4_Python_basics/data/image.png",'wb') as f:
    f.write(r.content)

#view image
Image.open("/Users/ninadombrowski/Desktop/WorkingDir/Notebooks/IBM_Data_Science/4_Python_basics/data/image.png")  
```

#### GET request with URL parameters

You can use the GET method to modify the results of your query. For example, retrieving data from an API.

 We send a GET request to the server. Like before, we have the Base URL in the Route and to this we append /get to indicate that we want to perform a get request: *http://httpbin.org/get*
 
 After GET is requested we have the query string. This is a part of a uniform resource locator (URL) and this sends other information to the web server. The start of the query is a *?*, followed by a series of parameter and value pairs:
*http://httpbin.org/get?Name=Joseph&ID=123*

Here, Name and ID are parameters and Joseph and 123 the search value pair.

```{python}
#set the base url with appending get
url_get="http://httpbin.org/get"

#create a query string via a dictionary
#the keys = parameter names
#the values = the value of the query string
payload={"name":"Joseph", "ID": "123"}

r = requests.get(url_get, params = payload)

#check response
print(r.status_code)

#check the url
print(r.url)

#view the response as text
print(r.text)

#look at the content type
print(r.headers["Content-Type"])
```

As the content 'Content-Type' is in the JSON, we format it using the method json() . It returns a Python dict: The key 'args' has the name and values for the  query string. 

```{python}
r.json()
```

#### POST request

Like a GET request a POST request is used to send data to a server, but the POST request sends the data in a request body, not the url.

```{python}
#prepare to send a post request
#this endpoint will expect data 
#and as such is an easy way to configure an http request to send data to a server
url_post = "http://httpbin.org/post"

#define our dictionary
payload={"name":"Joseph",
            "ID": "123"}

#make a post request
r_post = requests.post(url_post, data = payload)

#compare the requests
print("Post request URL: ", r_post.url)
print("Get request URL: ", r.url)
```

We can compare the POST and GET request body, we see only the POST request has a body:

```{python}
print("POST request body:",r_post.request.body)
print("GET request body:",r.request.body)
```

We see the POST request has no name or value pairs in it’s url. We can compare the POST and GET request body. We see only the POST request has a body.

```{python}
#view the key form to get the payload
r_post.json()["form"]
```


## HTML for webscraping

Let’s say you were asked to find the name and salary of players in a National Basketball League from a web page. 

<p align="left">
  <img width=300, height=400, src="../images/html_example.png">
</p>

The web page is comprised of HTML that consists of text surrounded by a series of blue text elements enclosed in angle brackets called **tags**. The tags tells the browser how to display the content. 

The first portion contains the "DOCTYPE html” which declares this document is an HTML document.

<html> element is the root element of an HTML page

<head> element contains meta information about the HTML page. 

Next, we have the body, this is what's displayed on the web page. This is usually the data we are interested in, we see the elements with an “h3”, this means type 3 heading, makes the text larger and bold. These tags have the names of the players, notice the data is enclosed in the elements.It starts with a h3 in brackets and ends in a slash h3 in brackets.

There is also a different tag “p”, this means paragraph, each p tag contains a player's salary.

Each HTML document can actually be referred to as a document tree. The tag HTML tag contains the head and body tag. The Head and body tag are the descendants of the html tag. In particular they are the children of the HTML tag. HTML tag is their parent.The head and body tag are siblings as they are on the same level. Title tag is the child of the head tag and its parent is the head tag. The title tag is a descendant of the HTML tag but not its child. 


## Webscraping

Webscraping is a process that can be used to automatically extract information from a website, and can easily be accomplished within a matter of minutes and not hours. 

To get started we just need a little Python code and the help of two modules named Requests and Beautiful Soup and the method find_all 

The requests method is used to download the webpage

BeautifulSoup represents HTML as a set of Tree like objects with methods used to parse the HTML.

Find_all is a filter, you can use filters to filter based on a tag’s name, it’s attributes, the text of a string, or on some combination of these. 

Let’s say we were asked to find the name and salary of players in a National Basketball League:

```{python}
#import libs for the webscraping and downloading the web page
from bs4 import BeautifulSoup 
import requests 
```

Beautiful Soup is a Python library for pulling data out of HTML and XML files, we will focus on HTML files. This is accomplished by representing the HTML as a set of objects with methods used to parse the HTML. We can navigate the HTML as a tree and/or filter out what we are looking for.

Consider the following HTML:

```
<!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>
<h3><b id='boldest'>Lebron James</b></h3>
<p> Salary: $ 92,000,000 </p>
<h3> Stephen Curry</h3>
<p> Salary: $85,000, 000 </p>
<h3> Kevin Durant </h3>
<p> Salary: $73,200, 000</p>
</body>
</html>
```

Renders to: 

<!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>
<h3><b id='boldest'>Lebron James</b></h3>
<p> Salary: $ 92,000,000 </p>
<h3> Stephen Curry</h3>
<p> Salary: $85,000, 000 </p>
<h3> Kevin Durant </h3>
<p> Salary: $73,200, 000</p>
</body>
</html>

We can store this html as a string varialbe:

```{python}
html="<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>"
```

To parse a document, pass it into the BeautifulSoup constructor, the BeautifulSoup object, which represents the document as a nested data structure:

```{python}
soup = BeautifulSoup(html, "html.parser")
```

First, the document is converted to Unicode, (similar to ASCII), and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The BeautifulSoup object can create other types of objects. In this lab, we will cover BeautifulSoup and Tag objects that for the purposes of this lab are identical, and NavigableString objects.

We can use the method prettify() to display the HTML in the nested structure:

```{python}
print(soup.prettify())
```

### Tags

Let's say we want the title of the page and the name of the top paid player we can use the Tag. The Tag object corresponds to an HTML tag in the original document, for example, the tag title.

```{python}
tag_object = soup.title
print("tag object:", tag_object)
```

We can extract the tag type like this:

```{python}
tag_object = soup.title
print("tag object type:", type(tag_object))
```

If there is more than one Tag with the same name, the first element with that Tag name is called, here, this corresponds to the most paid player:

```{python}
tag_object=soup.h3
tag_object
```

### Children, Parents and Sibilings

As stated above the Tag object is a tree of objects we can access the child of the tag or navigate down the branch as follows:

```{python}
tag_child =tag_object.b
tag_child
```
You can access the parent with the  parent

```{python}
parent_tag = tag_child.parent
parent_tag
```

tag_object parent is the body element.

```{python}
tag_object.parent
```

tag_object sibling is the paragraph element

```{python}
sibling_1=tag_object.next_sibling
sibling_1
```

sibling_2 is the header element which is also a sibling of both sibling_1 and tag_object

```{python}
sibling_2=sibling_1.next_sibling
sibling_2
```


### HTML attributes

If the tag has attributes, the tag id="boldest" has an attribute id whose value is boldest. You can access a tag’s attributes by treating the tag like a dictionary:

```{python}
tag_child['id']
```

You can access that dictionary directly as attrs:

```{python}
tag_child.attrs
```

You can also work with Multi-valued attribute check out [this page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01) for more.

We can also obtain the content if the attribute of the tag using the Python get() method.

```{python}
tag_child.get('id')
```


### Navigable string

A string corresponds to a bit of text or content within a tag. Beautiful Soup uses the NavigableString class to contain this text. In our HTML we can obtain the name of the first player by extracting the sting of the Tag object tag_child as follows:

```{python}
tag_string = tag_child.string
tag_string
```

A NavigableString is just like a Python string or Unicode string, to be more precise. The main difference is that it also supports some BeautifulSoup features. We can covert it to sting object in Python:

```{python}
unicode_string = str(tag_child.string)
unicode_string
```

### Filter

Filters allow you to find complex patterns, the simplest filter is a string. In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that exact string. Consider the following HTML of rocket launchs:

```
<table>
  <tr>
    <td id='flight' >Flight No</td>
    <td>Launch site</td> 
    <td>Payload mass</td>
   </tr>
  <tr> 
    <td>1</td>
    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>
    <td>300 kg</td>
  </tr>
  <tr>
    <td>2</td>
    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>
    <td>94 kg</td>
  </tr>
  <tr>
    <td>3</td>
    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>
    <td>80 kg</td>
  </tr>
</table>
```
<table>
  <tr>
    <td id='flight' >Flight No</td>
    <td>Launch site</td> 
    <td>Payload mass</td>
   </tr>
  <tr> 
    <td>1</td>
    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>
    <td>300 kg</td>
  </tr>
  <tr>
    <td>2</td>
    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>
    <td>94 kg</td>
  </tr>
  <tr>
    <td>3</td>
    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>
    <td>80 kg</td>
  </tr>
</table>

We can store it as a string in the variable table:

```{python}
table="<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>"

table_bs = BeautifulSoup(table, "html.parser")
table_bs
```

#### Find_all

The find_all() method looks through a tag’s descendants and retrieves all descendants that match your filters.

The Method signature for find_all(name, attrs, recursive, string, limit, **kwargs)

##### Name

When we set the name parameter to a tag name, the method will extract all the tags with that name and its children.

```{python}
table_rows = table_bs.find_all('tr')
table_rows
```

The result is a Python Iterable just like a list, each element is a tag object:

```{python}
first_row =table_rows[0]
first_row
```

we can obtain the child with:

```{python}
first_row.td
```

If we iterate through the list, each element corresponds to a row in the table:

```{python}
for i, row in enumerate(table_rows):
    print("row", i, "is", row)
```


As row is a cell object, we can apply the method find_all to it and extract table cells in the object cells using the tag td, this is all the children with the name td. The result is a list, each element corresponds to a cell and is a Tag object, we can iterate through this list as well. We can extract the content using the string attribute.

```{python}
for i, row in enumerate(table_rows):
    print("row", i)
    cells = row.find_all('td')
    for j, cell in enumerate(cells):
        print("column", j, "cell", cell)
```


If we use a list we can match against any item in that list.

```{python}
list_input=table_bs .find_all(name=["tr", "td"])
list_input
```

##### Attributes

If the argument is not recognized it will be turned into a filter on the tag’s attributes. For example the id argument, Beautiful Soup will filter against each tag’s id attribute. For example, the first td elements have a value of id of flight, therefore we can filter based on that id value.

```{python}
table_bs.find_all(id="flight")
```

We can find all the elements that have links to the Florida Wikipedia page:

```{python}
list_input=table_bs.find_all(href="https://en.wikipedia.org/wiki/Florida")
list_input
```

If we set the href attribute to True, regardless of what the value is, the code finds all tags with href value:

```{python}
table_bs.find_all(href=True)
```

There are other methods for dealing with attributes and other related methods; Check out the following [link](href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01#css-selectors)


##### Strings

With string you can search for strings instead of tags, where we find all the elments with Florida:

```{python}
table_bs.find_all(string = "Florida")
```


##### find

The find_all() method scans the entire document looking for results, it’s if you are looking for one element you can use the find() method to find the first element in the document. Consider the following two tables:


<h3>Rocket Launch </h3>

<p>
<table class='rocket'>
  <tr>
    <td>Flight No</td>
    <td>Launch site</td> 
    <td>Payload mass</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Florida</td>
    <td>300 kg</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Texas</td>
    <td>94 kg</td>
  </tr>
  <tr>
    <td>3</td>
    <td>Florida </td>
    <td>80 kg</td>
  </tr>
</table>
</p>
<p>



<h3>Pizza Party  </h3>
  
<p>    
<table class='pizza'>
  <tr>
    <td>Pizza Place</td>
    <td>Orders</td> 
    <td>Slices </td>
   </tr>
  <tr>
    <td>Domino's Pizza</td>
    <td>10</td>
    <td>100</td>
  </tr>
  <tr>
    <td>Little Caesars</td>
    <td>12</td>
    <td >144 </td>
  </tr>
  <tr>
    <td>Papa John's </td>
    <td>15 </td>
    <td>165</td>
  </tr>
</table>
</p>
<p>


```{python}
two_tables="<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>"

two_tables_bs= BeautifulSoup(two_tables, 'html.parser')
```

Access the first table with the tag name `table`:

```{python}
two_tables_bs.find("table")
```

We can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore.

```{python}
two_tables_bs.find("table",class_='pizza')
```


### Downloading and scraping from a web paste

```{python}
#download the contents from a webpage
url = "http://www.ibm.com"

#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text

#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")

#scrape all links (in html anchor/link is represented by the tag <a>)
for link in soup.find_all('a', href = True):
    print(link.get('href'))
```

We can also Scrape all images Tags

```{python}
for link in soup.find_all('img'):
    print(link)
    print(link.get('src'))
```

#### Scrape data from html tables

```{python}
#The below url contains an html table with data about colors and color codes.
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html"
```

Before proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check how many rows and columns (32 and 6) are there in the color table.

```{python}
# get the contents of the webpage in text format and store in a variable called data
data  = requests.get(url).text

#convert to a soup object
soup = BeautifulSoup(data,"html.parser")

#find a table in the html webpage via the table tag
table = soup.find('table')

#get all rows from the table via the tr tag
for row in table.find_all('tr'):
    #get all columns for each row using the td tag
    cols = row.find_all('td')
    #store the value in column3 as color name
    color_name = cols[2].string
    #store the color code in column 4 as color_code
    color_code = cols[3].string
    print("{}--->{}".format(color_name, color_code))
```

```{python}
#Get all rows from the table
for row in table.find_all('tr'): # in html table row is represented by the tag <tr>
    # Get all columns in each row.
    cols = row.find_all('td') # in html a column is represented by the tag <td>
    color_name = cols[2].string # store the value in column 3 as color_name
    color_code = cols[3].string # store the value in column 4 as color_code
    print("{}--->{}".format(color_name,color_code))
```


#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and Pandas


```{python}
import pandas as pd 

#The below url contains html tables with data about world population.
url = "https://en.wikipedia.org/wiki/World_population"

#get the contents and convert
data = requests.get(url).text
soup = BeautifulSoup(data, "html.parser")

#find all html tables
tables = soup.find_all('table')

#see how many tables we got
len(tables)
```


Assume that we are looking for the 10 most densly populated countries table, we can look through the tables list and find the right one we are look for based on the data in each table or we can search for the table name if it is in the table but this option might not always work.

```{python}
for index,table in enumerate(tables):
    if ("10 most densely populated countries" in str(table)):
        table_index = index
print(table_index)
```

See if you can locate the table name of the table, 10 most densly populated countries, below.

```{python}
#print(tables[table_index].prettify())
```

Now, we can extract data 

```{python}
population_data = pd.DataFrame(columns=["Rank", "Country", "Population", "Area", "Density"])

for row in tables[table_index].tbody.find_all("tr"):
    col = row.find_all("td")
    if (col != []):
        rank = col[0].text
        country = col[1].text
        population = col[2].text.strip()
        area = col[3].text.strip()
        density = col[4].text.strip()
        population_data = population_data.append({"Rank":rank, "Country":country, "Population":population, "Area":area, "Density":density}, ignore_index=True)

population_data
```

#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and read_html

Using the same url, data, soup, and tables object as in the last section we can use the read_html function to create a DataFrame.

Remember the table we need is located in tables[table_index]

We can now use the pandas function read_html and give it the string version of the table as well as the flavor which is the parsing engine bs4.

```{python}
#pd.read_html(str(tables[5]))
```
The function read_html always returns a list of DataFrames so we must pick the one we want out of the list.

```{python}
population_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]
#population_data_read_html
```

#### Scrape data from HTML tables into a dataframe using read_html

We can also use the read_html function to directly get DataFrames from a url.

```{python}
dataframe_list = pd.read_html(url)

#check if we find the tables again
print(len(dataframe_list))

#get the data we need
#dataframe_list[5]
```
We can also use the match parameter to select the specific table we want. If the table contains a string matching the text it will be read.

```{python}
df = pd.read_html(url, match="10 most densely populated countries")[0]
#df
```


### Working with different file formats

Data engineering is one of the most critical and foundational skills in any data scientist’s toolkit.

There are several steps in Data Engineering process.

1. Extract - Data extraction is getting data from multiple sources. Ex. Data extraction from a website using Web scraping or gathering information from the data that are stored in different formats(JSON, CSV, XLSX etc.).

1. Transform - Tarnsforming the data means removing the data that we don't need for further analysis and converting the data in the format that all the data from the multiple sources is in the same format.

1. Load - Loading the data inside a data warehouse. Data warehouse essentially contains large volumes of data that are accessed to gather insights.

A **file format** is a standard way in which information is encoded for storage in a file. First, the file format specifies whether the file is a binary or ASCII file. Second, it shows how the information is organized. For example, the comma-separated values (CSV) file format stores tabular data in plain text.

To identify a file format, you can usually look at the file extension to get an idea. For example, a file saved with name “Data” in “CSV” format will appear as “Data.csv”. By noticing the “.csv” extension, we can clearly identify that it is a “CSV” file and the data is stored in a tabular format.

#### csv

The Comma-separated values file format falls under a spreadsheet file format.

In a spreadsheet file format, data is stored in cells. Each cell is organized in rows and columns. A column in the spreadsheet file can have different types. For example, a column can be of string type, a date type, or an integer type.

Each line in CSV file represents an observation, or commonly called a record. Each record may contain one or more fields which are separated by a comma.

We use pandas.read_csv() function to read the csv file. In the parentheses, we put the file path along with a quotation mark as an argument, so that pandas will read the file into a data frame from that address. The file path can be either a URL or your local file address.

```{python}
import pandas as pd
import numpy as np

df = pd.read_csv("../data/addresses.csv", header=None)
df
```

We can also add column headers:

```{python}
df.columns = ["FirstName", "LastName", "Location", "City", "State", "AreaCode"]
#df
```

#### Pandas transform function 

Python’s [Transform function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01) returns a self-produced dataframe with transformed values after applying the function specified in its parameter.

```{python}
#create a test df
df=pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])
df

#add 10 to each element in a dataframe
df = df.transform(func = lambda x : x + 10)
print(df)

#to find the square root to each element of the dataframe
result = df.transform(func = ["sqrt"])
print(result)
```

#### Json

JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write.

JSON is built on two structures:

1. A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.
1. An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.

JSON is a language-independent data format. It was derived from JavaScript, but many modern programming languages include code to generate and parse JSON-format data. It is a very common data format with a diverse range of applications.

The text in JSON is done through quoted string which contains the values in key-value mappings within { }. It is similar to the dictionary in Python.


##### Writing JSON to a file

This is usually called serialization. It is the process of converting an object into a special format which is suitable for transmitting over the network or storing in file or database.

To handle the data flow in a file, the JSON library in Python uses the dump() or dumps() function to convert the Python objects into their respective JSON object. This makes it easy to write data to files.

```{python}
import json

person = {
    'first_name' : 'Mark',
    'last_name' : 'abc',
    'age' : 27,
    'address': {
        "streetAddress": "21 2nd Street",
        "city": "New York",
        "state": "NY",
        "postalCode": "10021-3100"
    }
}

person
```


**json.dump()** method can be used for writing to JSON file.

Syntax: json.dump(dict, file_pointer)

Parameters:

1. dictionary – name of the dictionary which should be converted to JSON object.
2. file pointer – pointer of the file opened in write or append mode.

```{python}
with open('../data/person.json', 'w') as f:  # writing JSON object
    json.dump(person, f)
```

**json.dumps()** that helps in converting a dictionary to a JSON object.

It takes two parameters:

1. dictionary – name of the dictionary which should be converted to JSON object.
1. indent – defines the number of units for indentation

```{python}
# Serializing json  
json_object = json.dumps(person, indent = 4) 
  
# Writing to sample.json 
with open("../data/sample.json", "w") as outfile: 
    outfile.write(json_object) 

print(json_object)
```



##### Reading JSON to a file

This process is usually called Deserialization - it is the reverse of serialization. It converts the special format returned by the serialization back into a usable object.

Using json.load()
The JSON package has json.load() function that loads the json content from a json file into a dictionary.

It takes one parameter:

File pointer: A file pointer that points to a JSON file.

```{python}
#open a json file
with open('../data/sample.json', 'r') as openfile:
    json_object = json.load(openfile)

print(json_object) 
print(type(json_object)) 
```


#### Reading XLSX to a file

```{python}
df = pd.read_excel("../data/file_example_XLSX_10.xlsx", engine='openpyxl')
df
```


####  XML format

XML is also known as Extensible Markup Language. As the name suggests, it is a markup language. It has certain rules for encoding data. XML file format is a human-readable and machine-readable file format.

Pandas does not include any methods to read and write XML files. Here, we will take a look at how we can use other modules to read data from an XML file, and load it into a Pandas DataFrame.

##### Writing with xml.etree.ElementTree

The xml.etree.ElementTree module comes built-in with Python. It provides functionality for parsing and creating XML documents. ElementTree represents the XML document as a tree. We can move across the document using nodes which are elements and sub-elements of the XML file.

For more information please read the xml.etree.ElementTree documentation.

```{python}
import xml.etree.ElementTree as ET

# create the file structure
employee = ET.Element('employee')
details = ET.SubElement(employee, 'details')
first = ET.SubElement(details, 'firstname')
second = ET.SubElement(details, 'lastname')
third = ET.SubElement(details, 'age')
first.text = 'Shiv'
second.text = 'Mishra'
third.text = '23'

# create a new XML file with the results
mydata1 = ET.ElementTree(employee)
# myfile = open("items2.xml", "wb")
# myfile.write(mydata)
with open("../data/new_sample.xml", "wb") as files:
    mydata1.write(files)
```


##### Reading with xml.etree.ElementTree¶

You would need to firstly parse an XML file and create a list of columns for data frame, then extract useful information from the XML file and add to a pandas data frame.

Here is a sample code that you can use.:

```{python}
import xml.etree.ElementTree as etree

tree = etree.parse("../data/Sample-employee-XML-file.xml")

root = tree.getroot()
columns = ["firstname", "lastname", "title", "division", "building","room"]

datatframe = pd.DataFrame(columns = columns)

for node in root: 
    firstname = node.find("firstname").text
    lastname = node.find("lastname").text 
    title = node.find("title").text 
    division = node.find("division").text 
    building = node.find("building").text
    room = node.find("room").text
    datatframe = datatframe.append(pd.Series([firstname, lastname, title, division, building, room], index = columns), ignore_index = True)

datatframe
```


##### Reading xml file using pandas.read_xml function

We can also read the downloaded xml file using the read_xml function present in the pandas library which returns a Dataframe object.

For more information read the pandas.read_xml documentation.

```{python}
# Herein xpath we mention the set of xml nodes to be considered for migrating  to the dataframe which in this case is details node under employees.
df2=pd.read_xml("../data/Sample-employee-XML-file.xml") 
df2
```


####  Binary file format


"Binary" files are any files where the format isn't made up of readable characters. It contain formatting information that only certain applications or processors can understand. While humans can read text files, binary files must be run on the appropriate software or processor before humans can read them.

Binary files can range from image files like JPEGs or GIFs, audio files like MP3s or binary document formats like Word or PDF.

Let's see how to read an Image file.

##### Reading an image file


Python supports very powerful tools when it comes to image processing. Let’s see how to process the images using the PIL library.

PIL is the Python Imaging Library which provides the python interpreter with image editing capabilities.


```{python}
from PIL import Image 

# Read image 
img = Image.open('../data/dog.jpg') 
  
# Output Images 
display(img)
```

## Data analysis

In this section, you will learn how to approach data acquisition in various ways and obtain necessary insights from a dataset. By the end of this lab, you will successfully load the data into Jupyter Notebook and gain some fundamental insights via the Pandas Library.

In our case, the Diabetes Dataset is an online source and it is in CSV (comma separated value) format. Let's use this dataset as an example to practice data reading.

Context: This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years of age of Pima Indian heritage.

Content: The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

We have 768 rows and 9 columns. The first 8 columns represent the features and the last column represent the target/label.

```{python}
df = pd.read_csv("../data/diabetes.csv")
df.head()
```

This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage

```{python}
df.info()
```

Pandas describe() is used to view some basic statistical details like percentile, mean, standard deviation, etc. of a data frame or a series of numeric values. When this method is applied to a series of strings, it returns a different output

```{python}
df.describe()
```


### Identifying and handling missing values

We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:

.isnull()

.notnull()

The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. "True" stands for missing value, while "False" stands for not missing value.


```{python}
missing_data = df.isnull()
missing_data.head()
```

Count missing values in each column
Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, "True" represents a missing value, "False" means the value is present in the dataset. In the body of the for loop the method ".value_counts()" counts the number of "True" values.

```{python}
for column in missing_data.columns.values.tolist():
    print(column)
    print(missing_data[column].value_counts())
    print("")
```


As you can see above, there is no missing values in the dataset.

Correct data format
Check all data is in the correct format (int, float, text or other).

In Pandas, we use

.dtype() to check the data type

.astype() to change the data type

Numerical variables should have type 'float' or 'int'.


```{python}
df.dtypes
```

### Data vis

Visualization is one of the best way to get insights from the dataset. Seaborn and Matplotlib are two of Python's most powerful visualization libraries.

```{python}
# import libraries
import matplotlib.pyplot as plt
import seaborn as sns

#define categories
labels= 'Diabetic','Not Diabetic'

plt.pie(df['Outcome'].value_counts(),labels=labels,autopct='%0.02f%%')
plt.legend()
plt.show()
```



```{python}
df.dtypes
```
