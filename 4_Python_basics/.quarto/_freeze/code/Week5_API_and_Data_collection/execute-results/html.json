{
  "hash": "50984bf5ae63213dc8148dafa95055a9",
  "result": {
    "markdown": "# APIs and Data collection\n\n\n\n```{bash}\npwd\n```\n\n\n\n## Simple Application Programmming Interaces (APIs)\n\nAn API lets two pieces of software talk to each other For example you have your program, you have some data, you have other software components. You use the api to communicate with the api via inputs and outputs.\n\nJust like a function, you don’t have to know how the API works, but just its inputs and outputs. Pandas is actually a set of software components, much of which are not even written in Python. You have some data. You have a set of software components. We use the pandas api to process the data by communicating with the other Software Components.\n\nAn example:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndict_ = {\"a\":[11,21,31], \"b\":[12,22,23]}\ndf = pd.DataFrame(dict_)\nprint(df.head())\n```\n:::\n\n\nWhen you create a dictionary, and then create a pandas object with the Dataframe constructor, in API lingo, this is an “instance.” The data in the dictionary is passed along to the pandas API. You then use the dataframe to communicate with the API. When you call the method head, the dataframe communicates with the API displaying the first few rows of the dataframe.\n\n\n## REST APIs\n\nREST APIs are another popular type of API; they allow you to communicate through the internet allowing you to take advantage of resources like storage, access more data, artificial intelligent algorithms, and much more. \n\nThe RE stands for Representational, the S stands for State, the T stand for Transfer. \n\nIn rest API’s your program is called the client. The API communicates with a web service you call through the internet. There is a set of rules regarding Communication, Input or Request, and Output or Response.\n\nYou or your code can be thought of as a **client**. The web service is referred to as a **resource**. The client finds the service via an **endpoint**.  The client sends requests to the resource and the response to the client. \n\nHTTP methods are a way of transmitting data over the internet We tell the Rest API’s what to do by sending a request. The request is usually communicated via an HTTP message. The HTTP message usually contains a JSON file. This contains instructions for what operation we would like the service to perform. This operation is transmitted to the webservice via the internet. The service performs the operation.In the similar manner, the webservice returns a response via an HTTP message, where the information is usually returned via a JSON file. This information is transmitted back to the client.\n\n\n\n## Pycoingeckp\n\nCrypto Currency data is excellent to be used in an API because it is being constantly updated and it is vital to CryptoCurrency Trading We will use the Py-Coin-Gecko Python Client/Wrapper for the Coin Gecko API, updated every minute by Coin-Gecko We use the Wrapper/Client because it is easy to use so you can focus on the task of collecting data, we will also introduce pandas time series functions for dealing with time series data\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#import software\nfrom pycoingecko import CoinGeckoAPI\n\n#create a client\ncg = CoinGeckoAPI()\n\n#use a fct to request data\n#getting data on bitcoin, in U.S. Dollars, for the past 30 days\n#we get a JSON expressed as a python dictionary of nested lists\nbitcoin_data = cg.get_coin_market_chart_by_id(id=\"bitcoin\", vs_currency=\"usd\",days=30)\n\n#select only the prices\nbitcoin_price_data = bitcoin_data['prices']\n\nbitcoin_price_data = pd.DataFrame(bitcoin_price_data, columns=[\"TimeStamp\", \"Price\"])\nbitcoin_price_data.head()\n```\n:::\n\n\nConvert the timestamp to a more readable format usign the pythin function *to_datetime*\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#convert the timestamp\nbitcoin_price_data['Date'] = pd.to_datetime(bitcoin_price_data['TimeStamp'], unit = 'ms')\nbitcoin_price_data.head()\n\n#an alternative way to do this\n#data['date'] = data['TimeStamp'].apply(lambda d: datetime.date.fromtimestamp(d/1000.0))\n```\n:::\n\n\nNow, we want to create a candlestick plot. To get the data for the daily candlesticks we will group by the date to find the minimum, maximum, first, and last price of each day Finally we will use plotly to create the candlestick chart and plot it.\n\nThe plotly Python package exists to create, manipulate and render graphical figures (i.e. charts, plots, maps and diagrams) represented by data structures also referred to as figures.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#convert the timestamp\ncandlestick_data = bitcoin_price_data.groupby(bitcoin_price_data.Date.dt.date).agg({'Price': ['min', 'max', 'first', 'last']})\ncandlestick_data.head()\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig = go.Figure(data=[go.Candlestick(x=candlestick_data.index,\n        open=candlestick_data['Price']['first'],\n        high=candlestick_data['Price']['max'],\n        low=candlestick_data['Price']['min'],\n        close=candlestick_data['Price']['last'])\n        ])\n\nfig.update_layout(xaxis_rangeslider_visible=False, xaxis_title=\"Date\",yaxis_title=\"Price in USD\", title='Bitcoin Candlestick Chart over Past 30 days')\n\nfig.show()\n```\n:::\n\n\n## Watson Text to Speech API\n\nWe will transcribe an audio file using the Watson Text to Speech API. We will then translate the text to a new language using the Watson Language Translator API. In the API call, you will send a copy of the audio file to the API. This is sometimes called a POST request. Then the API will send the text transcription of what the individual is saying. Under the hood, the API is making a GET request. We then send the text we would like to translate into a second language to a second API. The API will translate the text and send the translation back to you. In this case, we translate English to Spanish. We then provide an overview of API keys and endpoints, Watson Speech to Text, and Watson Translate. First, we will review API keys and endpoints. They will give you access to the API.\n\n\n## REST APIs & HTTP Requests\n\nThe HTTP protocol can be thought of as a general protocol of transferring information through the web.\n\nREST API’s function by sending a request, and the request is communicated via HTTP message. The HTTP message usually contains a JSON file. \n\nWhen you, the client, use a web page your browser sends an  **HTTP request** to the server where the page is hosted. The server tries to find the desired resource by default \"index.html\". If your request is successful, the server will send the object to the client in an **HTTP response**; this includes information like the type of the resource, the length of the resource, and other information.\n\nA **Uniform Resource Locator (URL)** is the most popular way to find resources on the web.\n\nWe can break the URL into three parts. \n\n- The **scheme**, i.e. the protocol, for this lab it will always be *http://* \n- The **Internet address** or Base URL which is used to find the location. I.e. www.gitlab.com\n- The **route**, the location on the webserver, i.e. /images/images.png\n\nTogether this gives something like: http://www.gitlab.com//images/images.png\n\n### Status codes\n\nThe prefix indicates the class; for example, the 100s are informational responses; 100 indicates that everything is OK so far. The 200s are Successful responses: For example, 200 The request has succeeded. Anything in the 400s is bad news. 401 means the request is unauthorized. 500’s stands for server errors, like 501 for not Implemented.\n\n### Requests\n\nThe process can be broken into the request and response process. The request using the get method is partially illustrated below. In the start line we have the GET method, this is an HTTP method. Also the location of the resource /index.html and the HTTP version. The Request header passes additional information with an HTTP request:\n\n### HTTP methods\n\nWhen an HTTP request is made, an  HTTP method is sent. This tells the server what action to perform. A list of several HTTP methods is shown below: \n\n- GET: Retrieve data from the server\n- POST: submits data to the server\n- PUT: Updates data already on the server\n- DELETE: Deletes data from the server\n\n### Response\n\nThe response start line contains the version number HTTP/1.0, a status code (200) meaning success, followed by a descriptive phrase (OK). The response header contains useful information. Finally, we have the response body containing the requested file, an  HTML  document. It should be noted that some requests have headers.\n\n### The python requests library\n\n#### Basics\n\nThe Requests Library a popular method for dealing with the HTTP protocol in Python. Requests is a python Library that allows you to send HTTP/1.1 requests easily.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#import the library \nimport requests\n\n#make a get request via the method get\nurl=\"https://www.ibm.com/\"\nr = requests.get(url)\n```\n:::\n\n\nWe have the response object ’r’ , this has information about the request, like the status of the request. We can view the status code using the attribute status_code, which is 200 for OK\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#view status of the request\nr.status_code\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#view the request headers\nr.request.headers\n```\n:::\n\n\nWe can view the request body in the following line. As there is no body for a GET request, we get a None or empty field.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n#view the request body\nr.request.body\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n#check the encoding of the data\nr.encoding\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n#view the request header\nheader = r.headers\nheader\n```\n:::\n\n\nAs shown above, we can view the HTTP response header using the attribute headers. This returns a python dictionary of HTTP response headers. We can look at the dictionary values. We can obtain the date the request was sent by using the key Date.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n#obtain the date a request was send\nprint(header['date'])\n\n#print the type of data obtained\nprint(header['Content-Type'])\n```\n:::\n\n\nAs the Content-Type is text/html, we can use the attribute text to display the HTML in the body. We can review the first 100 characters.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nr.text[0:100]\n```\n:::\n\n\nYou can load other types of data for non-text requests, like images. Consider the URL of the following image:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nurl='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/IDSNlogo.png'\n\n#make a request\nr=requests.get(url)\n\n#look at the response header \nprint(r.headers)\n\n#look at the content type\nprint(r.headers['Content-Type'])\n```\n:::\n\n\nAn image is a response object that contains the image as a bytes-like object. As a result, we must save it using a file object. First, we specify the file path and name\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n#load some libraries\nimport os \nfrom PIL import Image\n\n#specify the file path and name\npath=os.path.join(os.getcwd() + '/data/image.png')\npath\n```\n:::\n\n\nWe save the file, in order to access the body of the response we use the attribute content then save it using the open function and write method\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nwith open(\"/Users/ninadombrowski/Desktop/WorkingDir/Notebooks/IBM_Data_Science/4_Python_basics/data/image.png\",'wb') as f:\n    f.write(r.content)\n\n#view image\nImage.open(\"/Users/ninadombrowski/Desktop/WorkingDir/Notebooks/IBM_Data_Science/4_Python_basics/data/image.png\")  \n```\n:::\n\n\n#### GET request with URL parameters\n\nYou can use the GET method to modify the results of your query. For example, retrieving data from an API.\n\n We send a GET request to the server. Like before, we have the Base URL in the Route and to this we append /get to indicate that we want to perform a get request: *http://httpbin.org/get*\n \n After GET is requested we have the query string. This is a part of a uniform resource locator (URL) and this sends other information to the web server. The start of the query is a *?*, followed by a series of parameter and value pairs:\n*http://httpbin.org/get?Name=Joseph&ID=123*\n\nHere, Name and ID are parameters and Joseph and 123 the search value pair.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n#set the base url with appending get\nurl_get=\"http://httpbin.org/get\"\n\n#create a query string via a dictionary\n#the keys = parameter names\n#the values = the value of the query string\npayload={\"name\":\"Joseph\", \"ID\": \"123\"}\n\nr = requests.get(url_get, params = payload)\n\n#check response\nprint(r.status_code)\n\n#check the url\nprint(r.url)\n\n#view the response as text\nprint(r.text)\n\n#look at the content type\nprint(r.headers[\"Content-Type\"])\n```\n:::\n\n\nAs the content 'Content-Type' is in the JSON, we format it using the method json() . It returns a Python dict: The key 'args' has the name and values for the  query string. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nr.json()\n```\n:::\n\n\n#### POST request\n\nLike a GET request a POST request is used to send data to a server, but the POST request sends the data in a request body, not the url.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n#prepare to send a post request\n#this endpoint will expect data \n#and as such is an easy way to configure an http request to send data to a server\nurl_post = \"http://httpbin.org/post\"\n\n#define our dictionary\npayload={\"name\":\"Joseph\",\n            \"ID\": \"123\"}\n\n#make a post request\nr_post = requests.post(url_post, data = payload)\n\n#compare the requests\nprint(\"Post request URL: \", r_post.url)\nprint(\"Get request URL: \", r.url)\n```\n:::\n\n\nWe can compare the POST and GET request body, we see only the POST request has a body:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nprint(\"POST request body:\",r_post.request.body)\nprint(\"GET request body:\",r.request.body)\n```\n:::\n\n\nWe see the POST request has no name or value pairs in it’s url. We can compare the POST and GET request body. We see only the POST request has a body.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n#view the key form to get the payload\nr_post.json()[\"form\"]\n```\n:::\n\n\n## HTML for webscraping\n\nLet’s say you were asked to find the name and salary of players in a National Basketball League from a web page. \n\n<p align=\"left\">\n  <img width=300, height=400, src=\"../images/html_example.png\">\n</p>\n\nThe web page is comprised of HTML that consists of text surrounded by a series of blue text elements enclosed in angle brackets called **tags**. The tags tells the browser how to display the content. \n\nThe first portion contains the \"DOCTYPE html” which declares this document is an HTML document.\n\n<html> element is the root element of an HTML page\n\n<head> element contains meta information about the HTML page. \n\nNext, we have the body, this is what's displayed on the web page. This is usually the data we are interested in, we see the elements with an “h3”, this means type 3 heading, makes the text larger and bold. These tags have the names of the players, notice the data is enclosed in the elements.It starts with a h3 in brackets and ends in a slash h3 in brackets.\n\nThere is also a different tag “p”, this means paragraph, each p tag contains a player's salary.\n\nEach HTML document can actually be referred to as a document tree. The tag HTML tag contains the head and body tag. The Head and body tag are the descendants of the html tag. In particular they are the children of the HTML tag. HTML tag is their parent.The head and body tag are siblings as they are on the same level. Title tag is the child of the head tag and its parent is the head tag. The title tag is a descendant of the HTML tag but not its child. \n\n\n## Webscraping\n\nWebscraping is a process that can be used to automatically extract information from a website, and can easily be accomplished within a matter of minutes and not hours. \n\nTo get started we just need a little Python code and the help of two modules named Requests and Beautiful Soup and the method find_all \n\nThe requests method is used to download the webpage\n\nBeautifulSoup represents HTML as a set of Tree like objects with methods used to parse the HTML.\n\nFind_all is a filter, you can use filters to filter based on a tag’s name, it’s attributes, the text of a string, or on some combination of these. \n\nLet’s say we were asked to find the name and salary of players in a National Basketball League:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n#import libs for the webscraping and downloading the web page\nfrom bs4 import BeautifulSoup \nimport requests \n```\n:::\n\n\nBeautiful Soup is a Python library for pulling data out of HTML and XML files, we will focus on HTML files. This is accomplished by representing the HTML as a set of objects with methods used to parse the HTML. We can navigate the HTML as a tree and/or filter out what we are looking for.\n\nConsider the following HTML:\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title</title>\n</head>\n<body>\n<h3><b id='boldest'>Lebron James</b></h3>\n<p> Salary: $ 92,000,000 </p>\n<h3> Stephen Curry</h3>\n<p> Salary: $85,000, 000 </p>\n<h3> Kevin Durant </h3>\n<p> Salary: $73,200, 000</p>\n</body>\n</html>\n```\n\nRenders to: \n\n<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title</title>\n</head>\n<body>\n<h3><b id='boldest'>Lebron James</b></h3>\n<p> Salary: $ 92,000,000 </p>\n<h3> Stephen Curry</h3>\n<p> Salary: $85,000, 000 </p>\n<h3> Kevin Durant </h3>\n<p> Salary: $73,200, 000</p>\n</body>\n</html>\n\nWe can store this html as a string varialbe:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nhtml=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\"\n```\n:::\n\n\nTo parse a document, pass it into the BeautifulSoup constructor, the BeautifulSoup object, which represents the document as a nested data structure:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nsoup = BeautifulSoup(html, \"html.parser\")\n```\n:::\n\n\nFirst, the document is converted to Unicode, (similar to ASCII), and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The BeautifulSoup object can create other types of objects. In this lab, we will cover BeautifulSoup and Tag objects that for the purposes of this lab are identical, and NavigableString objects.\n\nWe can use the method prettify() to display the HTML in the nested structure:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nprint(soup.prettify())\n```\n:::\n\n\n### Tags\n\nLet's say we want the title of the page and the name of the top paid player we can use the Tag. The Tag object corresponds to an HTML tag in the original document, for example, the tag title.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntag_object = soup.title\nprint(\"tag object:\", tag_object)\n```\n:::\n\n\nWe can extract the tag type like this:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ntag_object = soup.title\nprint(\"tag object type:\", type(tag_object))\n```\n:::\n\n\nIf there is more than one Tag with the same name, the first element with that Tag name is called, here, this corresponds to the most paid player:\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ntag_object=soup.h3\ntag_object\n```\n:::\n\n\n### Children, Parents and Sibilings\n\nAs stated above the Tag object is a tree of objects we can access the child of the tag or navigate down the branch as follows:\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\ntag_child =tag_object.b\ntag_child\n```\n:::\n\n\nYou can access the parent with the  parent\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nparent_tag = tag_child.parent\nparent_tag\n```\n:::\n\n\ntag_object parent is the body element.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ntag_object.parent\n```\n:::\n\n\ntag_object sibling is the paragraph element\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nsibling_1=tag_object.next_sibling\nsibling_1\n```\n:::\n\n\nsibling_2 is the header element which is also a sibling of both sibling_1 and tag_object\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nsibling_2=sibling_1.next_sibling\nsibling_2\n```\n:::\n\n\n### HTML attributes\n\nIf the tag has attributes, the tag id=\"boldest\" has an attribute id whose value is boldest. You can access a tag’s attributes by treating the tag like a dictionary:\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ntag_child['id']\n```\n:::\n\n\nYou can access that dictionary directly as attrs:\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\ntag_child.attrs\n```\n:::\n\n\nYou can also work with Multi-valued attribute check out [this page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01) for more.\n\nWe can also obtain the content if the attribute of the tag using the Python get() method.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ntag_child.get('id')\n```\n:::\n\n\n### Navigable string\n\nA string corresponds to a bit of text or content within a tag. Beautiful Soup uses the NavigableString class to contain this text. In our HTML we can obtain the name of the first player by extracting the sting of the Tag object tag_child as follows:\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\ntag_string = tag_child.string\ntag_string\n```\n:::\n\n\nA NavigableString is just like a Python string or Unicode string, to be more precise. The main difference is that it also supports some BeautifulSoup features. We can covert it to sting object in Python:\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nunicode_string = str(tag_child.string)\nunicode_string\n```\n:::\n\n\n### Filter\n\nFilters allow you to find complex patterns, the simplest filter is a string. In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that exact string. Consider the following HTML of rocket launchs:\n\n```\n<table>\n  <tr>\n    <td id='flight' >Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n   </tr>\n  <tr> \n    <td>1</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n```\n<table>\n  <tr>\n    <td id='flight' >Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n   </tr>\n  <tr> \n    <td>1</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n\nWe can store it as a string in the variable table:\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\ntable=\"<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>\"\n\ntable_bs = BeautifulSoup(table, \"html.parser\")\ntable_bs\n```\n:::\n\n\n#### Find_all\n\nThe find_all() method looks through a tag’s descendants and retrieves all descendants that match your filters.\n\nThe Method signature for find_all(name, attrs, recursive, string, limit, **kwargs)\n\n##### Name\n\nWhen we set the name parameter to a tag name, the method will extract all the tags with that name and its children.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ntable_rows = table_bs.find_all('tr')\ntable_rows\n```\n:::\n\n\nThe result is a Python Iterable just like a list, each element is a tag object:\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nfirst_row =table_rows[0]\nfirst_row\n```\n:::\n\n\nwe can obtain the child with:\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nfirst_row.td\n```\n:::\n\n\nIf we iterate through the list, each element corresponds to a row in the table:\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nfor i, row in enumerate(table_rows):\n    print(\"row\", i, \"is\", row)\n```\n:::\n\n\nAs row is a cell object, we can apply the method find_all to it and extract table cells in the object cells using the tag td, this is all the children with the name td. The result is a list, each element corresponds to a cell and is a Tag object, we can iterate through this list as well. We can extract the content using the string attribute.\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nfor i, row in enumerate(table_rows):\n    print(\"row\", i)\n    cells = row.find_all('td')\n    for j, cell in enumerate(cells):\n        print(\"column\", j, \"cell\", cell)\n```\n:::\n\n\nIf we use a list we can match against any item in that list.\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nlist_input=table_bs .find_all(name=[\"tr\", \"td\"])\nlist_input\n```\n:::\n\n\n##### Attributes\n\nIf the argument is not recognized it will be turned into a filter on the tag’s attributes. For example the id argument, Beautiful Soup will filter against each tag’s id attribute. For example, the first td elements have a value of id of flight, therefore we can filter based on that id value.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\ntable_bs.find_all(id=\"flight\")\n```\n:::\n\n\nWe can find all the elements that have links to the Florida Wikipedia page:\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nlist_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\nlist_input\n```\n:::\n\n\nIf we set the href attribute to True, regardless of what the value is, the code finds all tags with href value:\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ntable_bs.find_all(href=True)\n```\n:::\n\n\nThere are other methods for dealing with attributes and other related methods; Check out the following [link](href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01#css-selectors)\n\n\n##### Strings\n\nWith string you can search for strings instead of tags, where we find all the elments with Florida:\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\ntable_bs.find_all(string = \"Florida\")\n```\n:::\n\n\n##### find\n\nThe find_all() method scans the entire document looking for results, it’s if you are looking for one element you can use the find() method to find the first element in the document. Consider the following two tables:\n\n\n<h3>Rocket Launch </h3>\n\n<p>\n<table class='rocket'>\n  <tr>\n    <td>Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>Florida</td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td>Texas</td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>Florida </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n</p>\n<p>\n\n\n\n<h3>Pizza Party  </h3>\n  \n<p>    \n<table class='pizza'>\n  <tr>\n    <td>Pizza Place</td>\n    <td>Orders</td> \n    <td>Slices </td>\n   </tr>\n  <tr>\n    <td>Domino's Pizza</td>\n    <td>10</td>\n    <td>100</td>\n  </tr>\n  <tr>\n    <td>Little Caesars</td>\n    <td>12</td>\n    <td >144 </td>\n  </tr>\n  <tr>\n    <td>Papa John's </td>\n    <td>15 </td>\n    <td>165</td>\n  </tr>\n</table>\n</p>\n<p>\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\ntwo_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\"\n\ntwo_tables_bs= BeautifulSoup(two_tables, 'html.parser')\n```\n:::\n\n\nAccess the first table with the tag name `table`:\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\ntwo_tables_bs.find(\"table\")\n```\n:::\n\n\nWe can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore.\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\ntwo_tables_bs.find(\"table\",class_='pizza')\n```\n:::\n\n\n### Downloading and scraping from a web paste\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\n#download the contents from a webpage\nurl = \"http://www.ibm.com\"\n\n#use get to download the contents of the webpage in text format and store in a variable called data:\ndata = requests.get(url).text\n\n#create a beautiful soup object\nsoup = BeautifulSoup(data,\"html.parser\")\n\n#scrape all links (in html anchor/link is represented by the tag <a>)\nfor link in soup.find_all('a', href = True):\n    print(link.get('href'))\n```\n:::\n\n\nWe can also Scrape all images Tags\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nfor link in soup.find_all('img'):\n    print(link)\n    print(link.get('src'))\n```\n:::\n\n\n#### Scrape data from html tables\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\n#The below url contains an html table with data about colors and color codes.\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\"\n```\n:::\n\n\nBefore proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check how many rows and columns (32 and 6) are there in the color table.\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\n# get the contents of the webpage in text format and store in a variable called data\ndata  = requests.get(url).text\n\n#convert to a soup object\nsoup = BeautifulSoup(data,\"html.parser\")\n\n#find a table in the html webpage via the table tag\ntable = soup.find('table')\n\n#get all rows from the table via the tr tag\nfor row in table.find_all('tr'):\n    #get all columns for each row using the td tag\n    cols = row.find_all('td')\n    #store the value in column3 as color name\n    color_name = cols[2].string\n    #store the color code in column 4 as color_code\n    color_code = cols[3].string\n    print(\"{}--->{}\".format(color_name, color_code))\n```\n:::\n\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\n#Get all rows from the table\nfor row in table.find_all('tr'): # in html table row is represented by the tag <tr>\n    # Get all columns in each row.\n    cols = row.find_all('td') # in html a column is represented by the tag <td>\n    color_name = cols[2].string # store the value in column 3 as color_name\n    color_code = cols[3].string # store the value in column 4 as color_code\n    print(\"{}--->{}\".format(color_name,color_code))\n```\n:::\n\n\n#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and Pandas\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\nimport pandas as pd \n\n#The below url contains html tables with data about world population.\nurl = \"https://en.wikipedia.org/wiki/World_population\"\n\n#get the contents and convert\ndata = requests.get(url).text\nsoup = BeautifulSoup(data, \"html.parser\")\n\n#find all html tables\ntables = soup.find_all('table')\n\n#see how many tables we got\nlen(tables)\n```\n:::\n\n\nAssume that we are looking for the 10 most densly populated countries table, we can look through the tables list and find the right one we are look for based on the data in each table or we can search for the table name if it is in the table but this option might not always work.\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\nfor index,table in enumerate(tables):\n    if (\"10 most densely populated countries\" in str(table)):\n        table_index = index\nprint(table_index)\n```\n:::\n\n\nSee if you can locate the table name of the table, 10 most densly populated countries, below.\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\n#print(tables[table_index].prettify())\n```\n:::\n\n\nNow, we can extract data \n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\npopulation_data = pd.DataFrame(columns=[\"Rank\", \"Country\", \"Population\", \"Area\", \"Density\"])\n\nfor row in tables[table_index].tbody.find_all(\"tr\"):\n    col = row.find_all(\"td\")\n    if (col != []):\n        rank = col[0].text\n        country = col[1].text\n        population = col[2].text.strip()\n        area = col[3].text.strip()\n        density = col[4].text.strip()\n        population_data = population_data.append({\"Rank\":rank, \"Country\":country, \"Population\":population, \"Area\":area, \"Density\":density}, ignore_index=True)\n\npopulation_data\n```\n:::\n\n\n#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and read_html\n\nUsing the same url, data, soup, and tables object as in the last section we can use the read_html function to create a DataFrame.\n\nRemember the table we need is located in tables[table_index]\n\nWe can now use the pandas function read_html and give it the string version of the table as well as the flavor which is the parsing engine bs4.\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\n#pd.read_html(str(tables[5]))\n```\n:::\n\n\nThe function read_html always returns a list of DataFrames so we must pick the one we want out of the list.\n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\npopulation_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]\n#population_data_read_html\n```\n:::\n\n\n#### Scrape data from HTML tables into a dataframe using read_html\n\nWe can also use the read_html function to directly get DataFrames from a url.\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\ndataframe_list = pd.read_html(url)\n\n#check if we find the tables again\nprint(len(dataframe_list))\n\n#get the data we need\n#dataframe_list[5]\n```\n:::\n\n\nWe can also use the match parameter to select the specific table we want. If the table contains a string matching the text it will be read.\n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\ndf = pd.read_html(url, match=\"10 most densely populated countries\")[0]\n#df\n```\n:::\n\n\n### Working with different file formats\n\nData engineering is one of the most critical and foundational skills in any data scientist’s toolkit.\n\nThere are several steps in Data Engineering process.\n\n1. Extract - Data extraction is getting data from multiple sources. Ex. Data extraction from a website using Web scraping or gathering information from the data that are stored in different formats(JSON, CSV, XLSX etc.).\n\n1. Transform - Tarnsforming the data means removing the data that we don't need for further analysis and converting the data in the format that all the data from the multiple sources is in the same format.\n\n1. Load - Loading the data inside a data warehouse. Data warehouse essentially contains large volumes of data that are accessed to gather insights.\n\nA **file format** is a standard way in which information is encoded for storage in a file. First, the file format specifies whether the file is a binary or ASCII file. Second, it shows how the information is organized. For example, the comma-separated values (CSV) file format stores tabular data in plain text.\n\nTo identify a file format, you can usually look at the file extension to get an idea. For example, a file saved with name “Data” in “CSV” format will appear as “Data.csv”. By noticing the “.csv” extension, we can clearly identify that it is a “CSV” file and the data is stored in a tabular format.\n\n#### csv\n\nThe Comma-separated values file format falls under a spreadsheet file format.\n\nIn a spreadsheet file format, data is stored in cells. Each cell is organized in rows and columns. A column in the spreadsheet file can have different types. For example, a column can be of string type, a date type, or an integer type.\n\nEach line in CSV file represents an observation, or commonly called a record. Each record may contain one or more fields which are separated by a comma.\n\nWe use pandas.read_csv() function to read the csv file. In the parentheses, we put the file path along with a quotation mark as an argument, so that pandas will read the file into a data frame from that address. The file path can be either a URL or your local file address.\n\n::: {.cell execution_count=67}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"../data/addresses.csv\", header=None)\ndf\n```\n:::\n\n\nWe can also add column headers:\n\n::: {.cell execution_count=68}\n``` {.python .cell-code}\ndf.columns = [\"FirstName\", \"LastName\", \"Location\", \"City\", \"State\", \"AreaCode\"]\n#df\n```\n:::\n\n\n#### Pandas transform function \n\nPython’s [Transform function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01) returns a self-produced dataframe with transformed values after applying the function specified in its parameter.\n\n::: {.cell execution_count=69}\n``` {.python .cell-code}\n#create a test df\ndf=pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\ndf\n\n#add 10 to each element in a dataframe\ndf = df.transform(func = lambda x : x + 10)\nprint(df)\n\n#to find the square root to each element of the dataframe\nresult = df.transform(func = [\"sqrt\"])\nprint(result)\n```\n:::\n\n\n#### Json\n\nJSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write.\n\nJSON is built on two structures:\n\n1. A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\n1. An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n\nJSON is a language-independent data format. It was derived from JavaScript, but many modern programming languages include code to generate and parse JSON-format data. It is a very common data format with a diverse range of applications.\n\nThe text in JSON is done through quoted string which contains the values in key-value mappings within { }. It is similar to the dictionary in Python.\n\n\n##### Writing JSON to a file\n\nThis is usually called serialization. It is the process of converting an object into a special format which is suitable for transmitting over the network or storing in file or database.\n\nTo handle the data flow in a file, the JSON library in Python uses the dump() or dumps() function to convert the Python objects into their respective JSON object. This makes it easy to write data to files.\n\n::: {.cell execution_count=70}\n``` {.python .cell-code}\nimport json\n\nperson = {\n    'first_name' : 'Mark',\n    'last_name' : 'abc',\n    'age' : 27,\n    'address': {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n    }\n}\n\nperson\n```\n:::\n\n\n**json.dump()** method can be used for writing to JSON file.\n\nSyntax: json.dump(dict, file_pointer)\n\nParameters:\n\n1. dictionary – name of the dictionary which should be converted to JSON object.\n2. file pointer – pointer of the file opened in write or append mode.\n\n::: {.cell execution_count=71}\n``` {.python .cell-code}\nwith open('../data/person.json', 'w') as f:  # writing JSON object\n    json.dump(person, f)\n```\n:::\n\n\n**json.dumps()** that helps in converting a dictionary to a JSON object.\n\nIt takes two parameters:\n\n1. dictionary – name of the dictionary which should be converted to JSON object.\n1. indent – defines the number of units for indentation\n\n::: {.cell execution_count=72}\n``` {.python .cell-code}\n# Serializing json  \njson_object = json.dumps(person, indent = 4) \n  \n# Writing to sample.json \nwith open(\"../data/sample.json\", \"w\") as outfile: \n    outfile.write(json_object) \n\nprint(json_object)\n```\n:::\n\n\n##### Reading JSON to a file\n\nThis process is usually called Deserialization - it is the reverse of serialization. It converts the special format returned by the serialization back into a usable object.\n\nUsing json.load()\nThe JSON package has json.load() function that loads the json content from a json file into a dictionary.\n\nIt takes one parameter:\n\nFile pointer: A file pointer that points to a JSON file.\n\n::: {.cell execution_count=73}\n``` {.python .cell-code}\n#open a json file\nwith open('../data/sample.json', 'r') as openfile:\n    json_object = json.load(openfile)\n\nprint(json_object) \nprint(type(json_object)) \n```\n:::\n\n\n#### Reading XLSX to a file\n\n::: {.cell execution_count=74}\n``` {.python .cell-code}\ndf = pd.read_excel(\"../data/file_example_XLSX_10.xlsx\", engine='openpyxl')\ndf\n```\n:::\n\n\n####  XML format\n\nXML is also known as Extensible Markup Language. As the name suggests, it is a markup language. It has certain rules for encoding data. XML file format is a human-readable and machine-readable file format.\n\nPandas does not include any methods to read and write XML files. Here, we will take a look at how we can use other modules to read data from an XML file, and load it into a Pandas DataFrame.\n\n##### Writing with xml.etree.ElementTree\n\nThe xml.etree.ElementTree module comes built-in with Python. It provides functionality for parsing and creating XML documents. ElementTree represents the XML document as a tree. We can move across the document using nodes which are elements and sub-elements of the XML file.\n\nFor more information please read the xml.etree.ElementTree documentation.\n\n::: {.cell execution_count=75}\n``` {.python .cell-code}\nimport xml.etree.ElementTree as ET\n\n# create the file structure\nemployee = ET.Element('employee')\ndetails = ET.SubElement(employee, 'details')\nfirst = ET.SubElement(details, 'firstname')\nsecond = ET.SubElement(details, 'lastname')\nthird = ET.SubElement(details, 'age')\nfirst.text = 'Shiv'\nsecond.text = 'Mishra'\nthird.text = '23'\n\n# create a new XML file with the results\nmydata1 = ET.ElementTree(employee)\n# myfile = open(\"items2.xml\", \"wb\")\n# myfile.write(mydata)\nwith open(\"../data/new_sample.xml\", \"wb\") as files:\n    mydata1.write(files)\n```\n:::\n\n\n##### Reading with xml.etree.ElementTree¶\n\nYou would need to firstly parse an XML file and create a list of columns for data frame, then extract useful information from the XML file and add to a pandas data frame.\n\nHere is a sample code that you can use.:\n\n::: {.cell execution_count=76}\n``` {.python .cell-code}\nimport xml.etree.ElementTree as etree\n\ntree = etree.parse(\"../data/Sample-employee-XML-file.xml\")\n\nroot = tree.getroot()\ncolumns = [\"firstname\", \"lastname\", \"title\", \"division\", \"building\",\"room\"]\n\ndatatframe = pd.DataFrame(columns = columns)\n\nfor node in root: \n    firstname = node.find(\"firstname\").text\n    lastname = node.find(\"lastname\").text \n    title = node.find(\"title\").text \n    division = node.find(\"division\").text \n    building = node.find(\"building\").text\n    room = node.find(\"room\").text\n    datatframe = datatframe.append(pd.Series([firstname, lastname, title, division, building, room], index = columns), ignore_index = True)\n\ndatatframe\n```\n:::\n\n\n##### Reading xml file using pandas.read_xml function\n\nWe can also read the downloaded xml file using the read_xml function present in the pandas library which returns a Dataframe object.\n\nFor more information read the pandas.read_xml documentation.\n\n::: {.cell execution_count=77}\n``` {.python .cell-code}\n# Herein xpath we mention the set of xml nodes to be considered for migrating  to the dataframe which in this case is details node under employees.\ndf2=pd.read_xml(\"../data/Sample-employee-XML-file.xml\") \ndf2\n```\n:::\n\n\n####  Binary file format\n\n\n\"Binary\" files are any files where the format isn't made up of readable characters. It contain formatting information that only certain applications or processors can understand. While humans can read text files, binary files must be run on the appropriate software or processor before humans can read them.\n\nBinary files can range from image files like JPEGs or GIFs, audio files like MP3s or binary document formats like Word or PDF.\n\nLet's see how to read an Image file.\n\n##### Reading an image file\n\n\nPython supports very powerful tools when it comes to image processing. Let’s see how to process the images using the PIL library.\n\nPIL is the Python Imaging Library which provides the python interpreter with image editing capabilities.\n\n::: {.cell execution_count=78}\n``` {.python .cell-code}\nfrom PIL import Image \n\n# Read image \nimg = Image.open('../data/dog.jpg') \n  \n# Output Images \ndisplay(img)\n```\n:::\n\n\n## Data analysis\n\nIn this section, you will learn how to approach data acquisition in various ways and obtain necessary insights from a dataset. By the end of this lab, you will successfully load the data into Jupyter Notebook and gain some fundamental insights via the Pandas Library.\n\nIn our case, the Diabetes Dataset is an online source and it is in CSV (comma separated value) format. Let's use this dataset as an example to practice data reading.\n\nContext: This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years of age of Pima Indian heritage.\n\nContent: The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\nWe have 768 rows and 9 columns. The first 8 columns represent the features and the last column represent the target/label.\n\n::: {.cell execution_count=79}\n``` {.python .cell-code}\ndf = pd.read_csv(\"../data/diabetes.csv\")\ndf.head()\n```\n:::\n\n\nThis method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage\n\n::: {.cell execution_count=80}\n``` {.python .cell-code}\ndf.info()\n```\n:::\n\n\nPandas describe() is used to view some basic statistical details like percentile, mean, standard deviation, etc. of a data frame or a series of numeric values. When this method is applied to a series of strings, it returns a different output\n\n::: {.cell execution_count=81}\n``` {.python .cell-code}\ndf.describe()\n```\n:::\n\n\n### Identifying and handling missing values\n\nWe use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n\n.isnull()\n\n.notnull()\n\nThe output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. \"True\" stands for missing value, while \"False\" stands for not missing value.\n\n::: {.cell execution_count=82}\n``` {.python .cell-code}\nmissing_data = df.isnull()\nmissing_data.head()\n```\n:::\n\n\nCount missing values in each column\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value, \"False\" means the value is present in the dataset. In the body of the for loop the method \".value_counts()\" counts the number of \"True\" values.\n\n::: {.cell execution_count=83}\n``` {.python .cell-code}\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())\n    print(\"\")\n```\n:::\n\n\nAs you can see above, there is no missing values in the dataset.\n\nCorrect data format\nCheck all data is in the correct format (int, float, text or other).\n\nIn Pandas, we use\n\n.dtype() to check the data type\n\n.astype() to change the data type\n\nNumerical variables should have type 'float' or 'int'.\n\n::: {.cell execution_count=84}\n``` {.python .cell-code}\ndf.dtypes\n```\n:::\n\n\n### Data vis\n\nVisualization is one of the best way to get insights from the dataset. Seaborn and Matplotlib are two of Python's most powerful visualization libraries.\n\n::: {.cell execution_count=85}\n``` {.python .cell-code}\n# import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#define categories\nlabels= 'Diabetic','Not Diabetic'\n\nplt.pie(df['Outcome'].value_counts(),labels=labels,autopct='%0.02f%%')\nplt.legend()\nplt.show()\n```\n:::\n\n\n::: {.cell execution_count=86}\n``` {.python .cell-code}\ndf.dtypes\n```\n:::\n\n\n",
    "supporting": [
      "Week5_API_and_Data_collection_files"
    ],
    "filters": [],
    "includes": {}
  }
}