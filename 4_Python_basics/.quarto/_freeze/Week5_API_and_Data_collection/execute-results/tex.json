{
  "hash": "10fc5402be0141a0e59a844aeb97e12d",
  "result": {
    "markdown": "# APIs and Data collection\n\n\n\n## Simple Application Programmming Interaces (APIs)\n\nAn API lets two pieces of software talk to each other For example you have your program, you have some data, you have other software components. You use the api to communicate with the api via inputs and outputs.\n\nJust like a function, you don’t have to know how the API works, but just its inputs and outputs. Pandas is actually a set of software components, much of which are not even written in Python. You have some data. You have a set of software components. We use the pandas api to process the data by communicating with the other Software Components.\n\nAn example:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndict_ = {\"a\":[11,21,31], \"b\":[12,22,23]}\ndf = pd.DataFrame(dict_)\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    a   b\n0  11  12\n1  21  22\n2  31  23\n```\n:::\n:::\n\n\nWhen you create a dictionary, and then create a pandas object with the Dataframe constructor, in API lingo, this is an “instance.” The data in the dictionary is passed along to the pandas API. You then use the dataframe to communicate with the API. When you call the method head, the dataframe communicates with the API displaying the first few rows of the dataframe.\n\n\n## REST APIs\n\nREST APIs are another popular type of API; they allow you to communicate through the internet allowing you to take advantage of resources like storage, access more data, artificial intelligent algorithms, and much more. \n\nThe RE stands for Representational, the S stands for State, the T stand for Transfer. \n\nIn rest API’s your program is called the client. The API communicates with a web service you call through the internet. There is a set of rules regarding Communication, Input or Request, and Output or Response.\n\nYou or your code can be thought of as a **client**. The web service is referred to as a **resource**. The client finds the service via an **endpoint**.  The client sends requests to the resource and the response to the client. \n\nHTTP methods are a way of transmitting data over the internet We tell the Rest API’s what to do by sending a request. The request is usually communicated via an HTTP message. The HTTP message usually contains a JSON file. This contains instructions for what operation we would like the service to perform. This operation is transmitted to the webservice via the internet. The service performs the operation.In the similar manner, the webservice returns a response via an HTTP message, where the information is usually returned via a JSON file. This information is transmitted back to the client.\n\n\n\n## Pycoingeckp\n\nCrypto Currency data is excellent to be used in an API because it is being constantly updated and it is vital to CryptoCurrency Trading We will use the Py-Coin-Gecko Python Client/Wrapper for the Coin Gecko API, updated every minute by Coin-Gecko We use the Wrapper/Client because it is easy to use so you can focus on the task of collecting data, we will also introduce pandas time series functions for dealing with time series data\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#import software\nfrom pycoingecko import CoinGeckoAPI\n\n#create a client\ncg = CoinGeckoAPI()\n\n#use a fct to request data\n#getting data on bitcoin, in U.S. Dollars, for the past 30 days\n#we get a JSON expressed as a python dictionary of nested lists\nbitcoin_data = cg.get_coin_market_chart_by_id(id=\"bitcoin\", vs_currency=\"usd\",days=30)\n\n#select only the prices\nbitcoin_price_data = bitcoin_data['prices']\n\nbitcoin_price_data = pd.DataFrame(bitcoin_price_data, columns=[\"TimeStamp\", \"Price\"])\nbitcoin_price_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=258}\n```{=tex}\n\\begin{tabular}{lrr}\n\\toprule\n{} &      TimeStamp &         Price \\\\\n\\midrule\n0 &  1660489283194 &  24524.766449 \\\\\n1 &  1660492939214 &  24549.692073 \\\\\n2 &  1660496474642 &  24451.904381 \\\\\n3 &  1660500083326 &  24306.625500 \\\\\n4 &  1660503694590 &  24319.996576 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nConvert the timestamp to a more readable format usign the pythin function *to_datetime*\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#convert the timestamp\nbitcoin_price_data['Date'] = pd.to_datetime(bitcoin_price_data['TimeStamp'], unit = 'ms')\nbitcoin_price_data.head()\n\n#an alternative way to do this\n#data['date'] = data['TimeStamp'].apply(lambda d: datetime.date.fromtimestamp(d/1000.0))\n```\n\n::: {.cell-output .cell-output-display execution_count=259}\n```{=tex}\n\\begin{tabular}{lrrl}\n\\toprule\n{} &      TimeStamp &         Price &                    Date \\\\\n\\midrule\n0 &  1660489283194 &  24524.766449 & 2022-08-14 15:01:23.194 \\\\\n1 &  1660492939214 &  24549.692073 & 2022-08-14 16:02:19.214 \\\\\n2 &  1660496474642 &  24451.904381 & 2022-08-14 17:01:14.642 \\\\\n3 &  1660500083326 &  24306.625500 & 2022-08-14 18:01:23.326 \\\\\n4 &  1660503694590 &  24319.996576 & 2022-08-14 19:01:34.590 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nNow, we want to create a candlestick plot. To get the data for the daily candlesticks we will group by the date to find the minimum, maximum, first, and last price of each day Finally we will use plotly to create the candlestick chart and plot it.\n\nThe plotly Python package exists to create, manipulate and render graphical figures (i.e. charts, plots, maps and diagrams) represented by data structures also referred to as figures.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#convert the timestamp\ncandlestick_data = bitcoin_price_data.groupby(bitcoin_price_data.Date.dt.date).agg({'Price': ['min', 'max', 'first', 'last']})\ncandlestick_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=260}\n```{=tex}\n\\begin{tabular}{lrrrr}\n\\toprule\n{} & \\multicolumn{4}{l}{Price} \\\\\n{} &           min &           max &         first &          last \\\\\nDate       &               &               &               &               \\\\\n\\midrule\n2022-08-14 &  24306.625500 &  24549.692073 &  24524.766449 &  24367.764394 \\\\\n2022-08-15 &  24001.621672 &  24995.084224 &  24322.437555 &  24054.940243 \\\\\n2022-08-16 &  23748.937385 &  24201.106463 &  24179.014652 &  23919.737715 \\\\\n2022-08-17 &  23310.441133 &  24438.651145 &  23898.443545 &  23385.146665 \\\\\n2022-08-18 &  23283.007168 &  23544.996596 &  23372.850059 &  23376.626385 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig = go.Figure(data=[go.Candlestick(x=candlestick_data.index,\n        open=candlestick_data['Price']['first'],\n        high=candlestick_data['Price']['max'],\n        low=candlestick_data['Price']['min'],\n        close=candlestick_data['Price']['last'])\n        ])\n\nfig.update_layout(xaxis_rangeslider_visible=False, xaxis_title=\"Date\",yaxis_title=\"Price in USD\", title='Bitcoin Candlestick Chart over Past 30 days')\n\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n:::\n\n\n## Watson Text to Speech API\n\nWe will transcribe an audio file using the Watson Text to Speech API. We will then translate the text to a new language using the Watson Language Translator API. In the API call, you will send a copy of the audio file to the API. This is sometimes called a POST request. Then the API will send the text transcription of what the individual is saying. Under the hood, the API is making a GET request. We then send the text we would like to translate into a second language to a second API. The API will translate the text and send the translation back to you. In this case, we translate English to Spanish. We then provide an overview of API keys and endpoints, Watson Speech to Text, and Watson Translate. First, we will review API keys and endpoints. They will give you access to the API.\n\n\n## REST APIs & HTTP Requests\n\nThe HTTP protocol can be thought of as a general protocol of transferring information through the web.\n\nREST API’s function by sending a request, and the request is communicated via HTTP message. The HTTP message usually contains a JSON file. \n\nWhen you, the client, use a web page your browser sends an  **HTTP request** to the server where the page is hosted. The server tries to find the desired resource by default \"index.html\". If your request is successful, the server will send the object to the client in an **HTTP response**; this includes information like the type of the resource, the length of the resource, and other information.\n\nA **Uniform Resource Locator (URL)** is the most popular way to find resources on the web.\n\nWe can break the URL into three parts. \n\n- The **scheme**, i.e. the protocol, for this lab it will always be *http://* \n- The **Internet address** or Base URL which is used to find the location. I.e. www.gitlab.com\n- The **route**, the location on the webserver, i.e. /images/images.png\n\nTogether this gives something like: http://www.gitlab.com//images/images.png\n\n### Status codes\n\nThe prefix indicates the class; for example, the 100s are informational responses; 100 indicates that everything is OK so far. The 200s are Successful responses: For example, 200 The request has succeeded. Anything in the 400s is bad news. 401 means the request is unauthorized. 500’s stands for server errors, like 501 for not Implemented.\n\n### Requests\n\nThe process can be broken into the request and response process. The request using the get method is partially illustrated below. In the start line we have the GET method, this is an HTTP method. Also the location of the resource /index.html and the HTTP version. The Request header passes additional information with an HTTP request:\n\n### HTTP methods\n\nWhen an HTTP request is made, an  HTTP method is sent. This tells the server what action to perform. A list of several HTTP methods is shown below: \n\n- GET: Retrieve data from the server\n- POST: submits data to the server\n- PUT: Updates data already on the server\n- DELETE: Deletes data from the server\n\n### Response\n\nThe response start line contains the version number HTTP/1.0, a status code (200) meaning success, followed by a descriptive phrase (OK). The response header contains useful information. Finally, we have the response body containing the requested file, an  HTML  document. It should be noted that some requests have headers.\n\n### The python requests library\n\n#### Basics\n\nThe Requests Library a popular method for dealing with the HTTP protocol in Python. Requests is a python Library that allows you to send HTTP/1.1 requests easily.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#import the library \nimport requests\n\n#make a get request via the method get\nurl=\"https://www.ibm.com/\"\nr = requests.get(url)\n```\n:::\n\n\nWe have the response object ’r’ , this has information about the request, like the status of the request. We can view the status code using the attribute status_code, which is 200 for OK\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#view status of the request\nr.status_code\n```\n\n::: {.cell-output .cell-output-display execution_count=263}\n```\n200\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#view the request headers\nr.request.headers\n```\n\n::: {.cell-output .cell-output-display execution_count=264}\n```\n{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': '_abck=785E9584B7C2B460C6BF7750EABA7EF3~-1~YAAQDvBuaBOAoieDAQAAu+ZCNwh3ivtw8JJ6fhL4RmJOF5uwH50rN0PWyu3y2zlGAgmCPw/cavUMvFiA+n4FUyyubVRK6agnrflTNY+H2Urx/B9fanOow7ahyF4YgctnlopPxKcQqTFL725YNs8Luq11Ika4HG6EV39OKqAAjhpC+HRAMjgtqFuEpNdQsoW7Ig1EBR5RahTfcTpqw1Nb9qDY5WTY4M0hCL54R6VwfUHFS5ALcK/ca4CQhbxTQ8NHqqP5Fp4uaGlC/Zx6QQBI8X8WPCpXnObh0aA255C+scPKcB5/W+BYm1pe3qcqjy6Gz7ndUeOA9hWZqABjayZ1yjhOZrZo/zMJW7IaY8exODq5LQ5N7Cw=~-1~-1~-1; bm_sz=AC631E5ED04D4D575E33D790D39DF962~YAAQDvBuaBSAoieDAQAAu+ZCNxEHeGi7cBroGm1B1gQUxl+vCxhDozx6bXVhjno5usa2HdQQ87G6k438f/t+UMaOFNuDsnnAM5wUQ6GxfZXloygO57un3jfJj+FvF2XTfN+8tALVdszoOc+L8rBHjSg2D4ghpKvNCPEvmX3WY2omUHODTNdaQKl+90Rtrb1JosK87PVsYp5LNaoNBagAk6drAjhoxOiRdGCRH9sgm3ZdXvrDhsdHucwVNp7EFWa3bnRueVfDHXdMMZzEOzorvYIyQ9tFg2yRIcav+YhMLHg=~4600625~3290945'}\n```\n:::\n:::\n\n\nWe can view the request body in the following line. As there is no body for a GET request, we get a None or empty field.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n#view the request body\nr.request.body\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n#check the encoding of the data\nr.encoding\n```\n\n::: {.cell-output .cell-output-display execution_count=266}\n```\n'UTF-8'\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n#view the request header\nheader = r.headers\nheader\n```\n\n::: {.cell-output .cell-output-display execution_count=267}\n```\n{'Server': 'Apache', 'x-drupal-dynamic-cache': 'UNCACHEABLE', 'Link': '<https://www.ibm.com/nl-en>; rel=\"canonical\", <//1.cms.s81c.com>; rel=preconnect; crossorigin, <//1.cms.s81c.com>; rel=dns-prefetch', 'x-ua-compatible': 'IE=edge', 'Content-Language': 'en-nl', 'Permissions-Policy': 'interest-cohort=()', 'x-generator': 'Drupal 9 (https://www.drupal.org)', 'x-dns-prefetch-control': 'on', 'x-drupal-cache': 'MISS', 'Last-Modified': 'Tue, 13 Sep 2022 09:35:11 GMT', 'ETag': '\"1663061711\"', 'Content-Type': 'text/html; charset=UTF-8', 'x-acquia-host': 'www.ibm.com', 'x-acquia-path': '/nl-en', 'x-acquia-site': '', 'x-acquia-purge-tags': '', 'x-varnish': '335024058 331613000', 'x-cache-hits': '3', 'x-age': '0', 'Accept-Ranges': 'bytes', 'Content-Encoding': 'gzip', 'Cache-Control': 'public, max-age=300', 'Expires': 'Tue, 13 Sep 2022 14:36:15 GMT', 'X-Akamai-Transformed': '9 9469 0 pmb=mTOE,2', 'Date': 'Tue, 13 Sep 2022 14:31:15 GMT', 'Content-Length': '9640', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'x-content-type-options': 'nosniff', 'X-XSS-Protection': '1; mode=block', 'Content-Security-Policy': 'upgrade-insecure-requests', 'Strict-Transport-Security': 'max-age=31536000', 'x-ibm-trace': 'www-dipatcher: dynamic rule'}\n```\n:::\n:::\n\n\nAs shown above, we can view the HTTP response header using the attribute headers. This returns a python dictionary of HTTP response headers. We can look at the dictionary values. We can obtain the date the request was sent by using the key Date.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n#obtain the date a request was send\nprint(header['date'])\n\n#print the type of data obtained\nprint(header['Content-Type'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTue, 13 Sep 2022 14:31:15 GMT\ntext/html; charset=UTF-8\n```\n:::\n:::\n\n\nAs the Content-Type is text/html, we can use the attribute text to display the HTML in the body. We can review the first 100 characters.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nr.text[0:100]\n```\n\n::: {.cell-output .cell-output-display execution_count=269}\n```\n'<!DOCTYPE html>\\n<html lang=\"en-nl\" dir=\"ltr\">\\n  <head>\\n    <meta charset=\"utf-8\" />\\n<script>digitalD'\n```\n:::\n:::\n\n\nYou can load other types of data for non-text requests, like images. Consider the URL of the following image:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nurl='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/IDSNlogo.png'\n\n#make a request\nr=requests.get(url)\n\n#look at the response header \nprint(r.headers)\n\n#look at the content type\nprint(r.headers['Content-Type'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'Date': 'Tue, 13 Sep 2022 14:31:16 GMT', 'X-Clv-Request-Id': '623e600e-a2f3-44b3-bfcd-407415412495', 'Server': 'Cleversafe', 'X-Clv-S3-Version': '2.5', 'Accept-Ranges': 'bytes', 'x-amz-request-id': '623e600e-a2f3-44b3-bfcd-407415412495', 'Cache-Control': 'max-age=0,public', 'ETag': '\"a831e767d02efd21b904ec485ac0c769\"', 'Content-Type': 'image/png', 'Last-Modified': 'Thu, 08 Sep 2022 12:41:33 GMT', 'Content-Length': '21590'}\nimage/png\n```\n:::\n:::\n\n\nAn image is a response object that contains the image as a bytes-like object. As a result, we must save it using a file object. First, we specify the file path and name\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n#load some libraries\nimport os \nfrom PIL import Image\n\n#specify the file path and name\npath=os.path.join(os.getcwd(),'data/image.png')\npath\n```\n\n::: {.cell-output .cell-output-display execution_count=271}\n```\n'/Users/ninadombrowski/Desktop/WorkingDir/Notebooks/IBM_Data_Science/4_Python_basics/data/image.png'\n```\n:::\n:::\n\n\nWe save the file, in order to access the body of the response we use the attribute content then save it using the open function and write method\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nwith open(path,'wb') as f:\n    f.write(r.content)\n\n#view image\nImage.open(path)  \n```\n\n::: {.cell-output .cell-output-display execution_count=272}\n![](Week5_API_and_Data_collection_files/figure-pdf/cell-18-output-1.png){fig-pos='H'}\n:::\n:::\n\n\n#### GET request with URL parameters\n\nYou can use the GET method to modify the results of your query. For example, retrieving data from an API.\n\n We send a GET request to the server. Like before, we have the Base URL in the Route and to this we append /get to indicate that we want to perform a get request: *http://httpbin.org/get*\n \n After GET is requested we have the query string. This is a part of a uniform resource locator (URL) and this sends other information to the web server. The start of the query is a *?*, followed by a series of parameter and value pairs:\n*http://httpbin.org/get?Name=Joseph&ID=123*\n\nHere, Name and ID are parameters and Joseph and 123 the search value pair.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n#set the base url with appending get\nurl_get=\"http://httpbin.org/get\"\n\n#create a query string via a dictionary\n#the keys = parameter names\n#the values = the value of the query string\npayload={\"name\":\"Joseph\", \"ID\": \"123\"}\n\nr = requests.get(url_get, params = payload)\n\n#check response\nprint(r.status_code)\n\n#check the url\nprint(r.url)\n\n#view the response as text\nprint(r.text)\n\n#look at the content type\nprint(r.headers[\"Content-Type\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n200\nhttp://httpbin.org/get?name=Joseph&ID=123\n{\n  \"args\": {\n    \"ID\": \"123\", \n    \"name\": \"Joseph\"\n  }, \n  \"headers\": {\n    \"Accept\": \"*/*\", \n    \"Accept-Encoding\": \"gzip, deflate\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"python-requests/2.28.1\", \n    \"X-Amzn-Trace-Id\": \"Root=1-63209434-7b5353e25a61c2ae6c248265\"\n  }, \n  \"origin\": \"81.206.95.219\", \n  \"url\": \"http://httpbin.org/get?name=Joseph&ID=123\"\n}\n\napplication/json\n```\n:::\n:::\n\n\nAs the content 'Content-Type' is in the JSON, we format it using the method json() . It returns a Python dict: The key 'args' has the name and values for the  query string. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nr.json()\n```\n\n::: {.cell-output .cell-output-display execution_count=274}\n```\n{'args': {'ID': '123', 'name': 'Joseph'},\n 'headers': {'Accept': '*/*',\n  'Accept-Encoding': 'gzip, deflate',\n  'Host': 'httpbin.org',\n  'User-Agent': 'python-requests/2.28.1',\n  'X-Amzn-Trace-Id': 'Root=1-63209434-7b5353e25a61c2ae6c248265'},\n 'origin': '81.206.95.219',\n 'url': 'http://httpbin.org/get?name=Joseph&ID=123'}\n```\n:::\n:::\n\n\n#### POST request\n\nLike a GET request a POST request is used to send data to a server, but the POST request sends the data in a request body, not the url.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n#prepare to send a post request\n#this endpoint will expect data \n#and as such is an easy way to configure an http request to send data to a server\nurl_post = \"http://httpbin.org/post\"\n\n#define our dictionary\npayload={\"name\":\"Joseph\",\n            \"ID\": \"123\"}\n\n#make a post request\nr_post = requests.post(url_post, data = payload)\n\n#compare the requests\nprint(\"Post request URL: \", r_post.url)\nprint(\"Get request URL: \", r.url)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPost request URL:  http://httpbin.org/post\nGet request URL:  http://httpbin.org/get?name=Joseph&ID=123\n```\n:::\n:::\n\n\nWe can compare the POST and GET request body, we see only the POST request has a body:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nprint(\"POST request body:\",r_post.request.body)\nprint(\"GET request body:\",r.request.body)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPOST request body: name=Joseph&ID=123\nGET request body: None\n```\n:::\n:::\n\n\nWe see the POST request has no name or value pairs in it’s url. We can compare the POST and GET request body. We see only the POST request has a body.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n#view the key form to get the payload\nr_post.json()[\"form\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=277}\n```\n{'ID': '123', 'name': 'Joseph'}\n```\n:::\n:::\n\n\n## HTML for webscraping\n\nLet’s say you were asked to find the name and salary of players in a National Basketball League from a web page. \n\n<p align=\"left\">\n  <img width=300, height=400, src=\"images/html_example.png\">\n</p>\n\nThe web page is comprised of HTML that consists of text surrounded by a series of blue text elements enclosed in angle brackets called **tags**. The tags tells the browser how to display the content. \n\nThe first portion contains the \"DOCTYPE html” which declares this document is an HTML document.\n\n<html> element is the root element of an HTML page\n\n<head> element contains meta information about the HTML page. \n\nNext, we have the body, this is what's displayed on the web page. This is usually the data we are interested in, we see the elements with an “h3”, this means type 3 heading, makes the text larger and bold. These tags have the names of the players, notice the data is enclosed in the elements.It starts with a h3 in brackets and ends in a slash h3 in brackets.\n\nThere is also a different tag “p”, this means paragraph, each p tag contains a player's salary.\n\nEach HTML document can actually be referred to as a document tree. The tag HTML tag contains the head and body tag. The Head and body tag are the descendants of the html tag. In particular they are the children of the HTML tag. HTML tag is their parent.The head and body tag are siblings as they are on the same level. Title tag is the child of the head tag and its parent is the head tag. The title tag is a descendant of the HTML tag but not its child. \n\n\n## Webscraping\n\nWebscraping is a process that can be used to automatically extract information from a website, and can easily be accomplished within a matter of minutes and not hours. \n\nTo get started we just need a little Python code and the help of two modules named Requests and Beautiful Soup and the method find_all \n\nThe requests method is used to download the webpage\n\nBeautifulSoup represents HTML as a set of Tree like objects with methods used to parse the HTML.\n\nFind_all is a filter, you can use filters to filter based on a tag’s name, it’s attributes, the text of a string, or on some combination of these. \n\nLet’s say we were asked to find the name and salary of players in a National Basketball League:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n#import libs for the webscraping and downloading the web page\nfrom bs4 import BeautifulSoup \nimport requests \n```\n:::\n\n\nBeautiful Soup is a Python library for pulling data out of HTML and XML files, we will focus on HTML files. This is accomplished by representing the HTML as a set of objects with methods used to parse the HTML. We can navigate the HTML as a tree and/or filter out what we are looking for.\n\nConsider the following HTML:\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title</title>\n</head>\n<body>\n<h3><b id='boldest'>Lebron James</b></h3>\n<p> Salary: $ 92,000,000 </p>\n<h3> Stephen Curry</h3>\n<p> Salary: $85,000, 000 </p>\n<h3> Kevin Durant </h3>\n<p> Salary: $73,200, 000</p>\n</body>\n</html>\n```\n\nRenders to: \n\n<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title</title>\n</head>\n<body>\n<h3><b id='boldest'>Lebron James</b></h3>\n<p> Salary: $ 92,000,000 </p>\n<h3> Stephen Curry</h3>\n<p> Salary: $85,000, 000 </p>\n<h3> Kevin Durant </h3>\n<p> Salary: $73,200, 000</p>\n</body>\n</html>\n\nWe can store this html as a string varialbe:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nhtml=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\"\n```\n:::\n\n\nTo parse a document, pass it into the BeautifulSoup constructor, the BeautifulSoup object, which represents the document as a nested data structure:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nsoup = BeautifulSoup(html, \"html.parser\")\n```\n:::\n\n\nFirst, the document is converted to Unicode, (similar to ASCII), and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The BeautifulSoup object can create other types of objects. In this lab, we will cover BeautifulSoup and Tag objects that for the purposes of this lab are identical, and NavigableString objects.\n\nWe can use the method prettify() to display the HTML in the nested structure:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nprint(soup.prettify())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<!DOCTYPE html>\n<html>\n <head>\n  <title>\n   Page Title\n  </title>\n </head>\n <body>\n  <h3>\n   <b id=\"boldest\">\n    Lebron James\n   </b>\n  </h3>\n  <p>\n   Salary: $ 92,000,000\n  </p>\n  <h3>\n   Stephen Curry\n  </h3>\n  <p>\n   Salary: $85,000, 000\n  </p>\n  <h3>\n   Kevin Durant\n  </h3>\n  <p>\n   Salary: $73,200, 000\n  </p>\n </body>\n</html>\n```\n:::\n:::\n\n\n### Tags\n\nLet's say we want the title of the page and the name of the top paid player we can use the Tag. The Tag object corresponds to an HTML tag in the original document, for example, the tag title.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntag_object = soup.title\nprint(\"tag object:\", tag_object)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntag object: <title>Page Title</title>\n```\n:::\n:::\n\n\nWe can extract the tag type like this:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ntag_object = soup.title\nprint(\"tag object type:\", type(tag_object))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntag object type: <class 'bs4.element.Tag'>\n```\n:::\n:::\n\n\nIf there is more than one Tag with the same name, the first element with that Tag name is called, here, this corresponds to the most paid player:\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ntag_object=soup.h3\ntag_object\n```\n\n::: {.cell-output .cell-output-display execution_count=284}\n```\n<h3><b id=\"boldest\">Lebron James</b></h3>\n```\n:::\n:::\n\n\n### Children, Parents and Sibilings\n\nAs stated above the Tag object is a tree of objects we can access the child of the tag or navigate down the branch as follows:\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\ntag_child =tag_object.b\ntag_child\n```\n\n::: {.cell-output .cell-output-display execution_count=285}\n```\n<b id=\"boldest\">Lebron James</b>\n```\n:::\n:::\n\n\nYou can access the parent with the  parent\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nparent_tag = tag_child.parent\nparent_tag\n```\n\n::: {.cell-output .cell-output-display execution_count=286}\n```\n<h3><b id=\"boldest\">Lebron James</b></h3>\n```\n:::\n:::\n\n\ntag_object parent is the body element.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ntag_object.parent\n```\n\n::: {.cell-output .cell-output-display execution_count=287}\n```\n<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>\n```\n:::\n:::\n\n\ntag_object sibling is the paragraph element\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nsibling_1=tag_object.next_sibling\nsibling_1\n```\n\n::: {.cell-output .cell-output-display execution_count=288}\n```\n<p> Salary: $ 92,000,000 </p>\n```\n:::\n:::\n\n\nsibling_2 is the header element which is also a sibling of both sibling_1 and tag_object\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nsibling_2=sibling_1.next_sibling\nsibling_2\n```\n\n::: {.cell-output .cell-output-display execution_count=289}\n```\n<h3> Stephen Curry</h3>\n```\n:::\n:::\n\n\n### HTML attributes\n\nIf the tag has attributes, the tag id=\"boldest\" has an attribute id whose value is boldest. You can access a tag’s attributes by treating the tag like a dictionary:\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ntag_child['id']\n```\n\n::: {.cell-output .cell-output-display execution_count=290}\n```\n'boldest'\n```\n:::\n:::\n\n\nYou can access that dictionary directly as attrs:\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\ntag_child.attrs\n```\n\n::: {.cell-output .cell-output-display execution_count=291}\n```\n{'id': 'boldest'}\n```\n:::\n:::\n\n\nYou can also work with Multi-valued attribute check out [this page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01) for more.\n\nWe can also obtain the content if the attribute of the tag using the Python get() method.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ntag_child.get('id')\n```\n\n::: {.cell-output .cell-output-display execution_count=292}\n```\n'boldest'\n```\n:::\n:::\n\n\n### Navigable string\n\nA string corresponds to a bit of text or content within a tag. Beautiful Soup uses the NavigableString class to contain this text. In our HTML we can obtain the name of the first player by extracting the sting of the Tag object tag_child as follows:\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\ntag_string = tag_child.string\ntag_string\n```\n\n::: {.cell-output .cell-output-display execution_count=293}\n```\n'Lebron James'\n```\n:::\n:::\n\n\nA NavigableString is just like a Python string or Unicode string, to be more precise. The main difference is that it also supports some BeautifulSoup features. We can covert it to sting object in Python:\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nunicode_string = str(tag_child.string)\nunicode_string\n```\n\n::: {.cell-output .cell-output-display execution_count=294}\n```\n'Lebron James'\n```\n:::\n:::\n\n\n### Filter\n\nFilters allow you to find complex patterns, the simplest filter is a string. In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that exact string. Consider the following HTML of rocket launchs:\n\n```\n<table>\n  <tr>\n    <td id='flight' >Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n   </tr>\n  <tr> \n    <td>1</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n```\n<table>\n  <tr>\n    <td id='flight' >Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n   </tr>\n  <tr> \n    <td>1</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n\nWe can store it as a string in the variable table:\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\ntable=\"<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>\"\n\ntable_bs = BeautifulSoup(table, \"html.parser\")\ntable_bs\n```\n\n::: {.cell-output .cell-output-display execution_count=295}\n```\n<table><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr></table>\n```\n:::\n:::\n\n\n#### Find_all\n\nThe find_all() method looks through a tag’s descendants and retrieves all descendants that match your filters.\n\nThe Method signature for find_all(name, attrs, recursive, string, limit, **kwargs)\n\n##### Name\n\nWhen we set the name parameter to a tag name, the method will extract all the tags with that name and its children.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ntable_rows = table_bs.find_all('tr')\ntable_rows\n```\n\n::: {.cell-output .cell-output-display execution_count=296}\n```\n[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>,\n <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>]\n```\n:::\n:::\n\n\nThe result is a Python Iterable just like a list, each element is a tag object:\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nfirst_row =table_rows[0]\nfirst_row\n```\n\n::: {.cell-output .cell-output-display execution_count=297}\n```\n<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>\n```\n:::\n:::\n\n\nwe can obtain the child with:\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nfirst_row.td\n```\n\n::: {.cell-output .cell-output-display execution_count=298}\n```\n<td id=\"flight\">Flight No</td>\n```\n:::\n:::\n\n\nIf we iterate through the list, each element corresponds to a row in the table:\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nfor i, row in enumerate(table_rows):\n    print(\"row\", i, \"is\", row)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrow 0 is <tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>\nrow 1 is <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>\nrow 2 is <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>\nrow 3 is <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>\n```\n:::\n:::\n\n\nAs row is a cell object, we can apply the method find_all to it and extract table cells in the object cells using the tag td, this is all the children with the name td. The result is a list, each element corresponds to a cell and is a Tag object, we can iterate through this list as well. We can extract the content using the string attribute.\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nfor i, row in enumerate(table_rows):\n    print(\"row\", i)\n    cells = row.find_all('td')\n    for j, cell in enumerate(cells):\n        print(\"column\", j, \"cell\", cell)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrow 0\ncolumn 0 cell <td id=\"flight\">Flight No</td>\ncolumn 1 cell <td>Launch site</td>\ncolumn 2 cell <td>Payload mass</td>\nrow 1\ncolumn 0 cell <td>1</td>\ncolumn 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td>\ncolumn 2 cell <td>300 kg</td>\nrow 2\ncolumn 0 cell <td>2</td>\ncolumn 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>\ncolumn 2 cell <td>94 kg</td>\nrow 3\ncolumn 0 cell <td>3</td>\ncolumn 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td>\ncolumn 2 cell <td>80 kg</td>\n```\n:::\n:::\n\n\nIf we use a list we can match against any item in that list.\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nlist_input=table_bs .find_all(name=[\"tr\", \"td\"])\nlist_input\n```\n\n::: {.cell-output .cell-output-display execution_count=301}\n```\n[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n <td id=\"flight\">Flight No</td>,\n <td>Launch site</td>,\n <td>Payload mass</td>,\n <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td><td>300 kg</td></tr>,\n <td>1</td>,\n <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a></td>,\n <td>300 kg</td>,\n <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n <td>2</td>,\n <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n <td>94 kg</td>,\n <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td><td>80 kg</td></tr>,\n <td>3</td>,\n <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a></td>,\n <td>80 kg</td>]\n```\n:::\n:::\n\n\n##### Attributes\n\nIf the argument is not recognized it will be turned into a filter on the tag’s attributes. For example the id argument, Beautiful Soup will filter against each tag’s id attribute. For example, the first td elements have a value of id of flight, therefore we can filter based on that id value.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\ntable_bs.find_all(id=\"flight\")\n```\n\n::: {.cell-output .cell-output-display execution_count=302}\n```\n[<td id=\"flight\">Flight No</td>]\n```\n:::\n:::\n\n\nWe can find all the elements that have links to the Florida Wikipedia page:\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nlist_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\nlist_input\n```\n\n::: {.cell-output .cell-output-display execution_count=303}\n```\n[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a>,\n <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a>]\n```\n:::\n:::\n\n\nIf we set the href attribute to True, regardless of what the value is, the code finds all tags with href value:\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ntable_bs.find_all(href=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=304}\n```\n[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a></a></a>,\n <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida<a> </a></a>]\n```\n:::\n:::\n\n\nThere are other methods for dealing with attributes and other related methods; Check out the following [link](href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01#css-selectors)\n\n\n##### Strings\n\nWith string you can search for strings instead of tags, where we find all the elments with Florida:\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\ntable_bs.find_all(string = \"Florida\")\n```\n\n::: {.cell-output .cell-output-display execution_count=305}\n```\n['Florida', 'Florida']\n```\n:::\n:::\n\n\n##### find\n\nThe find_all() method scans the entire document looking for results, it’s if you are looking for one element you can use the find() method to find the first element in the document. Consider the following two tables:\n\n<h3>Rocket Launch </h3>\n\n<p>\n<table class='rocket'>\n  <tr>\n    <td>Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>Florida</td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td>Texas</td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>Florida </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n</p>\n<p>\n\n<h3>Pizza Party  </h3>\n  \n    \n<table class='pizza'>\n  <tr>\n    <td>Pizza Place</td>\n    <td>Orders</td> \n    <td>Slices </td>\n   </tr>\n  <tr>\n    <td>Domino's Pizza</td>\n    <td>10</td>\n    <td>100</td>\n  </tr>\n  <tr>\n    <td>Little Caesars</td>\n    <td>12</td>\n    <td >144 </td>\n  </tr>\n  <tr>\n    <td>Papa John's </td>\n    <td>15 </td>\n    <td>165</td>\n  </tr>\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\ntwo_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\"\n\ntwo_tables_bs= BeautifulSoup(two_tables, 'html.parser')\n```\n:::\n\n\nAccess the first table with the tag name `table`:\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\ntwo_tables_bs.find(\"table\")\n```\n\n::: {.cell-output .cell-output-display execution_count=307}\n```\n<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>\n```\n:::\n:::\n\n\nWe can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore.\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\ntwo_tables_bs.find(\"table\",class_='pizza')\n```\n\n::: {.cell-output .cell-output-display execution_count=308}\n```\n<table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>\n```\n:::\n:::\n\n\n### Downloading and scraping from a web paste\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\n#download the contents from a webpage\nurl = \"http://www.ibm.com\"\n\n#use get to download the contents of the webpage in text format and store in a variable called data:\ndata = requests.get(url).text\n\n#create a beautiful soup object\nsoup = BeautifulSoup(data,\"html.parser\")\n\n#scrape all links (in html anchor/link is represented by the tag <a>)\nfor link in soup.find_all('a', href = True):\n    print(link.get('href'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhttps://www.ibm.com/nl/en\nhttps://www.ibm.com/sitemap/nl/en\nhttps://developer.ibm.com/callforcode/?lnk=nlhpv18l1\nhttps://www.ibm.com/thought-leadership/new-creators/\nhttps://www.ibm.com/nl-en/products/linuxone-emperor-4?lnk=nlhpv18f0\nhttps://www.ibm.com/about/nl-en/secure-your-business/?lnk=nlhpv18f71\nhttps://www.ibm.com/cloud/nl-en/campaign/cloud-simplicity/?lnk=nlhpv18f72\nhttps://www.ibm.com/analytics/nl-en/data-fabric/?lnk=nlhpv18f73\nhttps://www.ibm.com/cloud/nl-en/aiops/?lnk=nlhpv18f74\nhttps://www.ibm.com/consulting/nl-en/?lnk=nlhpv18f75\n/nl-en/products/offers-and-discounts?lnk=hpv18t5\nhttps://www.ibm.com/cloud/watson-assistant?lnk=nlhpv18t1\nhttps://www.ibm.com/nl-en/cloud/satellite?lnk=nlhpv18t2\nhttps://www.ibm.com/nl-en/cloud/sap?lnk=nlhpv18t3\nhttps://www.ibm.com/cloud/blog/announcements/ibm-cloud-for-vmware-special-promotion?lnk=nlhpv18t4\nhttps://developer.ibm.com/depmodels/cloud/?lnk=hpv18ct16\nhttps://developer.ibm.com/technologies/artificial-intelligence?lnk=hpv18ct19\nhttps://developer.ibm.com/?lnk=hpv18ct9\nhttps://www.ibm.com/docs/en?lnk=hpv18ct14\nhttps://www.redbooks.ibm.com/?lnk=ushpv18ct10\nhttps://www.ibm.com/support/home/?lnk=hpv18ct11\nhttps://www.ibm.com/training/?lnk=hpv18ct15\n/cloud/hybrid?lnk=hpv18pt14\n/cloud/learn/public-cloud?lnk=hpv18ct1\n/watson?lnk=ushpv18pt17\n/garage?lnk=hpv18pt13\n/blockchain?lnk=hpv18pt4\nhttps://www.ibm.com/thought-leadership/institute-business-value/?lnk=hpv18pt12\n/analytics?lnk=hpv18pt1\n/security?lnk=hpv18pt9\n/financing?lnk=hpv18pt3\nhttps://www.ibm.com/quantum-computing?lnk=hpv18pt16\n/cloud/hybrid?lnk=hpv18ct20\n/cloud/learn/public-cloud?lnk=hpv18ct1\n/cloud/redhat?lnk=hpv18ct13\n/watson?lnk=hpv18ct3\nhttps://www.ibm.com/quantum-computing?lnk=hpv18ct18\n/cloud/learn/kubernetes?lnk=hpv18ct8\n/products/spss-statistics?lnk=ushpv18ct7\n/blockchain?lnk=hpv18ct1\nhttps://www.ibm.com/employment?lnk=hpv18ct2\nhttps://www.ibm.com/blogs/think/be-en/2021/07/09/identify-the-right-mvp-use-case/?lnk=hpv18cs1\nhttps://www.ibm.com/case-studies/abn-amro-bank?lnk=hpv18cs2\n#\n```\n:::\n:::\n\n\nWe can also Scrape all images Tags\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nfor link in soup.find_all('img'):\n    print(link)\n    print(link.get('src'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<img alt=\"call for code\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-07/20220704-ls-call-for-code-26798-720x360_1.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-07/20220704-ls-call-for-code-26798-720x360_1.jpg\n<img alt=\"IBM LinuxONE image\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-09-12/20220913-f-linux-one-chevrons-26793-444x254_1.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-09-12/20220913-f-linux-one-chevrons-26793-444x254_1.jpg\n<img alt=\"Man working at a computer\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-02-15/Secure.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-02-15/Secure.jpg\n<img alt=\"Two women working in a lab\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-02-15/Simplify.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-02-15/Simplify.jpg\n<img alt=\"oranges in production process\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-02-15/Data%20driven.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-02-15/Data%20driven.jpg\n<img alt=\"Man in automation working environment\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-02-15/Automate.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-02-15/Automate.jpg\n<img alt=\"Two men working\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2022-02-15/Transform_0.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2022-02-15/Transform_0.jpg\n<img alt=\"Screenshot of the IBM Watson Assistant template\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2021-08-17/Watson-Assistant-23212-700x420_0.png\"/>\n//1.cms.s81c.com/sites/default/files/2021-08-17/Watson-Assistant-23212-700x420_0.png\n<img alt=\"Screenshot of IBM Cloud Satellite template\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2021-07-30/Satellite-Template-444x254-%281%29_0.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2021-07-30/Satellite-Template-444x254-%281%29_0.jpg\n<img alt=\"Screenshot of SAP on IBM Cloud page\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2021-07-30/SAP-Template-444x254-%281%29_0.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2021-07-30/SAP-Template-444x254-%281%29_0.jpg\n<img alt=\"Screenshot of VMware template\" class=\"\" loading=\"lazy\" src=\"//1.cms.s81c.com/sites/default/files/2021-07-30/VM-Template-444x254-%281%29_0.jpg\"/>\n//1.cms.s81c.com/sites/default/files/2021-07-30/VM-Template-444x254-%281%29_0.jpg\n```\n:::\n:::\n\n\n#### Scrape data from html tables\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\n#The below url contains an html table with data about colors and color codes.\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\"\n```\n:::\n\n\nBefore proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check how many rows and columns (32 and 6) are there in the color table.\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\n# get the contents of the webpage in text format and store in a variable called data\ndata  = requests.get(url).text\n\n#convert to a soup object\nsoup = BeautifulSoup(data,\"html.parser\")\n\n#find a table in the html webpage via the table tag\ntable = soup.find('table')\n\n#get all rows from the table via the tr tag\nfor row in table.find_all('tr'):\n    #get all columns for each row using the td tag\n    cols = row.find_all('td')\n    #store the value in column3 as color name\n    color_name = cols[2].string\n    #store the color code in column 4 as color_code\n    color_code = cols[3].string\n    print(\"{}--->{}\".format(color_name, color_code))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColor Name--->None\nlightsalmon--->#FFA07A\nsalmon--->#FA8072\ndarksalmon--->#E9967A\nlightcoral--->#F08080\ncoral--->#FF7F50\ntomato--->#FF6347\norangered--->#FF4500\ngold--->#FFD700\norange--->#FFA500\ndarkorange--->#FF8C00\nlightyellow--->#FFFFE0\nlemonchiffon--->#FFFACD\npapayawhip--->#FFEFD5\nmoccasin--->#FFE4B5\npeachpuff--->#FFDAB9\npalegoldenrod--->#EEE8AA\nkhaki--->#F0E68C\ndarkkhaki--->#BDB76B\nyellow--->#FFFF00\nlawngreen--->#7CFC00\nchartreuse--->#7FFF00\nlimegreen--->#32CD32\nlime--->#00FF00\nforestgreen--->#228B22\ngreen--->#008000\npowderblue--->#B0E0E6\nlightblue--->#ADD8E6\nlightskyblue--->#87CEFA\nskyblue--->#87CEEB\ndeepskyblue--->#00BFFF\nlightsteelblue--->#B0C4DE\ndodgerblue--->#1E90FF\n```\n:::\n:::\n\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\n#Get all rows from the table\nfor row in table.find_all('tr'): # in html table row is represented by the tag <tr>\n    # Get all columns in each row.\n    cols = row.find_all('td') # in html a column is represented by the tag <td>\n    color_name = cols[2].string # store the value in column 3 as color_name\n    color_code = cols[3].string # store the value in column 4 as color_code\n    print(\"{}--->{}\".format(color_name,color_code))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColor Name--->None\nlightsalmon--->#FFA07A\nsalmon--->#FA8072\ndarksalmon--->#E9967A\nlightcoral--->#F08080\ncoral--->#FF7F50\ntomato--->#FF6347\norangered--->#FF4500\ngold--->#FFD700\norange--->#FFA500\ndarkorange--->#FF8C00\nlightyellow--->#FFFFE0\nlemonchiffon--->#FFFACD\npapayawhip--->#FFEFD5\nmoccasin--->#FFE4B5\npeachpuff--->#FFDAB9\npalegoldenrod--->#EEE8AA\nkhaki--->#F0E68C\ndarkkhaki--->#BDB76B\nyellow--->#FFFF00\nlawngreen--->#7CFC00\nchartreuse--->#7FFF00\nlimegreen--->#32CD32\nlime--->#00FF00\nforestgreen--->#228B22\ngreen--->#008000\npowderblue--->#B0E0E6\nlightblue--->#ADD8E6\nlightskyblue--->#87CEFA\nskyblue--->#87CEEB\ndeepskyblue--->#00BFFF\nlightsteelblue--->#B0C4DE\ndodgerblue--->#1E90FF\n```\n:::\n:::\n\n\n#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and Pandas\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\nimport pandas as pd \n\n#The below url contains html tables with data about world population.\nurl = \"https://en.wikipedia.org/wiki/World_population\"\n\n#get the contents and convert\ndata = requests.get(url).text\nsoup = BeautifulSoup(data, \"html.parser\")\n\n#find all html tables\ntables = soup.find_all('table')\n\n#see how many tables we got\nlen(tables)\n```\n\n::: {.cell-output .cell-output-display execution_count=314}\n```\n25\n```\n:::\n:::\n\n\nAssume that we are looking for the 10 most densly populated countries table, we can look through the tables list and find the right one we are look for based on the data in each table or we can search for the table name if it is in the table but this option might not always work.\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\nfor index,table in enumerate(tables):\n    if (\"10 most densely populated countries\" in str(table)):\n        table_index = index\nprint(table_index)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5\n```\n:::\n:::\n\n\nSee if you can locate the table name of the table, 10 most densly populated countries, below.\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\n#print(tables[table_index].prettify())\n```\n:::\n\n\nNow, we can extract data \n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\npopulation_data = pd.DataFrame(columns=[\"Rank\", \"Country\", \"Population\", \"Area\", \"Density\"])\n\nfor row in tables[table_index].tbody.find_all(\"tr\"):\n    col = row.find_all(\"td\")\n    if (col != []):\n        rank = col[0].text\n        country = col[1].text\n        population = col[2].text.strip()\n        area = col[3].text.strip()\n        density = col[4].text.strip()\n        population_data = population_data.append({\"Rank\":rank, \"Country\":country, \"Population\":population, \"Area\":area, \"Density\":density}, ignore_index=True)\n\npopulation_data\n```\n\n::: {.cell-output .cell-output-display execution_count=317}\n```{=tex}\n\\begin{tabular}{llllll}\n\\toprule\n{} & Rank &           Country &   Population &     Area & Density \\\\\n\\midrule\n0 &    1 &         Singapore &    5,704,000 &      710 &   8,033 \\\\\n1 &    2 &        Bangladesh &  173,400,000 &  143,998 &   1,204 \\\\\n2 &    3 &  \\textbackslash n Palestine\\textbackslash n\\textbackslash n &    5,266,785 &    6,020 &     847 \\\\\n3 &    4 &           Lebanon &    6,856,000 &   10,452 &     656 \\\\\n4 &    5 &            Taiwan &   23,604,000 &   36,193 &     652 \\\\\n5 &    6 &       South Korea &   51,781,000 &   99,538 &     520 \\\\\n6 &    7 &            Rwanda &   12,374,000 &   26,338 &     470 \\\\\n7 &    8 &             Haiti &   11,578,000 &   27,065 &     428 \\\\\n8 &    9 &       Netherlands &   17,740,000 &   41,526 &     427 \\\\\n9 &   10 &            Israel &    9,570,000 &   22,072 &     434 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n#### Scrape data from HTML tables into a DataFrame using BeautifulSoup and read_html\n\nUsing the same url, data, soup, and tables object as in the last section we can use the read_html function to create a DataFrame.\n\nRemember the table we need is located in tables[table_index]\n\nWe can now use the pandas function read_html and give it the string version of the table as well as the flavor which is the parsing engine bs4.\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\n#pd.read_html(str(tables[5]))\n```\n:::\n\n\nThe function read_html always returns a list of DataFrames so we must pick the one we want out of the list.\n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\npopulation_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]\npopulation_data_read_html\n```\n\n::: {.cell-output .cell-output-display execution_count=319}\n```{=tex}\n\\begin{tabular}{lrlrrr}\n\\toprule\n{} &  Rank &      Country &  Population &  Area(km2) &  Density(pop/km2) \\\\\n\\midrule\n0 &     1 &    Singapore &     5704000 &        710 &              8033 \\\\\n1 &     2 &   Bangladesh &   173400000 &     143998 &              1204 \\\\\n2 &     3 &    Palestine &     5266785 &       6020 &               847 \\\\\n3 &     4 &      Lebanon &     6856000 &      10452 &               656 \\\\\n4 &     5 &       Taiwan &    23604000 &      36193 &               652 \\\\\n5 &     6 &  South Korea &    51781000 &      99538 &               520 \\\\\n6 &     7 &       Rwanda &    12374000 &      26338 &               470 \\\\\n7 &     8 &        Haiti &    11578000 &      27065 &               428 \\\\\n8 &     9 &  Netherlands &    17740000 &      41526 &               427 \\\\\n9 &    10 &       Israel &     9570000 &      22072 &               434 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n#### Scrape data from HTML tables into a dataframe using read_html\n\nWe can also use the read_html function to directly get DataFrames from a url.\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\ndataframe_list = pd.read_html(url)\n\n#check if we find the tables again\nprint(len(dataframe_list))\n\n#get the data we need\ndataframe_list[5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n25\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=320}\n```{=tex}\n\\begin{tabular}{lrlrrr}\n\\toprule\n{} &  Rank &      Country &  Population &  Area(km2) &  Density(pop/km2) \\\\\n\\midrule\n0 &     1 &    Singapore &     5704000 &        710 &              8033 \\\\\n1 &     2 &   Bangladesh &   173400000 &     143998 &              1204 \\\\\n2 &     3 &    Palestine &     5266785 &       6020 &               847 \\\\\n3 &     4 &      Lebanon &     6856000 &      10452 &               656 \\\\\n4 &     5 &       Taiwan &    23604000 &      36193 &               652 \\\\\n5 &     6 &  South Korea &    51781000 &      99538 &               520 \\\\\n6 &     7 &       Rwanda &    12374000 &      26338 &               470 \\\\\n7 &     8 &        Haiti &    11578000 &      27065 &               428 \\\\\n8 &     9 &  Netherlands &    17740000 &      41526 &               427 \\\\\n9 &    10 &       Israel &     9570000 &      22072 &               434 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nWe can also use the match parameter to select the specific table we want. If the table contains a string matching the text it will be read.\n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\npd.read_html(url, match=\"10 most densely populated countries\")[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=321}\n```{=tex}\n\\begin{tabular}{lrlrrr}\n\\toprule\n{} &  Rank &      Country &  Population &  Area(km2) &  Density(pop/km2) \\\\\n\\midrule\n0 &     1 &    Singapore &     5704000 &        710 &              8033 \\\\\n1 &     2 &   Bangladesh &   173400000 &     143998 &              1204 \\\\\n2 &     3 &    Palestine &     5266785 &       6020 &               847 \\\\\n3 &     4 &      Lebanon &     6856000 &      10452 &               656 \\\\\n4 &     5 &       Taiwan &    23604000 &      36193 &               652 \\\\\n5 &     6 &  South Korea &    51781000 &      99538 &               520 \\\\\n6 &     7 &       Rwanda &    12374000 &      26338 &               470 \\\\\n7 &     8 &        Haiti &    11578000 &      27065 &               428 \\\\\n8 &     9 &  Netherlands &    17740000 &      41526 &               427 \\\\\n9 &    10 &       Israel &     9570000 &      22072 &               434 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "Week5_API_and_Data_collection_files/figure-pdf"
    ],
    "filters": []
  }
}