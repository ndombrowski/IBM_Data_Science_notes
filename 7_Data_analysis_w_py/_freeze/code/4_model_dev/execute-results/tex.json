{
  "hash": "b3525a2f5beaae19b9b16e4b6702be4e",
  "result": {
    "markdown": "# Model Development in python\n\n## Introduction\n\n- Model : A mathematical equation used to predict a value given one or more values\n- Relating one or more independent variables to dependent variables. For example we can input a cars highway-mpg as independent variable (ore feature). The output of the model or the dependent variable is the predicted price.\n- The more relevant data we have, the more accurate a model usually is. For example, we can input multiple independent variables (highway-mpg, engine-size, etc) and therefore our model might predict a more accurate price for a car.\n\nSome questions we want to ask in this module\n\nDo I know if the dealer is offering fair value for my trade-in?\nDo I know if I put a fair value on my car?\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#setup general libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#setup lib for linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#model vis\nimport seaborn as sns\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#read in example data\ndf = pd.read_csv(\"../data/automobileEDA.csv.1\")\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=tex}\n\\begin{tabular}{lrrllllllrrrrrllrlrrrrrrrrrlrr}\n\\toprule\n{} &  symboling &  normalized-losses &         make & aspiration & num-of-doors &   body-style & drive-wheels & engine-location &  wheel-base &    length &     width &  height &  curb-weight & engine-type & num-of-cylinders &  engine-size & fuel-system &  bore &  stroke &  compression-ratio &  horsepower &  peak-rpm &  city-mpg &  highway-mpg &    price &  city-L/100km & horsepower-binned &  diesel &  gas \\\\\n\\midrule\n0 &          3 &                122 &  alfa-romero &        std &          two &  convertible &          rwd &           front &        88.6 &  0.811148 &  0.890278 &    48.8 &         2548 &        dohc &             four &          130 &        mpfi &  3.47 &    2.68 &                9.0 &       111.0 &    5000.0 &        21 &           27 &  13495.0 &     11.190476 &            Medium &       0 &    1 \\\\\n1 &          3 &                122 &  alfa-romero &        std &          two &  convertible &          rwd &           front &        88.6 &  0.811148 &  0.890278 &    48.8 &         2548 &        dohc &             four &          130 &        mpfi &  3.47 &    2.68 &                9.0 &       111.0 &    5000.0 &        21 &           27 &  16500.0 &     11.190476 &            Medium &       0 &    1 \\\\\n2 &          1 &                122 &  alfa-romero &        std &          two &    hatchback &          rwd &           front &        94.5 &  0.822681 &  0.909722 &    52.4 &         2823 &        ohcv &              six &          152 &        mpfi &  2.68 &    3.47 &                9.0 &       154.0 &    5000.0 &        19 &           26 &  16500.0 &     12.368421 &            Medium &       0 &    1 \\\\\n3 &          2 &                164 &         audi &        std &         four &        sedan &          fwd &           front &        99.8 &  0.848630 &  0.919444 &    54.3 &         2337 &         ohc &             four &          109 &        mpfi &  3.19 &    3.40 &               10.0 &       102.0 &    5500.0 &        24 &           30 &  13950.0 &      9.791667 &            Medium &       0 &    1 \\\\\n4 &          2 &                164 &         audi &        std &         four &        sedan &          4wd &           front &        99.4 &  0.848630 &  0.922222 &    54.3 &         2824 &         ohc &             five &          136 &        mpfi &  3.19 &    3.40 &                8.0 &       115.0 &    5500.0 &        18 &           22 &  17450.0 &     13.055556 &            Medium &       0 &    1 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n## Linear Regression and Multiple Linear Regression\n\n**Linear regression** will refer to one independent variable to make a prediction.\n**Multiple linear regression** will refer to multiple independent variables to make a prediction.\n\n\n### Simple linear regression (SLR)\n\n- A method to help us understand the relationship between two variables:\n    - The predictor (independent) variable **x** and \n    - the target (dependent) variable **y**\n\nThe result of Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable.\n\n$\\begin{aligned}\n    y = b_0 + b_1x\\\\\n\\end{aligned}$  \n\nb<sub>o</sub>: the intercept of the regression line, in other words: the value of Y when X is 0\nb<sub>1</sub>:  the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit\n\nIf we assume there is a linear relationship between the highway-mpg and the price, we can use this relationship to formulate a model to determine the price of the car. I.e. for a given car for which we know the price, we can check the highway-mpg in the manual. If the highway miles per gallon is 20, we can input this value into the model to obtain a prediction of $22,000. \n\nIn order to determine the line, we take data points from our data set. We then use these training points to fit our model. The results of the training points are the parameters, b<sub>o</sub> and b<sub>1</sub>.\n\nIn many cases, many factors influence how much people pay for a car. For example, make or how old the car is. In this model, this uncertainty is taken into account by assuming a small random value is added to the point on the line. This is called **noise**.\n\nIn short:\n\n- We have a set of training points. \n- We use these training points to fit or train the model and get parameters. \n- We then use these parameters in the model. \n- We now have a model. \n- We use the hat on the y to denote the model is an estimate. \n- We can use this model to predict values that we haven't seen.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#create a linear regression object\nlm = LinearRegression()\nlm\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n```\n:::\n:::\n\n\n**Question**: How could \"highway-mpg\" help us predict car price?\n\nFor this example, we want to look at how highway-mpg can help us predict car price. Using simple linear regression, we will create a linear function with \"highway-mpg\" as the predictor variable and the \"price\" as the response variable.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#set the predictor variable (X), highway-mpg , and response variable (Y), price\nX = df[['highway-mpg']]\nY = df[['price']]\n\n#fit the linear model using highway-mpg\nlm.fit(X,Y)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#output a prediction\nYhat = lm.predict(X)\nYhat[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[16236.50464347],\n       [16236.50464347],\n       [17058.23802179],\n       [13771.3045085 ],\n       [20345.17153508]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#What is the value of the intercept (a)?\nlm.intercept_\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([38423.30585816])\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#What is the value of the slope?\nlm.coef_\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[-821.73337832]])\n```\n:::\n:::\n\n\nWith these values we get a final linear model with the structure:\n\n**Price** = 38423.31 -821.73 * **highway-mpg**\n\n\n\n### Multiple linear regression (MLR)\n\nWhat if we want to predict car price using more than one variable?\n\nIf we want to use more variables in our model to predict car price, we can use Multiple Linear Regression. Multiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between:\n\n- One continuous target (Y) variable\n- Two or more predictor (X) variables\n\nIf we would have four predictor variables, we would have something like this\n\n$\\begin{aligned}\n    y = b_0 + b_1x_1+b_2x_2+b_3x_3+b_4x_4\\\\\n\\end{aligned}$  \n\nb<sub>o</sub>: the intercept  (x=0) <br>\nb<sub>1</sub>: the coefficient or parameter of x<sub>1</sub> <br>\nb<sub>2</sub>: the coefficient or parameter of x<sub>2</sub> and so on\n\nFrom the previous section we know that other good predictors of price could be:\n\n- Horsepower\n- Curb-weight\n- Engine-size\n- Highway-mpg\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#use the variables above as predictor variables\nZ = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n\n#fit the linear model\nlm = LinearRegression()\nlm.fit(Z, df[['price']])\n\n#get the intercept\nprint(lm.intercept_)\n\n#get the coefficients\nprint(lm.coef_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[-15806.62462633]\n[[53.49574423  4.70770099 81.53026382 36.05748882]]\n```\n:::\n:::\n\n\nThe final model we get with these values is:\n\n**Price** = -15806.62 + 53.50 * **horsepower** + 4.71 * **curb-weight** + 81.52 * **engine-size** + 36.01 * **highway-mpg**\n\n\n\n## Model Evaluation using Visualization\n\nNow that we've developed some models, how do we evaluate our models and choose the best one? One way to do this is by using a visualization.\n\n### Regression plots\n\nWhen it comes to simple linear regression, an excellent way to visualize the fit of our model is by using regression plots.\n\nThis plot will show a combination of a scattered data points (a scatterplot), as well as the fitted linear regression line going through the data. This will give us a reasonable estimate of the relationship between the two variables, the strength of the correlation, as well as the direction (positive or negative correlation).\n\nGive good estimates of:\n\n- Relationships between 2 variables\n- The strength of the correlation\n- The direction of the relatioship (positive or negative)\n\nA regression plot shows us:\n\n- The horizontal axis is the independent variable\n- The vertical axis is the dependent variable \n- Each point represents a different target point\n- The fitted line represents the predicted value\n\nLet's visualize highway-mpg as potential predictor variable of price:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nwidth = 6\nheight = 5\n\nplt.figure(figsize=(width, height))\nsns.regplot(x = \"highway-mpg\", y = \"price\", data = df)\n#plt.ylim((0,))\n\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-10-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can see from this plot that price is negatively correlated to highway-mpg since the regression slope is negative.\n\nOne thing to keep in mind when looking at a regression plot is to pay attention to how scattered the data points are around the regression line. This will give you a good indication of the variance of the data and whether a linear model would be the best fit or not. If the data is too far off from the line, this linear model might not be the best model for this data.\n\nLet's compare this plot to the regression plot of \"peak-rpm\".\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize=(width, height))\nsns.regplot(x=\"peak-rpm\", y=\"price\", data=df)\n#plt.ylim((0,))\n\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-11-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nComparing the regression plot of \"peak-rpm\" and \"highway-mpg\", we see that the points for \"highway-mpg\" are much closer to the generated line and, on average, decrease. The points for \"peak-rpm\" have more spread around the predicted line and it is much harder to determine if the points are decreasing or increasing as the \"peak-rpm\" increases.\n\n**Given the regression plots above, is \"peak-rpm\" or \"highway-mpg\" more strongly correlated with \"price\"?**\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndf[['price', 'highway-mpg', 'peak-rpm']].corr()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=tex}\n\\begin{tabular}{lrrr}\n\\toprule\n{} &     price &  highway-mpg &  peak-rpm \\\\\n\\midrule\nprice       &  1.000000 &    -0.704692 & -0.101616 \\\\\nhighway-mpg & -0.704692 &     1.000000 & -0.058598 \\\\\npeak-rpm    & -0.101616 &    -0.058598 &  1.000000 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nWe see that the variable \"highway-mpg\" has a stronger correlation with \"price\", it is approximate -0.704692 compared to \"peak-rpm\" which is approximate -0.101616. You can verify it using the following command:\n \n \n\n### Residual plot\n\nA good way to visualize the variance of the data is to use a residual plot.\n\n**Residual** : The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.\n\n**Residual plot**: A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.\n\n**What to look for**: We look at the spread of the residuals --> If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.\n\n\n#### Single linear regression\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nwidth = 6\nheight = 5\n\nplt.figure(figsize=(width, height))\nsns.residplot(x = df['highway-mpg'], y = df['price'])\n\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-13-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can see from this residual plot that the residuals are not randomly spread around the x-axis, leading us to believe that maybe a non-linear model is more appropriate for this data.\n\n\n### Distribution plots\n\nHow do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because you can't visualize it with regression or residual plot.\n\nOne way to look at the fit of the model is by looking at the distribution plot. We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n#make a prediction using 4 variables\nY_hat = lm.predict(Z)\n\n#prepare the plot\nplt.figure(figsize=(width, height))\n\nax1 = sns.distplot(df['price'], hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(Y_hat, hist = False, color = \"b\", label = \"Fitted Values\", ax = ax1)\n\nplt.title(\"Actual vs fitted values for price\")\nplt.xlabel(\"Price (in dollars\")\nplt.ylabel(\"Proportion of cars\")\n\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-14-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can see that the fitted values are reasonably close to the actual values since the two distributions overlap a bit. However, there is definitely some room for improvement.\n\n\n\n## Polynomial Regression and Pipelines\n\nWhat do we do when a linear model is not the best fit for our data?\n\n### Polynomial Regression\n\nPolynomial regression is a special case of the general linear regression or multiple linear regression models. We get non-linear relationships by squaring or setting higher-order terms of the predictor variables. This method is beneficial for describing curvilinear relationships.\n\nCurvilinear relationship: what you get by squaring or setting higher order terms of the predictor variables in the model transforming the data. The model can be quadratic, which means that the predictor variable in the model is squared.\n\n<center><b>Quadratic - 2nd Order</b></center>\n$$\nYhat = a + b_1 X +b_2 X^2 \n$$\n\n<center><b>Cubic - 3rd Order</b></center>\n$$\nYhat = a + b_1 X +b_2 X^2 +b_3 X^3\\\\\\\\\\\\\\\\\\\\\n$$\n\n<center><b>Higher-Order</b>:</center>\n$$\nY = a + b_1 X +b_2 X^2 +b_3 X^3 ....\\\\\\\\\n$$\n\nWe saw earlier that a linear model did not provide the best fit while using \"highway-mpg\" as the predictor variable. Let's see if we can try fitting a polynomial model to the data instead.\n\nWe will use the following function to plot the data:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('Price of Cars')\n\n    plt.show()\n    plt.close()\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n#get the variables\nx = df['highway-mpg']\ny = df['price']\n```\n:::\n\n\nLet's fit the polynomial using the function polyfit, then use the function poly1d to display the polynomial function\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n#use a polynomal of the 3rd order (cubic)\nf = np.polyfit(x, y, 3)\np = np.poly1d(f)\nprint(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        3         2\n-1.557 x + 204.8 x - 8965 x + 1.379e+05\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n#plot \nPlotPolly(p, x, y, 'highway-mpg')\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-18-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nnp.polyfit(x, y, 3)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([-1.55663829e+00,  2.04754306e+02, -8.96543312e+03,  1.37923594e+05])\n```\n:::\n:::\n\n\nWe can already see from plotting that this polynomial model performs better than the linear model. This is because the generated polynomial function \"hits\" more of the data points.\n\nThe analytical expression for Multivariate Polynomial function gets complicated. For example, the expression for a second-order (degree=2) polynomial with two variables is given by:\n\n$$\nYhat = a + b\\_1 X\\_1 +b\\_2 X\\_2 +b\\_3 X\\_1 X\\_2+b\\_4 X\\_1^2+b\\_5 X\\_2^2\n$$\n\nWe can perform a polynomial transform on multiple features. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n#import module\nfrom sklearn.preprocessing import PolynomialFeatures\n```\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n#We create a PolynomialFeatures object of degree 2\npr=PolynomialFeatures(degree=2)\n\n#transform data\nZ_pr=pr.fit_transform(Z)\n\n#view data before transformation\nprint(Z.shape)\n\n#view data after the transformation\nprint(Z_pr.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(201, 4)\n(201, 15)\n```\n:::\n:::\n\n\n### Pipelines\n\nWe can simplify our code by using a pipeline library. \n\nThere are many steps to getting a prediction. For example, normalization, polynomial transform, and linear regression. \n\nWe simplify the process using a pipeline, which sequentially performs a series of transformations and were the last step carries out a prediction. \n\nA pipeline could look as follows:\n\n- First we import all the modules we need\n- then we import the library pipeline\n- We create a list of tuples, the first element in the tuple contains the name of the estimator model. The second element contains model constructor. \n- We input the list in the pipeline constructor. We now have a pipeline object. \n- We can train the pipeline by applying the train method to the pipeline object. \n- We can also produce a prediction as well. \n- The method normalizes the data, performs a polynomial transform, then outputs a prediction.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n#import libs\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n\nWe create the pipeline by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nInput=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]\n```\n:::\n\n\nWe input the list as an argument to the pipeline constructor:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\npipe=Pipeline(Input)\npipe\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nPipeline(memory=None,\n         steps=[('scale',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('polynomial',\n                 PolynomialFeatures(degree=2, include_bias=False,\n                                    interaction_only=False, order='C')),\n                ('model',\n                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n                                  normalize=False))],\n         verbose=False)\n```\n:::\n:::\n\n\nFirst, we convert the data type Z to type float to avoid conversion warnings that may appear as a result of StandardScaler taking float inputs.\n\nThen, we can normalize the data, perform a transform and fit the model simultaneously.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nZ = Z.astype(float)\npipe.fit(Z,y)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\nPipeline(memory=None,\n         steps=[('scale',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('polynomial',\n                 PolynomialFeatures(degree=2, include_bias=False,\n                                    interaction_only=False, order='C')),\n                ('model',\n                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n                                  normalize=False))],\n         verbose=False)\n```\n:::\n:::\n\n\nSimilarly, we can normalize the data, perform a transform and produce a prediction simultaneously.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nypipe=pipe.predict(Z)\nypipe[0:4]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([13102.74784201, 13102.74784201, 18225.54572197, 10390.29636555])\n```\n:::\n:::\n\n\n## Measures for In-Sample Evaluation\n\nThese measures are a way to numerically determine how good the model fits on our data. \n\nTwo important measures that we often use to determine the fit of a model are: \n\n- Mean Square Error (MSE): To measure the MSE, we find the difference between the actual value y and the predicted value then then square it. In this case, the actual value is 150; the predicted value is 50. Subtracting these points we get 100. We then square the number. We then take the Mean or average of all the errors by adding then all together and dividing by the number of samples. \n\n- R-squared: R-squared is also called the coefficient of determination. It’s a measure to determine how close the data is to the fitted regression line. So how close is our actual data to our estimated model?\n\n### Example R-square for SLR\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n#highway_mpg_fit\nlm.fit(X, Y)\n\n# Find the R^2\nprint('The R-square is: ', lm.score(X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe R-square is:  0.4965911884339176\n```\n:::\n:::\n\n\nWe can say that ~49.659% of the variation of the price is explained by this simple linear model \"horsepower_fit\".\n\nLet's calculate the MSE:\n\nWe can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nYhat=lm.predict(X)\nprint('The output of the first four predicted value is: ', Yhat[0:4])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe output of the first four predicted value is:  [[16236.50464347]\n [16236.50464347]\n [17058.23802179]\n [13771.3045085 ]]\n```\n:::\n:::\n\n\nLet's import the function mean_squared_error from the module metrics:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nmse = mean_squared_error(df['price'], Yhat)\nprint('The mean square error of price and predicted value is: ', mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean square error of price and predicted value is:  31635042.944639888\n```\n:::\n:::\n\n\n### Example R-square for MLR\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# fit the model \nlm.fit(Z, df['price'])\n\n# Find the R^2\nprint('The R-square is: ', lm.score(Z, df['price']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe R-square is:  0.8093562806577458\n```\n:::\n:::\n\n\nWe can say that ~80.896 % of the variation of price is explained by this multiple linear regression \"multi_fit\".\n\nLet's calculate the MSE.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# We produce a prediction: \nY_predict_multifit = lm.predict(Z)\n\n#We compare the predicted results with the actual results:\nprint('The mean square error of price and predicted value using multifit is: ', mean_squared_error(df['price'], Y_predict_multifit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean square error of price and predicted value using multifit is:  11980366.870726489\n```\n:::\n:::\n\n\n### Example R-square for polynomial fit\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n# import the function r2_score from the module metrics as we are using a different function\nfrom sklearn.metrics import r2_score\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# We apply the function to get the value of R^2:\nr_squared = r2_score(y, p(x))\nprint('The R-square value is: ', r_squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe R-square value is:  0.6741946663906513\n```\n:::\n:::\n\n\nWe can say that ~67.419 % of the variation of price is explained by this polynomial fit.\n\nWe can also calculate the MSE:\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nmean_squared_error(df['price'], p(x))\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n20474146.42636125\n```\n:::\n:::\n\n\n## Prediction and Decision Making\n\nHow can we make sure our model is correct?\n\nTo determine the best fit, we look at a combination of:\n\n- do our model results make sense\n- visualize the data\n- use numerical measures for evaluation\n- compare models\n\n\n### Prediction\n\nIn the previous section, we trained the model using the method fit. Now we will use the method predict to produce a prediction. Lets import pyplot for plotting; we will also be using some functions from numpy.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n#create a new input \nnew_input=np.arange(1, 100, 1).reshape(-1, 1)\n\n#fit the model\nlm.fit(X,Y)\nlm\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n```\n:::\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n#produce a prediction\nyhat=lm.predict(new_input)\nyhat[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\narray([[37601.57247984],\n       [36779.83910151],\n       [35958.10572319],\n       [35136.37234487],\n       [34314.63896655]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n#plot data\nplt.plot(new_input, yhat)\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](4_model_dev_files/figure-pdf/cell-38-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Decision Making: Determining a Good Model Fit¶\n\nNow that we have visualized the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?\n\n- What is a good R-squared value?\n\nWhen comparing models, the model with the higher R-squared value is a better fit for the data.\n\n- What is a good MSE?\n\nWhen comparing models, the model with the smallest MSE value is a better fit for the data.\n\n**Let's take a look at the values for the different models.**\n\nSimple Linear Regression: Using Highway-mpg as a Predictor Variable of Price.\n\n- R-squared: 0.49659118843391759\n- MSE: 3.16 x10^7\n\nMultiple Linear Regression: Using Horsepower, Curb-weight, Engine-size, and Highway-mpg as Predictor Variables of Price.\n\n- R-squared: 0.80896354913783497\n- MSE: 1.2 x10^7\n\nPolynomial Fit: Using Highway-mpg as a Predictor Variable of Price.\n\n- R-squared: 0.6741946663906514\n- MSE: 2.05 x 10^7\n\n\n####  Simple Linear Regression Model (SLR) vs Multiple Linear Regression Model (MLR)\n\nUsually, the more variables you have, the better your model is at predicting, but this is not always true. Sometimes you may not have enough data, you may run into numerical problems, or many of the variables may not be useful and even act as noise. As a result, you should always check the MSE and R^2.\n\nIn order to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.\n\n- MSE: The MSE of SLR is 3.16x10^7 while MLR has an MSE of 1.2 x10^7. The MSE of MLR is much smaller.\n0 R-squared: In this case, we can also see that there is a big difference between the R-squared of the SLR and the R-squared of the MLR. The R-squared for the SLR (~0.497) is very small compared to the R-squared for the MLR (~0.809).\n\nThis R-squared in combination with the MSE show that MLR seems like the better model fit in this case compared to SLR.\n\n\n#### Simple Linear Model (SLR) vs. Polynomial Fit\n\n- MSE: We can see that Polynomial Fit brought down the MSE, since this MSE is smaller than the one from the SLR.\n- R-squared: The R-squared for the Polynomial Fit is larger than the R-squared for the SLR, so the Polynomial Fit also brought up the R-squared quite a bit.\n\nSince the Polynomial Fit resulted in a lower MSE and a higher R-squared, we can conclude that this was a better fit model than the simple linear regression for predicting \"price\" with \"highway-mpg\" as a predictor variable.\n\n#### Multiple Linear Regression (MLR) vs. Polynomial Fit¶\n\n- MSE: The MSE for the MLR is smaller than the MSE for the Polynomial Fit.\n- R-squared: The R-squared for the MLR is also much larger than for the Polynomial Fit.\n\n#### Conclusion\n\nComparing these three models, we conclude that the MLR model is the best model to be able to predict price from our dataset. This result makes sense since we have 27 variables in total and we know that more than one of those variables are potential predictors of the final car price.\n\n",
    "supporting": [
      "4_model_dev_files/figure-pdf"
    ],
    "filters": []
  }
}