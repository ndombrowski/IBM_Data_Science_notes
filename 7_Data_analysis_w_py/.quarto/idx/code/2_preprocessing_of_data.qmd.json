{"title":"Pre-processing of data in python","markdown":{"headingText":"Pre-processing of data in python","containsRefs":false,"markdown":"\n## Introduction\n\nData pre-processing is a necessary step in data analysis. It is the process of converting or mapping data from one raw form into another format to make it ready for further analysis. \n\nData pre-processing is often called data cleaning or data wrangling.\n\n- Identifying and handle missing values. A missing value condition occurs whenever a data entry is left empty.\n- Data formatting. Data from different sources maybe in various formats, in different units, or in various conventions. \n- Data normalization (centering/scaling). Different columns of numerical data may have very different ranges and direct comparison is often not meaningful. Normalization is a way to bring all data into a similar range for more useful comparison.\n- Data binning. Binning creates bigger categories from a set of numerical values. It is particularly useful for comparison between groups of data.\n- Turning categorical values to numeric variables to make statistical modeling easier\n\nLet's start with loading our test data\n\n\nYou can find the \"Automobile Dataset\" from the following link: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data. We will be using this dataset throughout this course.\n\n```{python}\nimport sys\nsys.executable\n```\n\n```{python}\n#load libs\nimport pandas as pd\nimport numpy as np\n\n#get the data url\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\"\n\n#add a header\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n\n#download data\ndf = pd.read_csv(url, names = headers)\ndf.head(5)\n```\n\n\n## Dealing with missing values\n\n- A missing value condition occurs whenever a data entry is left empty\n- Can be represented as: ?, N/A, 0 or a blank cell\n\nTypical options to consider to deal with missing data:\n\n- Check with the collection source to find missing values\n- Remove data were missing value is found, here we can do either drop the whole row or column (decide on what has the least amount of impact)\n    - drop the variable\n    - drop the data entry\n- Replace the missing values\n    - replace with the average (of similar data points)\n    - replace it by frequency\n    - replace it based on other functions\n- Leave it as missing data\n\nNext, let's first convert ? to NaN\n\n```{python}\n# replace \"?\" to NaN\ndf.replace(\"?\", np.nan, inplace = True)\ndf.head(5)\n```\n\n### Using `dropna()`\n\nTo remove data that contains missing values Panda's library has a built-in method called dropna. Essentially, with the dropna method, you can choose to drop rows or columns that contain missing values like NaN.\n\n- axis = 0 --> drop the entire row (default)\n- axis = 1 --> drop the entire column\n- inpalce = True --> modification is done on the dataset directly\n\n```\ndataframes.dropna()\n```\n\nIf we want to remove rows based on a specific column\n\n```\ndataframes.dropna(subset = ['price'], axis = 0, inplace = True)\n```\n\n\n### Replace missing values using `replace()`\n\n```\ndataframe.replace(missing_value, new_value)\n```\n\nReplace a value with the mean of a column:\n\n```\nmean = df['prices'].mean()\ndf['prices'].replace(np.nan, mean)\n```\n\n\n### Practical\n\n#### Identify missing data\n\nNow, let's find out if we have any missing data.\n\nThe missing values are converted by default. We use the following functions to identify these missing values. There are two methods to detect missing data:\n\n- .isnull()\n- .notnull()\n\nThe output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. \"True\" means the value is a missing value while \"False\" means the value is not a missing value\n\n```{python}\nmissing_data = df.isnull()\nmissing_data.head(5)\n```\n\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value and \"False\" means the value is present in the dataset. In the body of the for loop the method \".value_counts()\" counts the number of \"True\" values.\n\n```{python}\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\") \n```\n\nBased on the summary above, each column has 205 rows of data and seven of the columns containing missing data:\n\n\"normalized-losses\": 41 missing data\n\"num-of-doors\": 2 missing data\n\"bore\": 4 missing data\n\"stroke\" : 4 missing data\n\"horsepower\": 2 missing data\n\"peak-rpm\": 2 missing data\n\"price\": 4 missing data\n\nNow, we have to decide what to do with this missing data. Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:\n\nReplace by mean:\n\n- \"normalized-losses\": 41 missing data, replace them with mean\n- \"stroke\": 4 missing data, replace them with mean\n- \"bore\": 4 missing data, replace them with mean\n- \"horsepower\": 2 missing data, replace them with mean\n- \"peak-rpm\": 2 missing data, replace them with mean\n\nReplace by frequency:\n\n- \"num-of-doors\": 2 missing data, replace them with \"four\".\n    - Reason: 84% sedans is four doors. Since four doors is most frequent, it is most likely to occur\n\nDrop the whole row:\n\n- \"price\": 4 missing data, simply delete the whole row\n    - Reason: price is what we want to predict. Any data entry without price data cannot be used for prediction; therefore any row now without price data is not useful to us\n\n#### Deal with the \"normalized-losses\" column\n\n```{python}\navg_norm_loss = df['normalized-losses'].astype('float').mean(axis = 0)\nprint(\"Average of normalized losses:\", avg_norm_loss)\n```\n\n```{python}\ndf['normalized-losses'].replace(np.nan, avg_norm_loss, inplace = True)\n```\n\n\n\n#### Deal with the \"bore\" column¶\n\n```{python}\navg_bore = df['bore'].astype('float').mean(axis = 0)\nprint(\"Average of bore:\", avg_bore)\n```\n\n```{python}\ndf['bore'].replace(np.nan, avg_bore, inplace = True)\n```\n\n\n#### Deal with the \"stroke\" column\n\n```{python}\navg_bore = df['bore'].astype('float').mean(axis = 0)\nprint(\"Average of bore:\", avg_bore)\n\ndf['bore'].replace(np.nan, avg_bore, inplace = True)\n```\n\n\n#### Deal with the \"horsepower\" column\n\n```{python}\navg_horsepower = df['horsepower'].astype('float').mean(axis = 0)\nprint(\"Average of horsepower:\", avg_horsepower)\n\ndf['horsepower'].replace(np.nan, avg_horsepower, inplace = True)\n```\n\n\n\n#### Deal with the \"peak-rpm\" column\n\n```{python}\navg_peakrpm = df['peak-rpm'].astype('float').mean(axis = 0)\nprint(\"Average of peak-rpm:\", avg_peakrpm)\n\ndf['peak-rpm'].replace(np.nan, avg_peakrpm, inplace = True)\n```\n\n\n#### Replace missing door values with frequency\n\nTo see which values are present in a particular column, we can use the `.value_counts()` method:\n\n```{python}\ndf['num-of-doors'].value_counts()\n```\n\nWe can see that four doors are the most common type. We can also use the \".idxmax()\" method to calculate the most common type automatically:\n\n```{python}\ndoors_to_replace = df['num-of-doors'].value_counts().idxmax()\ndoors_to_replace\n```\n\nNow we can replace our missing values:\n\n```{python}\ndf['num-of-doors'].replace(np.nan,doors_to_replace, inplace = True )\n```\n\n#### Drop rows for the price column\n\n```{python}\n#drop nan\ndf.dropna(subset = ['price'], axis = 0 , inplace = True)\n\n#reset index, because we dropped two ros\ndf.reset_index(drop = True, inplace = True)\n\n#view df\ndf.head()\n```\n\n\n## Data formatting\n\nData is usually collected from different places by different people which may be stored in different formats. Data formatting means bringing data into a common standard of expression that allows users to make meaningful comparisons.  As a part of dataset cleaning, data formatting ensures the data is consistent and easily understandable. \n\nFor example, people may use different expressions to represent New York City, such as uppercase N uppercase Y, uppercase N lowercase y, uppercase N uppercase Y and New York. \n\n\nFormatted data is:\n\n- more clear\n- easier to aggregate\n- easier to compare\n\n\n### Incorrect data types\n\nSometimes the wrong data type is assigned to a feature. For example a number might be stored in an object, thus we could not use the mean function, without correcting this.\n\nTo identify a datatype:\n\n```\ndf.dtypes()\n```\n\nTo convert dataframes:\n\n```\ndf.astype()\n```\n\n\n\n### Practical\n\n#### Deal with incorrect data types\n\nThe last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other).\n\nIn Pandas, we use:\n\n- .dtype() to check the data type\n- .astype() to change the data type\n\n\nLet's check the data types for each column:\n\n```{python}\ndf.dtypes\n```\n\nAs we can see above, some columns are not of the correct data type. Numerical variables should have type 'float' or 'int', and variables with strings such as categories should have type 'object'. \n\nFor example, 'bore' and 'stroke' variables are numerical values that describe the engines, so we should expect them to be of the type 'float' or 'int'; however, they are shown as type 'object'. We have to convert data types into a proper format for each column using the \"astype()\" method.\n\n```{python}\ndf[['bore', 'stroke']] = df[['bore', 'stroke']].astype('float')\ndf[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")\ndf[[\"price\"]] = df[[\"price\"]].astype(\"float\")\ndf[[\"peak-rpm\"]] = df[[\"peak-rpm\"]].astype(\"float\")\n\n#check if that worked \ndf.dtypes\n```\n\n\n\n\n## Data normalization\n\n- Uniforms the features value with different ranges: if data are in different ranges, such as age 20-40 and income 10000 to 50000, are hard to compare and income will influence the result more but it might not be more important as a data predictor.\n- We could normalize all values to be in a range from 0-1\n\nThere are several approches:\n\n- **Simple feature scaling**: divides each value by the maximum value for that feature. This makes the new values range between zero and one.\n    \n```\ndf['length'] = df['length'] /df['length'].max()\n```\n\n- **Min-max**: each value X_old subtract it from the minimum value of that feature, then divides by the range of that feature\n\n```\ndf['length'] = (df['length'] - df['length'].min())/\n                (df['length'].max()-df['length'].min())\n```\n\n- **Z-score**: for each value you subtract the mu which is the average of the feature, and then divide by the standard deviation sigma.\n\n```\ndf['length'] = (df['length']-df['length'],mean()).df['length'].std()\n```\n\n### Practical\n\n#### Data Standardization\n\nIn our dataset, the fuel consumption columns \"city-mpg\" and \"highway-mpg\" are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accepts the fuel consumption with L/100km standard.\n\nWe will need to apply data transformation to transform mpg into L/100km.\n\nThe formula for unit conversion is:\n\nL/100km = 235 / mpg\n\nWe can do many mathematical operations directly in Pandas.\n\n```{python}\n# Convert mpg to L/100km by mathematical operation (235 divided by mpg)\ndf['city-L/100km'] = 235/df['city-mpg']\n\n#view transformed data\ndf.head()\n```\n\nNext, transform mpg to L/100km in the column of \"highway-mpg\" and change the name of column to \"highway-L/100km\".\n\n```{python}\n# Convert mpg to L/100km by mathematical operation (235 divided by mpg)\ndf['highway-L/100km'] = 235/df['highway-mpg']\n\n#view transformed data\ndf.head()\n```\n\n#### Data normalization\n\nTo demonstrate normalization, let's say we want to scale the columns \"length\", \"width\" and \"height\".\n\nTarget: would like to normalize those variables so their value ranges from 0 to 1\nApproach: replace original value by (original value)/(maximum value)\n\n```{python}\n# replace (original value) by (original value)/(maximum value)\ndf['length'] = df['length']/df['length'].max()\ndf['width'] = df['width']/df['width'].max()\ndf['height'] = df['height']/df['height'].max()\n\n\ndf[['length','width', 'height']].head()\n```\n\n\n\n## Binning\n\n- Binning: Grouping values into bins, i.e. we can bin “age” into [0 to 5], [6 to 10], [11 to 15] and so on\n- Can sometimes increase the accuracy of models\n- Converts numeric into categorical variables, i.e. the different prices can be grouped into low, medium and high\n- Group a set of numerical values into a set of bins\n\nWe can use the numpy function “linspace” to return the array “bins” that contains 4 equally spaced numbers over the specified interval of the price. We create a list “group_names “ that contains the different bin names. We use the pandas function ”cut” to segment and sort the data values into bins.\n\n```\nbin = np.linspace(min(df['price']),max(df['price']),4)\ngroup_names = ['Low', 'Medium', 'High']\ndf['price_binned'] = pd.cut(df['price'], bins, labels = group_names, include_lowest = True)\n```\n\nThe distribution of data can be visualized using histograms.\n\n### Practical\n\n#### Defining bins\n\nIn our dataset, \"horsepower\" is a real valued variable ranging from 48 to 288 and it has 59 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three ‘bins' to simplify analysis?\n\n```{python}\n#check data format\nprint(df['horsepower'].dtypes)\n\n#convert the data to the correct format\ndf['horsepower'] = df['horsepower'].astype(int, copy=True)\n```\n\nLet's plot the histogram of horsepower to see what the distribution of horsepower looks like.\n\n```{python}\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#define what we want to plot\nplt.hist(df[\"horsepower\"])\n\n#set axis labels\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\n#plot\nplt.show()\nplt.close()\n```\n\n\nWe would like 3 bins of equal size bandwidth so we use numpy's linspace(start_value, end_value, numbers_generated function.\n\n- Since we want to include the minimum value of horsepower, we want to set start_value = min(df[\"horsepower\"]).\n- Since we want to include the maximum value of horsepower, we want to set end_value = max(df[\"horsepower\"]).\n-  Since we are building 3 bins of equal length, there should be 4 dividers, so numbers_generated = 4.\n\nWe build a bin array with a minimum value to a maximum value by using the bandwidth calculated above. The values will determine when one bin ends and another begins.\n\n```{python}\nbins = np.linspace(min(df['horsepower']), max(df['horsepower']),4)\nbins\n```\n\nWe set group names:\n\n```{python}\ngroup_names = ['Low' , 'Medium', 'High']\n```\n\n\nWe apply the function \"cut\" to determine what each value of df['horsepower'] belongs to.\n\n```{python}\ndf['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels = group_names, include_lowest = True)\ndf[['horsepower', 'horsepower-binned']].head(10)\n```\nLet's see the number of vehicles in each bin:\n\n```{python}\ndf['horsepower-binned'].value_counts()\n```\n\nLet's plot the distribution of each bin:\n\n```{python}\n#define the bargraph\nplt.bar(group_names, df['horsepower-binned'].value_counts())\n\n#set labels\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\n#print\nplt.show()\nplt.close()\n```\n\n#### Visualizing bins\n\n```{python}\n# draw historgram of attribute \"horsepower\" with bins = 3\nplt.hist(df[\"horsepower\"], bins = 3)\n\n# set x/y labels and plot title\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\nplt.show()\nplt.close()\n```\n\n\n## Turning categorical variables into quantitative variables\n\n- Most statistical models cannot take in objects or strings as input and for model training only take the numbers as inputs. \n- We encode the values by adding new features corresponding to each unique element in the original feature we would like to encode. I.e if we have two unique values, gas and diesel, we create two new features and assign 0 and 1 to each category. This technique is also called one-hot encoding.\n- `pd.get_dummies()` method converts categorical variables to dummy variables, such as 0 and 1\n\nIndicator value: An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning.\n\nWe use indicator variables so we can use categorical variables for regression analysis in the later modules.\n\n```\npd.get_dummies(df['fuel'])\n```\n\n\n### Practical\n\nIn this practical we want to answer the question: What is the fuel consumption (L/100k) rate for the diesel car?\n\nWe see the column \"fuel-type\" has two unique values: \"gas\" or \"diesel\". Regression doesn't understand words, only numbers. To use this attribute in regression analysis, we convert \"fuel-type\" to indicator variables.\n\nWe will use pandas' method 'get_dummies' to assign numerical values to different categories of fuel type.\n\n```{python}\ndummy_variable_1 = pd.get_dummies(df['fuel-type'])\ndummy_variable_1.head()\n```\n\nChange the column names for clarity\n\n```{python}\ndummy_variable_1.rename(columns={'gas':'fuel-type-gas', 'diesel':'fuel-type-diesel'}, inplace=True)\ndummy_variable_1.head()\n```\nReplace the dummy df with our original gas column\n\n```{python}\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf = pd.concat([df, dummy_variable_1], axis = 1)\ndf.head()\n```\nDrop original column \"fuel-type\" from \"df\"\n\n\n```{python}\ndf.drop(\"fuel-type\", axis = 1, inplace = True)\ndf.head()\n```\n\nSimilar to before, create an indicator variable for the column \"aspiration\"\n\n```{python}\n#create dummy\ndummy_variable_2 = pd.get_dummies(df['aspiration'])\n\n#change the columns\ndummy_variable_2.rename(columns = {'std': 'aspiration-std', 'turbo':'aspiration-turbo'}, inplace = True)\n\nprint(dummy_variable_2.head())\n\n#combine with previous df\ndf = pd.concat([df,dummy_variable_2], axis = 1)\n\n#drop original aspiration column\ndf.drop('aspiration', axis = 1, inplace = True)\n\nprint(df.head())\n```\n\nStore clean df\n\n```{python}\ndf.to_csv(\"../data/clean_df.csv\")\n```\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"2_preprocessing_of_data.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"pdflatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"github","output-file":"2_preprocessing_of_data.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","documentclass":"scrreprt","geometry":["heightrounded"],"pandoc_args":"--listings","header-includes":["\\usepackage{fvextra} \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}"],"colorlinks":true,"code-block-bg":"D3D3D3"},"extensions":{"book":{}}}}}