{"title":"Exploratory data analysis (EDA) in python","markdown":{"headingText":"Exploratory data analysis (EDA) in python","containsRefs":false,"markdown":"\nExploratory data analysis is a preliminary step in data analysis to:\n\n-   Summarize main characteristics of the data\n-   Gain better understanding of the data set\n-   Uncover relationships between different variables\n-   Extract important variables for the problem we're trying to solve\n\nIn this tutorial we want to answer the key question: What are the main characteristics that have the most impact on the car price?\n\n## Notebook setup \n\n```{python}\n#load libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n#load data\ndf = pd.read_csv(\"../data/automobileEDA.csv\")\ndf.head()\n```\n\n## Analyzing Individual Feature Patterns Using Visualization\n\nHow to choose the right visualization method?¶\n\nWhen visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.\n\n```{python}\ndf.dtypes.head()\n```\nWe can calculate the correlation between variables  of type \"int64\" or \"float64\" using the method \"corr\". The diagonal elements are always one; we will study correlation more precisely Pearson correlation in-depth at the end of the notebook.\n\n```{python}\ndf.corr()\n```\n\n\n## Descriptive statistics\n\n-   Describe basic features of data\n-   Give short summaries about the sample and measures of the data\n\nExamples of useful tools in python:\n\n### The describe function\n\nThe describe function automatically computes basic statistics for all continuous variables. Any NaN values are automatically skipped in these statistics.\n\nThis will show:\n\n- the count of that variable\n- the mean\n- the standard deviation (std)\n- the minimum value\n- the IQR (Interquartile Range: 25%, 50% and 75%)\n- the maximum value\n- We can apply the method \"describe\" as follows:\n\n```{python}\ndf.describe()\n```\n\nThe default setting of \"describe\" skips variables of type object. We can apply the method \"describe\" on the variables of type 'object' as follows:\n\n```{python}\ndf.describe(include = ['object'])\n```\n\n\n### The value_counts function\n\nValue counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the \"value_counts\" method on the column \"drive-wheels\". Don’t forget the method \"value_counts\" only works on pandas series, not pandas dataframes. As a result, we only include one bracket df['drive-wheels'], not two brackets df[['drive-wheels']].\n\n```{python}\ndf['drive-wheels'].value_counts()\n```\n\nWe can convert the series to a dataframe as follows:\n\n```{python}\ndf['drive-wheels'].value_counts().to_frame()\n```\n\nLet's repeat the above steps but save the results to the dataframe \"drive_wheels_counts\" and rename the column 'drive-wheels' to 'value_counts'.\n\n```{python}\ndrive_wheels_counts = df['drive-wheels'].value_counts().to_frame()\ndrive_wheels_counts.rename(columns={'drive-wheels': 'value_counts'}, inplace=True)\ndrive_wheels_counts\n```\nNow let's rename the index to 'drive-wheels':\n\n```{python}\ndrive_wheels_counts.index.name = \"drive-wheels\"\ndrive_wheels_counts\n```\n\nWe can repeat the above process for the variable 'engine-location'.\n\n```{python}\nengine_loc_counts = df['engine-location'].value_counts().to_frame()\nengine_loc_counts.rename(columns={'engine-location': 'value_counts'}, inplace=True)\nengine_loc_counts.index.name = 'engine-location'\nengine_loc_counts.head(10)\n```\n\nAfter examining the value counts of the engine location, we see that engine location would not be a good predictor variable for the price. This is because we only have three cars with a rear engine and 198 with an engine in the front, so this result is skewed. Thus, we are not able to draw any conclusions about the engine location.\n\n\n### Boxplots\n\n-   [*Boxplot*]{.underline} allow to plot the distribution of numerical data and make it easy to compare between groups.\n    The boxplot shows:\n\n    -   The **median**: represents where the middle data point is.\n    -   The **upper quartile** shows where the 75th percentile is.\n    -   The **lower quartile** shows where the 25th percentile is.\n    -   The data between the upper and lower quartile represents the **interquartile range**.\n    -   The **lower and upper extremes** are calculated as 1.5 times the interquartile range, above the 75th percentile, and as 1.5 times the IQR below the 25th percentile.\n    -   **Outliers** are individual dots that occur outside the upper and lower extremes.\n\nLet's look at the relationship between \"body-style\" and \"price\".\n\n```{python}\nsns.boxplot(x = \"body-style\", y = \"price\", data = df)\nplt.show()\nplt.close()\n```\n\nWe see that the distributions of price between the different body-style categories have a significant overlap, so body-style would not be a good predictor of price. Let's examine engine \"engine-location\" and \"price\":\n\n```{python}\nsns.boxplot(x = \"engine-location\", y = \"price\", data = df)\nplt.show()\nplt.close()\n```\n\nHere we see that the distribution of price between these two engine-location categories, front and rear, are distinct enough to take engine-location as a potential good predictor of price.\n\nLet's examine \"drive-wheels\" and \"price\".\n\n```{python}\nsns.boxplot(x = \"drive-wheels\", y = \"price\", data = df)\nplt.show()\nplt.close()\n```\n\nHere we see that the distribution of price between the different drive-wheels categories differs. As such, drive-wheels could potentially be a predictor of price.\n\n\n### Scatterplots\n\n-   [*Scatter plots*]{.underline} allow the representation of observations as data points and allows to show the relationships between two variables.\n    Could engine size possibly predict the price of a car?\n    Scatterplots show the relationship between two variables:\n\n    -   **Predictor/independent variable** on the x-axis, i.e. the variable that we are using to predict an outcome, i.e. the engine size\n    -   **Target/dependent variable** on the y-axis, i.e. the price\n\n\n```{python}\nsns.regplot(x = \"engine-size\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n```\n\nAs the engine-size goes up, the price goes up: this indicates a positive direct correlation between these two variables. Engine size seems like a pretty good predictor of price since the regression line is almost a perfect diagonal line.\n\nWe can examine the correlation between 'engine-size' and 'price' and see that it's approximately 0.87.\n\n```{python}\ndf[[\"engine-size\", \"price\"]].corr()\n```\n\nHighway mpg is a potential predictor variable of price. Let's find the scatterplot of \"highway-mpg\" and \"price\".\n\n```{python}\nsns.regplot(x = \"highway-mpg\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n```\n\nAs highway-mpg goes up, the price goes down: this indicates an inverse/negative relationship between these two variables. Highway mpg could potentially be a predictor of price.\n\nWe can examine the correlation between 'highway-mpg' and 'price' and see it's approximately -0.704.\n\n```{python}\ndf[[\"highway-mpg\", \"price\"]].corr()\n```\n\nLet's see if \"peak-rpm\" is a predictor variable of \"price\".\n\n\n```{python}\nsns.regplot(x = \"peak-rpm\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n```\n\n```{python}\ndf[[\"peak-rpm\", \"price\"]].corr()\n```\n\nPeak rpm does not seem like a good predictor of the price at all since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore, it's not a reliable variable.\n\n\n\n\n## GroupBy in python\n\nAssume we want to know, is there any relationship between the different types of drive system, forward, rear, and four-wheel drive, and the price of the vehicles?\n\nIf so, which type of drive system adds the most value to a vehicle?\nIt would be nice if we could group all the data by the different types of drive wheels and compare the results of these different drive wheels against each other.\n\nTo do this we can use pandas groupby() method, which:\n\n- groups data by different categories\n- Can be applied on categorical values\n- Allows to group data into categories\n- Accepts single multiple variables\n\nFor example, let's group by the variable \"drive-wheels\". We see that there are 3 different categories of drive wheels.\n\n```{python}\ndf['drive-wheels'].unique()\n```\n\nIf we want to know, on average, which type of drive wheel is most valuable, we can group \"drive-wheels\" and then average them.\n\nWe can select the columns 'drive-wheels', 'body-style' and 'price', then assign it to the variable \"df_group_one\".\n\n\n```{python}\ndf_group_one = df[['drive-wheels', 'body-style', 'price']]\n```\n\nWe can then calculate the average price for each of the different categories of data.\n\n```{python}\ndf_group_one = df_group_one.groupby(['drive-wheels'], as_index = False).mean()\ndf_group_one\n```\n\nFrom our data, it seems rear-wheel drive vehicles are, on average, the most expensive, while 4-wheel and front-wheel are approximately the same in price.\n\nYou can also group by multiple variables. For example, let's group by both 'drive-wheels' and 'body-style'. This groups the dataframe by the unique combination of 'drive-wheels' and 'body-style'. We can store the results in the variable 'grouped_test1'.\n\n\n```{python}\ndf_gptest = df[['drive-wheels','body-style','price']]\ngrouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()\ngrouped_test1\n```\n\n\n### Pivot() in python\n\nTo make the output of the groupby method easier to read, we can transform this table to a pivot table by using the pivot method. A pivot table has one variable displayed along the columns and the other variable displayed along the rows.\n\nIn our case, we will leave the drive-wheels variable as the rows of the table, and pivot body-style to become the columns of the table:\n\n```{python}\ngrouped_pivot = grouped_test1.pivot(index = \"drive-wheels\", columns = \"body-style\")\ngrouped_pivot\n```\nOften, we won't have data for some of the pivot cells. We can fill these missing cells with the value 0, but any other value could potentially be used as well. It should be mentioned that missing data is quite a complex subject and is an entire course on its own.\n\n```{python}\ngrouped_pivot = grouped_pivot.fillna(0)\ngrouped_pivot\n```\n\n\n### Heatmap plots\n\nHeatmap plots:\n\n-   Another way to visualize tables and plot the target variable over multiple variables\n-   Takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points\n\n### Correlation heatmap\n\nTaking all variables into account, we can now create a heatmap that indicates the correlation between each of the variables with one another. The color scheme indicates the Pearson correlation coefficient, indicating the strength of the correlation between two variables.\n\nLet's use a heat map to visualize the relationship between Body Style vs Price.\n\n```{python}\nplt.pcolor(grouped_pivot, cmap = 'RdBu')\nplt.colorbar()\nplt.show()\nplt.close()\n```\n\nThe heatmap plots the target variable (price) proportional to colour with respect to the variables 'drive-wheel' and 'body-style' on the vertical and horizontal axis, respectively. This allows us to visualize how the price is related to 'drive-wheel' and 'body-style'.\n\nThe default labels convey no useful information to us. Let's change that:\n\n```{python}\nfig, ax = plt.subplots()\nim = ax.pcolor(grouped_pivot, cmap='RdBu')\n\n#label names\nrow_labels = grouped_pivot.columns.levels[1]\ncol_labels = grouped_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=90)\n\nfig.colorbar(im)\n\nplt.show()\nplt.close()\n```\n\nVisualization is very important in data science, and Python visualization packages provide great freedom. We will go more in-depth in a separate Python visualizations course.\n\nThe main question we want to answer in this module is, \"What are the main characteristics which have the most impact on the car price?\".\n\nTo get a better measure of the important characteristics, we look at the correlation of these variables with the car price. In other words: how is the car price dependent on this variable?\n\n\n## Correlation\n\nCorrelation is a statistical metric for measuring to what extent different variables are interdependent.\n\nIn other words, when we look at two variables over time, if one variable changes how does this affect change in the other variable?\n\nFor example, there is a correlation between umbrella and rain variables where more precipitation means more people use umbrellas.\nAlso, if it doesn't rain people would not carry umbrellas.\nTherefore, we can say that umbrellas and rain are interdependent and by definition they are correlated.\n\n**It is important to know that correlation doesn't imply causation.**\n\n**Correlation**: a measure of the extent of interdependence between variables.\n**Causation**: the relationship between cause and effect between two variables.\n\nIn fact, we can say that umbrella and rain are correlated but we would not have enough information to say whether the umbrella caused the rain or the rain caused the umbrella.\n\n\n#### Pearson correlation\n\nMeasures the strength of the correlation between a continuous numerical variable.\n\nThis method will give us two values:\n\n-   Correlation coefficient, a value:\n\n    -   Close to 1: Large positive relationship\n    -   Close to -1 : Large negative relationship\n    -   Close to 0: No relationship\n \nPearson Correlation is the default method of the function \"corr\". Like before, we can calculate the Pearson Correlation of the of the 'int64' or 'float64' variables.\n\n```{python}\ndf.corr().head()\n```\n \nWhat is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\n    \n-   P-value of:\n\n    - < 0.001 : strong certainty of the result\n    - < 0.05 : moderate certainty of the result\n    - < 0.1 : weak certainty of the result\n    - > 0.1 : no certainty of the result\n\nLet's calculate the Pearson Correlation Coefficient and P-value of 'wheel-base' and 'price'.\n\n\n```{python}\npearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n```\n\nSince the p-value is  <  0.001, the correlation between wheel-base and price is statistically significant, although the linear relationship isn't extremely strong (~0.585).\n\nLet's calculate the  Pearson Correlation Coefficient and P-value of 'horsepower' and 'price'.\n\n```{python}\npearson_coef, p_value = stats.pearsonr(df['horsepower'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n```\n\nSince the p-value is  <  0.001, the correlation between horsepower and price is statistically significant, and the linear relationship is quite strong (~0.809, close to 1).\n\nLet's calculate the Pearson Correlation Coefficient and P-value of 'length' and 'price'.\n\n```{python}\npearson_coef, p_value = stats.pearsonr(df['length'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n```\n\nSince the p-value is  <  0.001, the correlation between length and price is statistically significant, and the linear relationship is moderately strong (~0.691).\n\nLet's calculate the Pearson Correlation Coefficient and P-value of 'width' and 'price':\n\n```{python}\npearson_coef, p_value = stats.pearsonr(df['width'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n```\n\n\n#### Anova\n\nANOVA: Analysis of Variance\n\nThe Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:\n\nF-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\n\nP-value: P-value tells how statistically significant our calculated score value is.\n\nIf our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to return a sizeable F-test score and a small p-value.\n\nSince ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.\n\nTo see if different types of 'drive-wheels' impact 'price', we group the data.\n\n```{python}\ngrouped_test2=df_gptest[['drive-wheels', 'price']].groupby(['drive-wheels'])\ngrouped_test2.head(2)\n```\n```{python}\ndf_gptest.head(2)\n```\n\nWe can obtain the values of the method group using the method \"get_group\".\n\n```{python}\ngrouped_test2.get_group('4wd')['price']\n```\n\nWe can use the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.\n\n```{python}\n# ANOVA\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  \n \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   \n```\n\n\nThis is a great result with a large F-test score showing a strong correlation and a P-value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated?\n\nLet's examine them separately.\n\n**fwd and rwd**\n\n```{python}\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'])  \n \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val )\n```\n\n\n**4wd and rwd**\n\n```{python}\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('rwd')['price'])  \n   \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   \n```\n\n\n**4wd and fwd**\n\n```{python}\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('fwd')['price'])  \n \nprint(\"ANOVA results: F=\", f_val, \", P =\", p_val)   \n```\n\n\nWe now have a better idea of what our data looks like and which variables are important to take into account when predicting the car price. We have narrowed it down to the following variables:\n\nContinuous numerical variables:\n\nLength\nWidth\nCurb-weight\nEngine-size\nHorsepower\nCity-mpg\nHighway-mpg\nWheel-base\nBore\nCategorical variables:\n\nDrive-wheels\nAs we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model's prediction performance.\n\n\n## Association between two categorical variables: Chi-Square\n\n- Chi-square test for association allows to find out if there is a relationship between two categorical variables.\n- Intended to test how likely it is that an observed distribution is due to chance.\n- Measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent = i.e. it tests the null hypothesis that the variables are independent.\n-  Compares the observed data to the values that the model expects if the data was distributed in different categories by chance. Anytime the observed data doesn't fit within the model of the expected values, the probability that the variables are dependent becomes stronger, thus proving the null hypothesis incorrect.\n- The chi-square doesn't tell us what type of relationship exists between the two variables, only that a relationship exists.\n- **P-value < 0.05**: We reject the null hypothesis that the two variables are independent and can conclude that there is evidence for an association between variable A and variable B.\n\n(Row total * Column total) / grand total\n\ndegree of freedom = (row-1)*(column-1)\n\n```\nscipy.stats.chi2_contingency(cont_table, correction = True)\n```\n\n"},"formats":{"html":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"3_exploratory_data_analysis.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"pdflatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"github","output-file":"3_exploratory_data_analysis.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","documentclass":"scrreprt","geometry":["heightrounded"],"pandoc_args":"--listings","header-includes":["\\usepackage{fvextra} \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}"],"colorlinks":true,"code-block-bg":"D3D3D3"},"extensions":{"book":{}}}}}