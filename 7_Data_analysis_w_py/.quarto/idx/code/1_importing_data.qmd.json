{"title":"Importing datasets","markdown":{"headingText":"Importing datasets","containsRefs":false,"markdown":"\nData analysis and, in essence,data science, helps us unlock the information and insights from raw data to answer our questions. So data analysis plays an important role by helping us to discover useful information from the data, answer questions, and even predict the future or the unknown.\n\nThere are various formats for a dataset: .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.\n\nIn our case, the Automobile Dataset is an online source, and it is in a CSV (comma separated value) format. Let's use this dataset as an example to practice data reading.\n\nData source: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\nData type: csv\n\n\n## Understanding the data\n\nThe dataset used in this course is an open csv dataset by Jeffrey C. Schlemmer.\n\n**Symboling** =  the insurance risk level of a car. Cars are initially assigned a risk factor symbol associated with their price. Then, if an automobile is more risky, this symbol is adjusted by moving it up the scale. A value of plus three indicates that the auto is risky. Minus three, that is probably pretty safe.\n\n**Normalized-losses** =  the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification, two door small, station wagons, sports specialty, etc., and represents the average loss per car per year. \n\n**Price** =  our target value or label. This means price is the value that we want to predict from the dataset and the predictors should be all the other variables listed like symboling, normalized-losses, make, and so on.\n\n\n## Python packages for data science\n\nA Python library is a collection of functions and methods that allow you to perform lots of actions without writing any code. The libraries usually contain built in modules providing different functionalities which you can use directly.\n\n### Scientific computing libs\n\n- Pandas:  offers data structure and tools for effective data manipulation and analysis. It provides facts, access to structured data. The primary instrument of Pandas is the two dimensional table consisting of column and row labels, which are called a data frame.\n- Numpy: uses arrays for its inputs and outputs. It can be extended to objects for matrices and with minor coding changes, developers can perform fast array processing.\n- SciPy: ncludes functions for some advanced math problems as listed on this slide, as well as data visualization.\n\n\n### Data vis libs\n\n- Matplotlib: great for making graphs and plots. The graphs are also highly customizable. \n- Seaborn: based on Matplotlib. It's very easy to generate various plots such as heat maps, time series and violin plots.\n\n\n### Algorithm libs\n\n- Scikit-learn: contains tools statistical modeling, including regression, classification, clustering, and so on. \n- Statsmodels: a Python module that allows users to explore data, estimate statistical models and perform statistical tests. \n\n\n\n## Importing an exporting data in python\n\nProcess of loading and reading data into python from various resources.\n\nImportant properties:\n\n- Format\n- File path\n\n\n### Importing a csv in pandas\n\n```{python}\n#import lib\nimport pandas as pd\nimport numpy as np\n\n#store url were we want to retrieve our data in a variable\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n\n#read in the data\ndf = pd.read_csv(url, header = None)\n\n#view first rows of the data\ndf.head(n=4)\n```\n\n### Add Headers\n\nTake a look at our dataset. Pandas automatically set the header with an integer starting from 0.\n\nTo better describe our data, we can introduce a header. This information is available at: https://archive.ics.uci.edu/ml/datasets/Automobile.\n\nThus, we have to add headers manually.\n\nFirst, we create a list \"headers\" that include all column names in order. Then, we use dataframe.columns = headers to replace the headers with the list we created.\n\n```{python}\n# create headers list\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n\nprint(\"headers\\n\", headers)\n```\n\n```{python}\n#replace the headers in our df\ndf.columns = headers\ndf.head(10)\n```\n\nWe need to use `replace` to replace the \"?\" symbol with NaN so the `dropna()` can remove the missing values.\n\nIn `dropna()` we use the following arguments:\n\n- axis: Determine if rows or columns which contain missing values are removed.\n    - 0, or ‘index’ : Drop rows which contain missing values.\n    - 1, or ‘columns’ : Drop columns which contain missing value.\n    - default = 0\n- subset: Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.\n\n```{python}\n#replace the ? symbol\ndf1 = df.replace('?', np.NaN)\n\n#drop missing values in the price column\ndf = df1.dropna(subset=['price'], axis = 0)\ndf.head(10)\n```\n\n```{python}\n#find the name of the columns in the dataframe\ndf.columns   \n```\n\n\n### Save the dataset\n\nCorrespondingly, Pandas enables us to save the dataset to csv. By using the dataframe.to_csv() method, you can add the file path and name along with quotation marks in the brackets.\n\nFor example, if you would save the dataframe df as automobile.csv to your local machine, you may use the syntax below, where index = False means the row names will not be written.\n\n```{python}\ndf.to_csv(\"../data/automobile.csv\", index=False)\n```\n\nWe can also read and save other file formats. We can use similar functions like pd.read_csv() and df.to_csv() for other data formats. The functions are listed in the following table:\n\n<h3>Read/Save Other Data Formats</h3>\n\n| Data Formate |        Read       |            Save |\n| ------------ | :---------------: | --------------: |\n| csv          |  `pd.read_csv()`  |   `df.to_csv()` |\n| json         |  `pd.read_json()` |  `df.to_json()` |\n| excel        | `pd.read_excel()` | `df.to_excel()` |\n| hdf          |  `pd.read_hdf()`  |   `df.to_hdf()` |\n| sql          |  `pd.read_sql()`  |   `df.to_sql()` |\n| ...          |        ...        |             ... |\n\n\n## Basic data analysis \n\nPandas has several built-in methods that can be used to understand the datatype or features or to look at the distribution of data within the dataset. Using these methods, gives an overview of the dataset and also point out potential issues such as the wrong data type of features which may need to be resolved later on. \n\n### Data types\n\nData has a variety of types.\nThe main types stored in Pandas dataframes are object, float, int, bool and datetime64. In order to better learn about each attribute, it is always good for us to know the data type of each column. \n\nWhy check data type:\n\n- Pandas automatically assigns types based on the encoding it detects from the original data table. For a number of reasons, this assignment may be incorrect. For example, it should be awkward if the car price column which we should expect to contain continuous numeric numbers, is assigned the data type of object. It would be more natural for it to have the float type.\n- Allows a data scientists to see which Python functions can be applied to a specific column. For example, some math functions can only be applied to numerical data.\n\nAs shown below, it is clear to see that the data type of \"symboling\" and \"curb-weight\" are int64, \"normalized-losses\" is object, and \"wheel-base\" is float64, etc.\n\n```{python}\n#check the datatypes\nprint(df.dtypes)\n```\n\n### Describe\n\nNow, we would like to check the statistical summary of each column to learn about the distribution of data in each column. The statistical metrics can tell the data scientist if there are mathematical issues that may exist such as extreme outliers and large deviations.\n\nThe data scientists may have to address these issues later. To get the quick statistics, we use the describe method. It returns the number of terms in the column as count, average column value as mean, column standard deviation as std, the maximum minimum values, as well as the boundary of each of the quartiles. \n\nBy default, this method will provide various summary statistics, excluding NaN (Not a Number) values.\n\n```{python}\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe())\n```\n\nThis shows the statistical summary of all numeric-typed (int, float) columns.\nFor example, the attribute \"symboling\" has 205 counts, the mean value of this column is 0.83, the standard deviation is 1.25, the minimum value is -2, 25th percentile is 0, 50th percentile is 1, 75th percentile is 2, and the maximum value is 3.\n\nHowever, what if we would also like to check all the columns including those that are of type object?\n\nYou can add an argument include = \"all\" inside the bracket. Let's try it again.\n\nWe can also do this on all datatype using `include = 'all'`. \n\n```{python}\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe(include = 'all'))\n```\n\nWe see that for the object type columns, a different set of statistics is evaluated, like unique, top, and frequency. Unique is the number of distinct objects in the column. Top is most frequently occurring object, and freq is the number of times the top object appears in the column. \n\nSome values in the table are shown here as NaN which stands for not a number. This is because that particular statistical metric cannot be calculated for that specific column data type.\n\n\n### Selecting columns\n\nYou can select the columns of a dataframe by indicating the name of each column. For example, you can select the three columns as follows:\n\n```\ndataframe[[' column 1 ',column 2', 'column 3']]\n```\n\nWhere \"column\" is the name of the column, you can apply the method \".describe()\" to get the statistics of those columns as follows:\n\n```\ndataframe[[' column 1 ',column 2', 'column 3'] ].describe()\n```\n\nFor example, lets use the describe method to the columns length and compression-ratio\n\n```{python}\ndf[['length','compression-ratio']].describe()\n```\n\n\n### info\n\nThe info method provides a concise summary of your DataFrame.\n\nThis method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.\n\n```{python}\nprint(df.info())\n```\n\n## Accessing databases with python\n\nThe Python code connects to the database using API calls. An application programming interface is a set of functions that you can call to get access to some type of service.\n\n### SQL API\n\nThe SQL API consists of library function calls as an application programming interface, API, for the DBMS. \n\nTo pass SQL statements to the DBMS, an application program calls functions in the API, and it calls other functions to retrieve query results and status information from the DBMS. \n\n<p align=\"left\">\n  <img width=300, height=200, src=\"../images/sql_api.png\">\n</p>\n\n- The application program begins its database access with one or more API calls that connect the program to the DBMS. \n- To send the SQL statement to the DBMS, the program builds the statement as a text string in a buffer and then makes an API call to pass the buffer contents to the DBMS. \n- The application program makes API calls to check the status of its DBMS request and to handle errors. \n- The application program ends its database access with an API call that disconnects it from the database. \n\n\n### What is a DB-API\n\nDB-API is Python's standard API for accessing relational databases. It is a standard that allows you to write a single program that works with multiple kinds of relational databases instead of writing a separate program for each one. So, if you learn the DB-API functions, then you can apply that knowledge to use any database with Python.\n\nThe two main concepts in the Python DB-API are: \n\n- **connection objects** that you use to connect to a database and manage your transactions.\n- **Cursor objects** are used to run queries. You open a cursor object and then run queries. The cursor works similar to a cursor in a text processing system where you scroll down in your result set and get your data into the application. Cursors are used to scan through the results of a database. \n\nThe methods used with connection objects are:\n\n- The `cursor()` method returns a new cursor object using the connection. \n- The `commit()` method is used to commit any pending transaction to the database. \n- The `rollback()` method causes the database to roll back to the start of any pending transaction. \n- The `close()` method is used to close a database connection.\n\nAs an example: First, you import your database module by using the connect API from that module. To open a connection to the database, you use the connection function and pass in the parameters that is, the database name, username, and password. The connect function returns connection object. After this, you create a cursor object on the connection object. The cursor is used to run queries and fetch results. After running the queries using the cursor, we also use the cursor to fetch the results of the query. Finally, when the system is done running the queries, it frees all resources by closing the connection.\n\n<p align=\"left\">\n  <img width=300, height=200, src=\"../images/db_api.png\">\n</p>\n"},"formats":{"html":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"1_importing_data.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":3.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"pdflatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"github","output-file":"1_importing_data.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references.bib"],"space-before-code-block":"10pt","space-after-code-block":"8pt","linespacing":"22pt plus2pt","frontmatter-linespacing":"17pt plus1pt minus1pt","title-size":"22pt","title-size-linespacing":"28pt","gap-before-crest":"25mm","gap-after-crest":"25mm","execute-dir":"file","documentclass":"scrreprt","geometry":["heightrounded"],"pandoc_args":"--listings","header-includes":["\\usepackage{fvextra} \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}"],"colorlinks":true,"code-block-bg":"D3D3D3"},"extensions":{"book":{}}}}}