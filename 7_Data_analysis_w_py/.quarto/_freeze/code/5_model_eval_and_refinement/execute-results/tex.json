{
  "hash": "fb5e8de1fa2681bf1bb72db5ce09acd9",
  "result": {
    "markdown": "# Model Evaluation and Refinement\n\n## Training and Testing\n\nLet's first prepare our environment: \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#load libs\nimport pandas as pd\nimport numpy as np\n\n#libs for plotting\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for model training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import PolynomialFeatures\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#load data\ndf = pd.read_csv(\"../data/module_5_auto.csv\")\n\n#only use numeric data\ndf=df._get_numeric_data()\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=tex}\n\\begin{tabular}{lrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n{} &  Unnamed: 0 &  Unnamed: 0.1 &  symboling &  normalized-losses &  wheel-base &    length &     width &  height &  curb-weight &  engine-size &  bore &  stroke &  compression-ratio &  horsepower &  peak-rpm &  city-mpg &  highway-mpg &    price &  city-L/100km &  diesel &  gas \\\\\n\\midrule\n0 &           0 &             0 &          3 &                122 &        88.6 &  0.811148 &  0.890278 &    48.8 &         2548 &          130 &  3.47 &    2.68 &                9.0 &       111.0 &    5000.0 &        21 &           27 &  13495.0 &     11.190476 &       0 &    1 \\\\\n1 &           1 &             1 &          3 &                122 &        88.6 &  0.811148 &  0.890278 &    48.8 &         2548 &          130 &  3.47 &    2.68 &                9.0 &       111.0 &    5000.0 &        21 &           27 &  16500.0 &     11.190476 &       0 &    1 \\\\\n2 &           2 &             2 &          1 &                122 &        94.5 &  0.822681 &  0.909722 &    52.4 &         2823 &          152 &  2.68 &    3.47 &                9.0 &       154.0 &    5000.0 &        19 &           26 &  16500.0 &     12.368421 &       0 &    1 \\\\\n3 &           3 &             3 &          2 &                164 &        99.8 &  0.848630 &  0.919444 &    54.3 &         2337 &          109 &  3.19 &    3.40 &               10.0 &       102.0 &    5500.0 &        24 &           30 &  13950.0 &      9.791667 &       0 &    1 \\\\\n4 &           4 &             4 &          2 &                164 &        99.4 &  0.848630 &  0.922222 &    54.3 &         2824 &          136 &  3.19 &    3.40 &                8.0 &       115.0 &    5500.0 &        18 &           22 &  17450.0 &     13.055556 &       0 &    1 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#define functions for plotting\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 8\n    height = 6\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n    width = 8\n    height = 6\n    plt.figure(figsize=(width, height))\n    \n    #training data \n    #testing data \n    # lr:  linear regression object \n    #poly_transform:  polynomial transformation object \n    xmax=max([xtrain.values.max(), xtest.values.max()])\n    xmin=min([xtrain.values.min(), xtest.values.min()])\n    x=np.arange(xmin, xmax, 0.1)\n\n    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n    plt.plot(xtest, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)\n```\n:::\n\n\nAn important step in testing your model is to split your data into training and testing data. We will place the target data price in a separate dataframe y_data and Drop price data in dataframe x_data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#prepare the df\ny_data = df['price']\nx_data=df.drop('price',axis=1)\n```\n:::\n\n\nNow, we randomly split our data into training and testing data using the function train_test_split\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\n#split data\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n\n#see if this worked\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnumber of test samples : 21\nnumber of training samples: 180\n```\n:::\n:::\n\n\nWe see that the test_size parameter sets the proportion of data that is split into the testing set. In the above, the testing set is 10% of the total dataset.\n\nNow, lets prepare the model\"\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#load libs\nfrom sklearn.linear_model import LinearRegression\n\n#create regression object\nlre = LinearRegression()\n\n#fit the model using the feature `horsepower`\nlre.fit(x_train[['horsepower']], y_train)\n\n#calculate the R2 on the test data\nprint(lre.score(x_test[['horsepower']], y_test))\n\n#compare the R2 of the training data\nprint(lre.score(x_train[['horsepower']], y_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.36358755750788263\n0.6619724197515104\n```\n:::\n:::\n\n\nWe can see the R^2 is much smaller using the test data compared to the training data.\n\n\n## Model evaluation\n\nModel evaluation tells us how our model performs in the real world. \n\n- In-sample evaluation tells us how well our model fits the data already given to train it\n- However, the problem is that it does not give us an estimate of how well the train model can predict new data\n- The solution is to split our data up, use the in-sample data or training data to train the model. The rest of the data, called Test Data, is used as out-of-sample data. This data is then used to approximate, how the model performs in the real world\n- When we split a dataset, usually the larger portion of data is used for training and a smaller part is used for testing\n-  When we have completed testing our model, we should use all the data to train the model\n\n\n### Generalization performance\n\n- Generalization error is a measure how well our data does at predicting previously unseen data\n- The error we obtain using our testing data is an approximation of this error\n- Using a lot of data for training,gives us an accurate means of determining how well our model will perform in the real world. But the precision of the performance will be low\n- If we use fewer data points to train the model and more to test the model, the accuracy of the generalization performance will be less, but the model will have good precision.\n\nTo overcome this problem, we use cross-validation.\n\n\n\n### Cross validation\n\n- One of the most common out of sample evaluation metrics is cross-validation\n- More effective use of data, as each observation is used for both training and testing\n- In this method, the dataset is split into K equal groups. Each group is referred to as a fold.\n- Some of the folds can be used as a training set which we use to train the model and the remaining parts are used as a test set, which we use to test the model. For example, we can use three folds for training, then use one fold for testing\n- This is repeated until each partition is used for both training and testing \n- At the end, we use the average results as the estimate of out-of-sample error\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#load lib\nfrom sklearn.model_selection import cross_val_score\n```\n:::\n\n\nFor cross-validation we  input the object, the feature (\"horsepower\"), and the target data (y_data). The parameter 'cv' determines the number of folds. In this case, it is 4.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n#cross validate\nRcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv = 4)\n```\n:::\n\n\nThe default scoring is R^2. Each element in the array has the average R^2 value for the fold:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n#view data\nRcross\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([0.7746232 , 0.51716687, 0.74785353, 0.04839605])\n```\n:::\n:::\n\n\nWe can calculate the average and standard deviation of our estimate:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean of the folds are 0.522009915042119 and the standard deviation is 0.291183944475603\n```\n:::\n:::\n\n\nWe can use negative squared error as a score by setting the parameter 'scoring' metric to 'neg_mean_squared_error'.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([20254142.84026702, 43745493.2650517 , 12539630.34014931,\n       17561927.72247591])\n```\n:::\n:::\n\n\nYou can also use the function 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, with one fold for testing and the other folds are used for training. First, import the function:\n\nWe input the object, the feature \"horsepower\", and the target data y_data. The parameter 'cv' determines the number of folds. In this case, it is 4. We can produce an output:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n#load lib\nfrom sklearn.model_selection import cross_val_predict\n\n#predict the output\nyhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\nyhat[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([14141.63807508, 14141.63807508, 20814.29423473, 12745.03562306,\n       14762.35027598])\n```\n:::\n:::\n\n\n## Overfitting, Underfitting and Model Selection\n\nIt turns out that the test data, sometimes referred to as the \"out of sample data\", is a much better measure of how well your model performs in the real world. One reason for this is overfitting.\n\nLet's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.\n\nIn this section, we will discuss how to pick the best polynomial order and problems that arise when selecting the wrong order polynomial.\n\n- **Underfitting**: the model is too simple to fit the data\n- **Overfitting** : the estimated function oscillates but doesn't track the function\n\nFor Model selection we can compare the mean square error and the polynomial order:\n\n- The training error decreases with the order of the polynomial\n- The test error is a better means to estimate the error of the polynomial. The error decreases 'till the best order of the polynomial is determined. Then the error begins to increase. We select the order that minimizes the test error.Anything on the left would be considered underfitting. Anything on the right is overfitting\n- If we select the best order of the polynomial, we will still have some errors, i.e. due to random noise or our polynomial assumption may be wrong \n\n\nLet's create Multiple Linear Regression objects and train the model using 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg' as features.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nlr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n```\n:::\n:::\n\n\nNext, we run the prediction using training data:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nyhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([ 7426.6731551 , 28323.75090803, 14213.38819709,  4052.34146983,\n       34500.19124244])\n```\n:::\n:::\n\n\nNext, we run the prediction using test data:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nyhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([11349.35089149,  5884.11059106, 11208.6928275 ,  6641.07786278,\n       15565.79920282])\n```\n:::\n:::\n\n\nLet's perform some model evaluation using our training and testing data separately. First, we import the seaborn and matplotlib library for plotting.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\nLet's examine the distribution of the predicted values of the training data.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nTitle = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)\n```\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-20-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nSo far, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nTitle='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)\n```\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-21-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nComparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.\n\n\n### Overfitting\n\nOverfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.\n\nLet's use 55 percent of the data for training and the rest for testing:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n#load lib\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#use 55 percent of the data for training and the rest for testing\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\n\n#perform a degree 5 polynomial transformation on the feature 'horsepower'\npr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nPolynomialFeatures(degree=5, include_bias=True, interaction_only=False,\n                   order='C')\n```\n:::\n:::\n\n\nNow, let's create a Linear Regression model \"poly\" and train it.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\npoly = LinearRegression()\npoly.fit(x_train_pr, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n```\n:::\n:::\n\n\nWe can see the output of our model using the method \"predict.\" We assign the values to \"yhat\".\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nyhat = poly.predict(x_test_pr)\nyhat[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\narray([ 6728.65561887,  7307.98782321, 12213.78770965, 18893.24804015,\n       19995.95195136])\n```\n:::\n:::\n\n\nLet's take the first five predicted values and compare it to the actual targets.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nprint(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", y_test[0:4].values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted values: [ 6728.65561887  7307.98782321 12213.78770965 18893.24804015]\nTrue values: [ 6295. 10698. 13860. 13499.]\n```\n:::\n:::\n\n\nWe will use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nPollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)\n```\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-26-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nFigure 3: A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction.\n\nWe see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points.\n\nR^2 of the training data:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\npoly.score(x_train_pr, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n0.5567716902120265\n```\n:::\n:::\n\n\nR^2 of the test data:\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\npoly.score(x_test_pr, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n-29.871340302044135\n```\n:::\n:::\n\n\nWe see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87. The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting.\n\nLet's see how the R^2 changes on the test data for different order polynomials and then plot the results:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nRsqu_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    \n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    \n    Rsqu_test.append(lr.score(x_test_pr, y_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')    \nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-29-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe see the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial.\n\nThe following interface allows you to experiment with different polynomial orders and different amounts of data.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ninteract(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))\n```\n\n::: {.cell-output .cell-output-display}\n```\ninteractive(children=(IntSlider(value=3, description='order', max=6), FloatSlider(value=0.45, description='tes…\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n<function __main__.f(order, test_data)>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-30-output-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Ridge regression\n\nRidge regression is a regression that is employed in a Multiple regression model when Multicollinearity occurs. Multicollinearity is when there is a strong relationship among the independent variables. Ridge regression is very common with polynomial regression and is used to regularize and reduce the standard errors to avoid over-fitting a regression model.\n\nLet's perform a degree two polynomial transformation on our data.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\npr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n```\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n#load lib\nfrom sklearn.linear_model import Ridge\n\n#create a ridge regression object\nRigeModel=Ridge(alpha=1)\n\n#fit the model using the method fit.\nRigeModel.fit(x_train_pr, y_train)\n\n#obtain a prediction\nyhat = RigeModel.predict(x_test_pr)\n\n#Let's compare the first five predicted samples to our test set:\nprint('predicted:', yhat[0:4])\nprint('test set :', y_test[0:4].values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npredicted: [ 6570.82441941  9636.24891471 20949.92322737 19403.60313255]\ntest set : [ 6295. 10698. 13860. 13499.]\n```\n:::\n:::\n\n\nWe select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nfrom tqdm import tqdm\n\nRsqu_test = []\nRsqu_train = []\ndummy1 = []\nAlpha = 10 * np.array(range(0,1000))\npbar = tqdm(Alpha)\n\nfor alpha in pbar:\n    RigeModel = Ridge(alpha=alpha) \n    RigeModel.fit(x_train_pr, y_train)\n    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n    \n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    Rsqu_test.append(test_score)\n    Rsqu_train.append(train_score)\n```\n:::\n\n\nWe can plot out the value of R^2 for different alphas:\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nwidth = 5\nheight = 8\nplt.figure(figsize=(width, height))\n\nplt.plot(Alpha,Rsqu_test, label='validation data  ')\nplt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()\n\nplt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-display}\n![](5_model_eval_and_refinement_files/figure-pdf/cell-34-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nFigure 4: The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha.\n\nHere the model is built and tested on the same data, so the training and test data are the same.\n\nThe red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data\n\nThe blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point.\n\n\n\n\n## Grid Search\n\nGrid Search allows us to scan through multiple free parameters with few lines of code.\n\nParameters like the alpha term in Ridge regression are not part of the fitting or training process. These values are called **hyperparameters**.\n\nScikit-learn has a means of automatically iterating over these hyperparameters using cross-validation. This method is called **Grid Search**. Grid Search takes the model or objects you would like to train and different values of the hyperparameters. It then calculates the mean square error or R-squared for various hyperparameter values, allowing you to choose the best values based on the set of parameters that minimize the error.\n\nTo select the hyperparameter, we split our dataset into three parts:\n\n- The **training set** in which we train the model for different hyperparameters.\n- We select the hyperparameter that minimizes the mean squared error or maximizes the R-squared on the **validation set**. \n- Finally, we test our model set using the **test set**\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n#load lib\nfrom sklearn.model_selection import GridSearchCV\n\n#create a dic of the parameter values\nparameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n[{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]\n```\n:::\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n#lCreate a Ridge regression object:\nRR=Ridge()\nRR\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\nRidge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n```\n:::\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n#Create a ridge grid search object:\nGrid1 = GridSearchCV(RR, parameters1,cv=4)\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n#fit the model\nGrid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\nGridSearchCV(cv=4, error_score=nan,\n             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n                             max_iter=None, normalize=False, random_state=None,\n                             solver='auto', tol=0.001),\n             iid='deprecated', n_jobs=None,\n             param_grid=[{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000,\n                                    100000]}],\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0)\n```\n:::\n:::\n\n\nThe object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nBestRR=Grid1.best_estimator_\nBestRR\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nRidge(alpha=10000, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n```\n:::\n:::\n\n\nWe now test our model on the test data:\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nBestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n0.8411649831036149\n```\n:::\n:::\n\n\n",
    "supporting": [
      "5_model_eval_and_refinement_files/figure-pdf"
    ],
    "filters": []
  }
}