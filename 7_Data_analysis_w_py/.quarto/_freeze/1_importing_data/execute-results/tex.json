{
  "hash": "6e377c94831249be05b4c368019bd135",
  "result": {
    "markdown": "# Importing datasets\n\nData analysis and, in essence,data science, helps us unlock the information and insights from raw data to answer our questions. So data analysis plays an important role by helping us to discover useful information from the data, answer questions, and even predict the future or the unknown.\n\n## Understanding the data\n\nThe dataset used in this course is an open csv dataset by Jeffrey C. Schlemmer.\n\n**Symboling** =  the insurance risk level of a car. Cars are initially assigned a risk factor symbol associated with their price. Then, if an automobile is more risky, this symbol is adjusted by moving it up the scale. A value of plus three indicates that the auto is risky. Minus three, that is probably pretty safe.\n\n**Normalized-losses** =  the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification, two door small, station wagons, sports specialty, etc., and represents the average loss per car per year. \n\n**Price** =  our target value or label. This means price is the value that we want to predict from the dataset and the predictors should be all the other variables listed like symboling, normalized-losses, make, and so on.\n\n\n## Python packages for data science\n\nA Python library is a collection of functions and methods that allow you to perform lots of actions without writing any code. The libraries usually contain built in modules providing different functionalities which you can use directly.\n\n### Scientific computing libs\n\n- Pandas:  offers data structure and tools for effective data manipulation and analysis. It provides facts, access to structured data. The primary instrument of Pandas is the two dimensional table consisting of column and row labels, which are called a data frame.\n- Numpy: uses arrays for its inputs and outputs. It can be extended to objects for matrices and with minor coding changes, developers can perform fast array processing.\n- SciPy: ncludes functions for some advanced math problems as listed on this slide, as well as data visualization.\n\n\n### Data vis libs\n\n- Matplotlib: great for making graphs and plots. The graphs are also highly customizable. \n- Seaborn: based on Matplotlib. It's very easy to generate various plots such as heat maps, time series and violin plots.\n\n\n### Algorithm libs\n\n- Scikit-learn: contains tools statistical modeling, including regression, classification, clustering, and so on. \n- Statsmodels: a Python module that allows users to explore data, estimate statistical models and perform statistical tests. \n\n\n\n## Importing an exporting data in python\n\nProcess of loading and reading data into python from various resources.\n\nImportant properties:\n\n- Format\n- File path\n\n\n### Importing a csv in pandas\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#import lib\nimport pandas as pd\n\n#store url were we want to retrieve our data in a variable\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n\n#read in the data\ndf = pd.read_csv(url, header = None)\n\n#view first rows of the data\ndf.head(n=4)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=tex}\n\\begin{tabular}{lrllllllllrrrrrllrlllrllrrl}\n\\toprule\n{} &  0  &   1  &           2  &   3  &   4  &    5  &           6  &   7  &     8  &    9  &     10 &    11 &    12 &    13 &    14 &    15 &   16 &    17 &    18 &    19 &    20 &   21 &    22 &  23 &  24 &     25 \\\\\n\\midrule\n0 &   3 &    ? &  alfa-romero &  gas &  std &   two &  convertible &  rwd &  front &  88.6 &  168.8 &  64.1 &  48.8 &  2548 &  dohc &  four &  130 &  mpfi &  3.47 &  2.68 &   9.0 &  111 &  5000 &  21 &  27 &  13495 \\\\\n1 &   3 &    ? &  alfa-romero &  gas &  std &   two &  convertible &  rwd &  front &  88.6 &  168.8 &  64.1 &  48.8 &  2548 &  dohc &  four &  130 &  mpfi &  3.47 &  2.68 &   9.0 &  111 &  5000 &  21 &  27 &  16500 \\\\\n2 &   1 &    ? &  alfa-romero &  gas &  std &   two &    hatchback &  rwd &  front &  94.5 &  171.2 &  65.5 &  52.4 &  2823 &  ohcv &   six &  152 &  mpfi &  2.68 &  3.47 &   9.0 &  154 &  5000 &  19 &  26 &  16500 \\\\\n3 &   2 &  164 &         audi &  gas &  std &  four &        sedan &  fwd &  front &  99.8 &  176.6 &  66.2 &  54.3 &  2337 &   ohc &  four &  109 &  mpfi &  3.19 &  3.40 &  10.0 &  102 &  5500 &  24 &  30 &  13950 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n### Export to csv\n\n```\npath\"location/folder\"\ndf_to_csv(path)\n```\n\n\n## Basic data analysis \n\nPandas has several built-in methods that can be used to understand the datatype or features or to look at the distribution of data within the dataset. Using these methods, gives an overview of the dataset and also point out potential issues such as the wrong data type of features which may need to be resolved later on. \n\nData has a variety of types.The main types stored in Pandas' objects are object, float, Int, and datetime. \n\nWhy check data type:\n\n- Pandas automatically assigns types based on the encoding it detects from the original data table. For a number of reasons, this assignment may be incorrect. For example, it should be awkward if the car price column which we should expect to contain continuous numeric numbers, is assigned the data type of object. It would be more natural for it to have the float type.\n- Allows a data scientists to see which Python functions can be applied to a specific column. For example, some math functions can only be applied to numerical data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#check the datatypes\nprint(df.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0       int64\n1      object\n2      object\n3      object\n4      object\n5      object\n6      object\n7      object\n8      object\n9     float64\n10    float64\n11    float64\n12    float64\n13      int64\n14     object\n15     object\n16      int64\n17     object\n18     object\n19     object\n20    float64\n21     object\n22     object\n23      int64\n24      int64\n25     object\ndtype: object\n```\n:::\n:::\n\n\nNow, we would like to check the statistical summary of each column to learn about the distribution of data in each column. The statistical metrics can tell the data scientist if there are mathematical issues that may exist such as extreme outliers and large deviations.\n\nThe data scientists may have to address these issues later. To get the quick statistics, we use the describe method. It returns the number of terms in the column as count, average column value as mean, column standard deviation as std, the maximum minimum values, as well as the boundary of each of the quartiles. \n\nBy default, the dataframe.describe functions skips rows and columns that do not contain numbers.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               0           9           10          11          12  \\\ncount  205.000000  205.000000  205.000000  205.000000  205.000000   \nmean     0.834146   98.756585  174.049268   65.907805   53.724878   \nstd      1.245307    6.021776   12.337289    2.145204    2.443522   \nmin     -2.000000   86.600000  141.100000   60.300000   47.800000   \n25%      0.000000   94.500000  166.300000   64.100000   52.000000   \n50%      1.000000   97.000000  173.200000   65.500000   54.100000   \n75%      2.000000  102.400000  183.100000   66.900000   55.500000   \nmax      3.000000  120.900000  208.100000   72.300000   59.800000   \n\n                13          16          20          23          24  \ncount   205.000000  205.000000  205.000000  205.000000  205.000000  \nmean   2555.565854  126.907317   10.142537   25.219512   30.751220  \nstd     520.680204   41.642693    3.972040    6.542142    6.886443  \nmin    1488.000000   61.000000    7.000000   13.000000   16.000000  \n25%    2145.000000   97.000000    8.600000   19.000000   25.000000  \n50%    2414.000000  120.000000    9.000000   24.000000   30.000000  \n75%    2935.000000  141.000000    9.400000   30.000000   34.000000  \nmax    4066.000000  326.000000   23.000000   49.000000   54.000000  \n```\n:::\n:::\n\n\nWe can also do this on all datatype using `include = 'all'`. \n\nWe see that for the object type columns, a different set of statistics is evaluated, like unique, top, and frequency. Unique is the number of distinct objects in the column. Top is most frequently occurring object, and freq is the number of times the top object appears in the column. \n\nSome values in the table are shown here as NaN which stands for not a number. This is because that particular statistical metric cannot be calculated for that specific column data type.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe(include = 'all'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                0    1       2    3    4     5      6    7      8   \\\ncount   205.000000  205     205  205  205   205    205  205    205   \nunique         NaN   52      22    2    2     3      5    3      2   \ntop            NaN    ?  toyota  gas  std  four  sedan  fwd  front   \nfreq           NaN   41      32  185  168   114     96  120    202   \nmean      0.834146  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \nstd       1.245307  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \nmin      -2.000000  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \n25%       0.000000  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \n50%       1.000000  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \n75%       2.000000  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \nmax       3.000000  NaN     NaN  NaN  NaN   NaN    NaN  NaN    NaN   \n\n                9   ...          16    17    18    19          20   21    22  \\\ncount   205.000000  ...  205.000000   205   205   205  205.000000  205   205   \nunique         NaN  ...         NaN     8    39    37         NaN   60    24   \ntop            NaN  ...         NaN  mpfi  3.62  3.40         NaN   68  5500   \nfreq           NaN  ...         NaN    94    23    20         NaN   19    37   \nmean     98.756585  ...  126.907317   NaN   NaN   NaN   10.142537  NaN   NaN   \nstd       6.021776  ...   41.642693   NaN   NaN   NaN    3.972040  NaN   NaN   \nmin      86.600000  ...   61.000000   NaN   NaN   NaN    7.000000  NaN   NaN   \n25%      94.500000  ...   97.000000   NaN   NaN   NaN    8.600000  NaN   NaN   \n50%      97.000000  ...  120.000000   NaN   NaN   NaN    9.000000  NaN   NaN   \n75%     102.400000  ...  141.000000   NaN   NaN   NaN    9.400000  NaN   NaN   \nmax     120.900000  ...  326.000000   NaN   NaN   NaN   23.000000  NaN   NaN   \n\n                23          24   25  \ncount   205.000000  205.000000  205  \nunique         NaN         NaN  187  \ntop            NaN         NaN    ?  \nfreq           NaN         NaN    4  \nmean     25.219512   30.751220  NaN  \nstd       6.542142    6.886443  NaN  \nmin      13.000000   16.000000  NaN  \n25%      19.000000   25.000000  NaN  \n50%      24.000000   30.000000  NaN  \n75%      30.000000   34.000000  NaN  \nmax      49.000000   54.000000  NaN  \n\n[11 rows x 26 columns]\n```\n:::\n:::\n\n\nThe dataframe.info function shows the top 30 rows and bottom 30 rows of the data frame.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(df.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       205 non-null    int64  \n 1   1       205 non-null    object \n 2   2       205 non-null    object \n 3   3       205 non-null    object \n 4   4       205 non-null    object \n 5   5       205 non-null    object \n 6   6       205 non-null    object \n 7   7       205 non-null    object \n 8   8       205 non-null    object \n 9   9       205 non-null    float64\n 10  10      205 non-null    float64\n 11  11      205 non-null    float64\n 12  12      205 non-null    float64\n 13  13      205 non-null    int64  \n 14  14      205 non-null    object \n 15  15      205 non-null    object \n 16  16      205 non-null    int64  \n 17  17      205 non-null    object \n 18  18      205 non-null    object \n 19  19      205 non-null    object \n 20  20      205 non-null    float64\n 21  21      205 non-null    object \n 22  22      205 non-null    object \n 23  23      205 non-null    int64  \n 24  24      205 non-null    int64  \n 25  25      205 non-null    object \ndtypes: float64(5), int64(5), object(16)\nmemory usage: 41.8+ KB\nNone\n```\n:::\n:::\n\n\n## Accessing databases with python\n\nThe Python code connects to the database using API calls.\n\n### SQL API\n\n<p align=\"left\">\n  <img width=300, height=400, src=\"../images/sql_api.png\">\n</p>\n\n",
    "supporting": [
      "1_importing_data_files"
    ],
    "filters": []
  }
}