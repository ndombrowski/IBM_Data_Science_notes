[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis with python",
    "section": "",
    "text": "In this module, you will learn how to understand data and learn about how to use the libraries in Python to help you import data from multiple sources. You will then learn how to perform some basic tasks to start exploring and analyzing the imported data set."
  },
  {
    "objectID": "code/1_importing_data.html",
    "href": "code/1_importing_data.html",
    "title": "2  Importing datasets",
    "section": "",
    "text": "Data analysis and, in essence,data science, helps us unlock the information and insights from raw data to answer our questions. So data analysis plays an important role by helping us to discover useful information from the data, answer questions, and even predict the future or the unknown.\nThere are various formats for a dataset: .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.\nIn our case, the Automobile Dataset is an online source, and it is in a CSV (comma separated value) format. Let’s use this dataset as an example to practice data reading.\nData source: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data Data type: csv"
  },
  {
    "objectID": "code/1_importing_data.html#understanding-the-data",
    "href": "code/1_importing_data.html#understanding-the-data",
    "title": "2  Importing datasets",
    "section": "2.1 Understanding the data",
    "text": "2.1 Understanding the data\nThe dataset used in this course is an open csv dataset by Jeffrey C. Schlemmer.\nSymboling = the insurance risk level of a car. Cars are initially assigned a risk factor symbol associated with their price. Then, if an automobile is more risky, this symbol is adjusted by moving it up the scale. A value of plus three indicates that the auto is risky. Minus three, that is probably pretty safe.\nNormalized-losses = the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification, two door small, station wagons, sports specialty, etc., and represents the average loss per car per year.\nPrice = our target value or label. This means price is the value that we want to predict from the dataset and the predictors should be all the other variables listed like symboling, normalized-losses, make, and so on."
  },
  {
    "objectID": "code/1_importing_data.html#python-packages-for-data-science",
    "href": "code/1_importing_data.html#python-packages-for-data-science",
    "title": "2  Importing datasets",
    "section": "2.2 Python packages for data science",
    "text": "2.2 Python packages for data science\nA Python library is a collection of functions and methods that allow you to perform lots of actions without writing any code. The libraries usually contain built in modules providing different functionalities which you can use directly.\n\n2.2.1 Scientific computing libs\n\nPandas: offers data structure and tools for effective data manipulation and analysis. It provides facts, access to structured data. The primary instrument of Pandas is the two dimensional table consisting of column and row labels, which are called a data frame.\nNumpy: uses arrays for its inputs and outputs. It can be extended to objects for matrices and with minor coding changes, developers can perform fast array processing.\nSciPy: ncludes functions for some advanced math problems as listed on this slide, as well as data visualization.\n\n\n\n2.2.2 Data vis libs\n\nMatplotlib: great for making graphs and plots. The graphs are also highly customizable.\nSeaborn: based on Matplotlib. It’s very easy to generate various plots such as heat maps, time series and violin plots.\n\n\n\n2.2.3 Algorithm libs\n\nScikit-learn: contains tools statistical modeling, including regression, classification, clustering, and so on.\nStatsmodels: a Python module that allows users to explore data, estimate statistical models and perform statistical tests."
  },
  {
    "objectID": "code/1_importing_data.html#importing-an-exporting-data-in-python",
    "href": "code/1_importing_data.html#importing-an-exporting-data-in-python",
    "title": "2  Importing datasets",
    "section": "2.3 Importing an exporting data in python",
    "text": "2.3 Importing an exporting data in python\nProcess of loading and reading data into python from various resources.\nImportant properties:\n\nFormat\nFile path\n\n\n2.3.1 Importing a csv in pandas\n\n#import lib\nimport pandas as pd\nimport numpy as np\n\n#store url were we want to retrieve our data in a variable\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n\n#read in the data\ndf = pd.read_csv(url, header = None)\n\n#view first rows of the data\ndf.head(n=4)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n    \n  \n  \n    \n      0\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n  \n\n4 rows × 26 columns\n\n\n\n\n\n2.3.2 Add Headers\nTake a look at our dataset. Pandas automatically set the header with an integer starting from 0.\nTo better describe our data, we can introduce a header. This information is available at: https://archive.ics.uci.edu/ml/datasets/Automobile.\nThus, we have to add headers manually.\nFirst, we create a list “headers” that include all column names in order. Then, we use dataframe.columns = headers to replace the headers with the list we created.\n\n# create headers list\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n\nprint(\"headers\\n\", headers)\n\nheaders\n ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n\n\n\n#replace the headers in our df\ndf.columns = headers\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500\n      18\n      22\n      17450\n    \n    \n      5\n      2\n      ?\n      audi\n      gas\n      std\n      two\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      15250\n    \n    \n      6\n      1\n      158\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      105.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      17710\n    \n    \n      7\n      1\n      ?\n      audi\n      gas\n      std\n      four\n      wagon\n      fwd\n      front\n      105.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      18920\n    \n    \n      8\n      1\n      158\n      audi\n      gas\n      turbo\n      four\n      sedan\n      fwd\n      front\n      105.8\n      ...\n      131\n      mpfi\n      3.13\n      3.40\n      8.3\n      140\n      5500\n      17\n      20\n      23875\n    \n    \n      9\n      0\n      ?\n      audi\n      gas\n      turbo\n      two\n      hatchback\n      4wd\n      front\n      99.5\n      ...\n      131\n      mpfi\n      3.13\n      3.40\n      7.0\n      160\n      5500\n      16\n      22\n      ?\n    \n  \n\n10 rows × 26 columns\n\n\n\nWe need to use replace to replace the “?” symbol with NaN so the dropna() can remove the missing values.\nIn dropna() we use the following arguments:\n\naxis: Determine if rows or columns which contain missing values are removed.\n\n0, or ‘index’ : Drop rows which contain missing values.\n1, or ‘columns’ : Drop columns which contain missing value.\ndefault = 0\n\nsubset: Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.\n\n\n#replace the ? symbol\ndf1 = df.replace('?', np.NaN)\n\n#drop missing values in the price column\ndf = df1.dropna(subset=['price'], axis = 0)\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      3\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500\n      18\n      22\n      17450\n    \n    \n      5\n      2\n      NaN\n      audi\n      gas\n      std\n      two\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      15250\n    \n    \n      6\n      1\n      158\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      105.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      17710\n    \n    \n      7\n      1\n      NaN\n      audi\n      gas\n      std\n      four\n      wagon\n      fwd\n      front\n      105.8\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.5\n      110\n      5500\n      19\n      25\n      18920\n    \n    \n      8\n      1\n      158\n      audi\n      gas\n      turbo\n      four\n      sedan\n      fwd\n      front\n      105.8\n      ...\n      131\n      mpfi\n      3.13\n      3.40\n      8.3\n      140\n      5500\n      17\n      20\n      23875\n    \n    \n      10\n      2\n      192\n      bmw\n      gas\n      std\n      two\n      sedan\n      rwd\n      front\n      101.2\n      ...\n      108\n      mpfi\n      3.50\n      2.80\n      8.8\n      101\n      5800\n      23\n      29\n      16430\n    \n  \n\n10 rows × 26 columns\n\n\n\n\n#find the name of the columns in the dataframe\ndf.columns   \n\nIndex(['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',\n       'num-of-doors', 'body-style', 'drive-wheels', 'engine-location',\n       'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',\n       'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke',\n       'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n       'highway-mpg', 'price'],\n      dtype='object')\n\n\n\n\n2.3.3 Save the dataset\nCorrespondingly, Pandas enables us to save the dataset to csv. By using the dataframe.to_csv() method, you can add the file path and name along with quotation marks in the brackets.\nFor example, if you would save the dataframe df as automobile.csv to your local machine, you may use the syntax below, where index = False means the row names will not be written.\n\ndf.to_csv(\"../data/automobile.csv\", index=False)\n\nWe can also read and save other file formats. We can use similar functions like pd.read_csv() and df.to_csv() for other data formats. The functions are listed in the following table:\n\nRead/Save Other Data Formats\n\n\n\n\nData Formate\nRead\nSave\n\n\n\n\ncsv\npd.read_csv()\ndf.to_csv()\n\n\njson\npd.read_json()\ndf.to_json()\n\n\nexcel\npd.read_excel()\ndf.to_excel()\n\n\nhdf\npd.read_hdf()\ndf.to_hdf()\n\n\nsql\npd.read_sql()\ndf.to_sql()\n\n\n…\n…\n…"
  },
  {
    "objectID": "code/1_importing_data.html#basic-data-analysis",
    "href": "code/1_importing_data.html#basic-data-analysis",
    "title": "2  Importing datasets",
    "section": "2.4 Basic data analysis",
    "text": "2.4 Basic data analysis\nPandas has several built-in methods that can be used to understand the datatype or features or to look at the distribution of data within the dataset. Using these methods, gives an overview of the dataset and also point out potential issues such as the wrong data type of features which may need to be resolved later on.\n\n2.4.1 Data types\nData has a variety of types. The main types stored in Pandas dataframes are object, float, int, bool and datetime64. In order to better learn about each attribute, it is always good for us to know the data type of each column.\nWhy check data type:\n\nPandas automatically assigns types based on the encoding it detects from the original data table. For a number of reasons, this assignment may be incorrect. For example, it should be awkward if the car price column which we should expect to contain continuous numeric numbers, is assigned the data type of object. It would be more natural for it to have the float type.\nAllows a data scientists to see which Python functions can be applied to a specific column. For example, some math functions can only be applied to numerical data.\n\nAs shown below, it is clear to see that the data type of “symboling” and “curb-weight” are int64, “normalized-losses” is object, and “wheel-base” is float64, etc.\n\n#check the datatypes\nprint(df.dtypes)\n\nsymboling              int64\nnormalized-losses     object\nmake                  object\nfuel-type             object\naspiration            object\nnum-of-doors          object\nbody-style            object\ndrive-wheels          object\nengine-location       object\nwheel-base           float64\nlength               float64\nwidth                float64\nheight               float64\ncurb-weight            int64\nengine-type           object\nnum-of-cylinders      object\nengine-size            int64\nfuel-system           object\nbore                  object\nstroke                object\ncompression-ratio    float64\nhorsepower            object\npeak-rpm              object\ncity-mpg               int64\nhighway-mpg            int64\nprice                 object\ndtype: object\n\n\n\n\n2.4.2 Describe\nNow, we would like to check the statistical summary of each column to learn about the distribution of data in each column. The statistical metrics can tell the data scientist if there are mathematical issues that may exist such as extreme outliers and large deviations.\nThe data scientists may have to address these issues later. To get the quick statistics, we use the describe method. It returns the number of terms in the column as count, average column value as mean, column standard deviation as std, the maximum minimum values, as well as the boundary of each of the quartiles.\nBy default, this method will provide various summary statistics, excluding NaN (Not a Number) values.\n\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe())\n\n        symboling  wheel-base      length       width      height  \\\ncount  201.000000  201.000000  201.000000  201.000000  201.000000   \nmean     0.840796   98.797015  174.200995   65.889055   53.766667   \nstd      1.254802    6.066366   12.322175    2.101471    2.447822   \nmin     -2.000000   86.600000  141.100000   60.300000   47.800000   \n25%      0.000000   94.500000  166.800000   64.100000   52.000000   \n50%      1.000000   97.000000  173.200000   65.500000   54.100000   \n75%      2.000000  102.400000  183.500000   66.600000   55.500000   \nmax      3.000000  120.900000  208.100000   72.000000   59.800000   \n\n       curb-weight  engine-size  compression-ratio    city-mpg  highway-mpg  \ncount   201.000000   201.000000         201.000000  201.000000   201.000000  \nmean   2555.666667   126.875622          10.164279   25.179104    30.686567  \nstd     517.296727    41.546834           4.004965    6.423220     6.815150  \nmin    1488.000000    61.000000           7.000000   13.000000    16.000000  \n25%    2169.000000    98.000000           8.600000   19.000000    25.000000  \n50%    2414.000000   120.000000           9.000000   24.000000    30.000000  \n75%    2926.000000   141.000000           9.400000   30.000000    34.000000  \nmax    4066.000000   326.000000          23.000000   49.000000    54.000000  \n\n\nThis shows the statistical summary of all numeric-typed (int, float) columns. For example, the attribute “symboling” has 205 counts, the mean value of this column is 0.83, the standard deviation is 1.25, the minimum value is -2, 25th percentile is 0, 50th percentile is 1, 75th percentile is 2, and the maximum value is 3.\nHowever, what if we would also like to check all the columns including those that are of type object?\nYou can add an argument include = “all” inside the bracket. Let’s try it again.\nWe can also do this on all datatype using include = 'all'.\n\n#return a statistical summary of each column to learn about data distribution\nprint(df.describe(include = 'all'))\n\n         symboling normalized-losses    make fuel-type aspiration  \\\ncount   201.000000               164     201       201        201   \nunique         NaN                51      22         2          2   \ntop            NaN               161  toyota       gas        std   \nfreq           NaN                11      32       181        165   \nmean      0.840796               NaN     NaN       NaN        NaN   \nstd       1.254802               NaN     NaN       NaN        NaN   \nmin      -2.000000               NaN     NaN       NaN        NaN   \n25%       0.000000               NaN     NaN       NaN        NaN   \n50%       1.000000               NaN     NaN       NaN        NaN   \n75%       2.000000               NaN     NaN       NaN        NaN   \nmax       3.000000               NaN     NaN       NaN        NaN   \n\n       num-of-doors body-style drive-wheels engine-location  wheel-base  ...  \\\ncount           199        201          201             201  201.000000  ...   \nunique            2          5            3               2         NaN  ...   \ntop            four      sedan          fwd           front         NaN  ...   \nfreq            113         94          118             198         NaN  ...   \nmean            NaN        NaN          NaN             NaN   98.797015  ...   \nstd             NaN        NaN          NaN             NaN    6.066366  ...   \nmin             NaN        NaN          NaN             NaN   86.600000  ...   \n25%             NaN        NaN          NaN             NaN   94.500000  ...   \n50%             NaN        NaN          NaN             NaN   97.000000  ...   \n75%             NaN        NaN          NaN             NaN  102.400000  ...   \nmax             NaN        NaN          NaN             NaN  120.900000  ...   \n\n        engine-size  fuel-system  bore  stroke compression-ratio horsepower  \\\ncount    201.000000          201   197     197        201.000000        199   \nunique          NaN            8    38      36               NaN         58   \ntop             NaN         mpfi  3.62    3.40               NaN         68   \nfreq            NaN           92    23      19               NaN         19   \nmean     126.875622          NaN   NaN     NaN         10.164279        NaN   \nstd       41.546834          NaN   NaN     NaN          4.004965        NaN   \nmin       61.000000          NaN   NaN     NaN          7.000000        NaN   \n25%       98.000000          NaN   NaN     NaN          8.600000        NaN   \n50%      120.000000          NaN   NaN     NaN          9.000000        NaN   \n75%      141.000000          NaN   NaN     NaN          9.400000        NaN   \nmax      326.000000          NaN   NaN     NaN         23.000000        NaN   \n\n        peak-rpm    city-mpg highway-mpg price  \ncount        199  201.000000  201.000000   201  \nunique        22         NaN         NaN   186  \ntop         5500         NaN         NaN  8921  \nfreq          36         NaN         NaN     2  \nmean         NaN   25.179104   30.686567   NaN  \nstd          NaN    6.423220    6.815150   NaN  \nmin          NaN   13.000000   16.000000   NaN  \n25%          NaN   19.000000   25.000000   NaN  \n50%          NaN   24.000000   30.000000   NaN  \n75%          NaN   30.000000   34.000000   NaN  \nmax          NaN   49.000000   54.000000   NaN  \n\n[11 rows x 26 columns]\n\n\nWe see that for the object type columns, a different set of statistics is evaluated, like unique, top, and frequency. Unique is the number of distinct objects in the column. Top is most frequently occurring object, and freq is the number of times the top object appears in the column.\nSome values in the table are shown here as NaN which stands for not a number. This is because that particular statistical metric cannot be calculated for that specific column data type.\n\n\n2.4.3 Selecting columns\nYou can select the columns of a dataframe by indicating the name of each column. For example, you can select the three columns as follows:\ndataframe[[' column 1 ',column 2', 'column 3']]\nWhere “column” is the name of the column, you can apply the method “.describe()” to get the statistics of those columns as follows:\ndataframe[[' column 1 ',column 2', 'column 3'] ].describe()\nFor example, lets use the describe method to the columns length and compression-ratio\n\ndf[['length','compression-ratio']].describe()\n\n\n\n\n\n  \n    \n      \n      length\n      compression-ratio\n    \n  \n  \n    \n      count\n      201.000000\n      201.000000\n    \n    \n      mean\n      174.200995\n      10.164279\n    \n    \n      std\n      12.322175\n      4.004965\n    \n    \n      min\n      141.100000\n      7.000000\n    \n    \n      25%\n      166.800000\n      8.600000\n    \n    \n      50%\n      173.200000\n      9.000000\n    \n    \n      75%\n      183.500000\n      9.400000\n    \n    \n      max\n      208.100000\n      23.000000\n    \n  \n\n\n\n\n\n\n2.4.4 info\nThe info method provides a concise summary of your DataFrame.\nThis method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.\n\nprint(df.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 201 entries, 0 to 204\nData columns (total 26 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   symboling          201 non-null    int64  \n 1   normalized-losses  164 non-null    object \n 2   make               201 non-null    object \n 3   fuel-type          201 non-null    object \n 4   aspiration         201 non-null    object \n 5   num-of-doors       199 non-null    object \n 6   body-style         201 non-null    object \n 7   drive-wheels       201 non-null    object \n 8   engine-location    201 non-null    object \n 9   wheel-base         201 non-null    float64\n 10  length             201 non-null    float64\n 11  width              201 non-null    float64\n 12  height             201 non-null    float64\n 13  curb-weight        201 non-null    int64  \n 14  engine-type        201 non-null    object \n 15  num-of-cylinders   201 non-null    object \n 16  engine-size        201 non-null    int64  \n 17  fuel-system        201 non-null    object \n 18  bore               197 non-null    object \n 19  stroke             197 non-null    object \n 20  compression-ratio  201 non-null    float64\n 21  horsepower         199 non-null    object \n 22  peak-rpm           199 non-null    object \n 23  city-mpg           201 non-null    int64  \n 24  highway-mpg        201 non-null    int64  \n 25  price              201 non-null    object \ndtypes: float64(5), int64(5), object(16)\nmemory usage: 42.4+ KB\nNone"
  },
  {
    "objectID": "code/1_importing_data.html#accessing-databases-with-python",
    "href": "code/1_importing_data.html#accessing-databases-with-python",
    "title": "2  Importing datasets",
    "section": "2.5 Accessing databases with python",
    "text": "2.5 Accessing databases with python\nThe Python code connects to the database using API calls. An application programming interface is a set of functions that you can call to get access to some type of service.\n\n2.5.1 SQL API\nThe SQL API consists of library function calls as an application programming interface, API, for the DBMS.\nTo pass SQL statements to the DBMS, an application program calls functions in the API, and it calls other functions to retrieve query results and status information from the DBMS.\n\n\n\n\nThe application program begins its database access with one or more API calls that connect the program to the DBMS.\nTo send the SQL statement to the DBMS, the program builds the statement as a text string in a buffer and then makes an API call to pass the buffer contents to the DBMS.\nThe application program makes API calls to check the status of its DBMS request and to handle errors.\nThe application program ends its database access with an API call that disconnects it from the database.\n\n\n\n2.5.2 What is a DB-API\nDB-API is Python’s standard API for accessing relational databases. It is a standard that allows you to write a single program that works with multiple kinds of relational databases instead of writing a separate program for each one. So, if you learn the DB-API functions, then you can apply that knowledge to use any database with Python.\nThe two main concepts in the Python DB-API are:\n\nconnection objects that you use to connect to a database and manage your transactions.\nCursor objects are used to run queries. You open a cursor object and then run queries. The cursor works similar to a cursor in a text processing system where you scroll down in your result set and get your data into the application. Cursors are used to scan through the results of a database.\n\nThe methods used with connection objects are:\n\nThe cursor() method returns a new cursor object using the connection.\nThe commit() method is used to commit any pending transaction to the database.\nThe rollback() method causes the database to roll back to the start of any pending transaction.\nThe close() method is used to close a database connection.\n\nAs an example: First, you import your database module by using the connect API from that module. To open a connection to the database, you use the connection function and pass in the parameters that is, the database name, username, and password. The connect function returns connection object. After this, you create a cursor object on the connection object. The cursor is used to run queries and fetch results. After running the queries using the cursor, we also use the cursor to fetch the results of the query. Finally, when the system is done running the queries, it frees all resources by closing the connection."
  },
  {
    "objectID": "code/2_preprocessing_of_data.html",
    "href": "code/2_preprocessing_of_data.html",
    "title": "3  Pre-processing of data in python",
    "section": "",
    "text": "Data pre-processing is a necessary step in data analysis. It is the process of converting or mapping data from one raw form into another format to make it ready for further analysis.\nData pre-processing is often called data cleaning or data wrangling.\n\nIdentifying and handle missing values. A missing value condition occurs whenever a data entry is left empty.\nData formatting. Data from different sources maybe in various formats, in different units, or in various conventions.\nData normalization (centering/scaling). Different columns of numerical data may have very different ranges and direct comparison is often not meaningful. Normalization is a way to bring all data into a similar range for more useful comparison.\nData binning. Binning creates bigger categories from a set of numerical values. It is particularly useful for comparison between groups of data.\nTurning categorical values to numeric variables to make statistical modeling easier\n\nLet’s start with loading our test data\nYou can find the “Automobile Dataset” from the following link: https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data. We will be using this dataset throughout this course.\n\nimport sys\nsys.executable\n\n'/opt/anaconda3/bin/python3'\n\n\n\n#load libs\nimport pandas as pd\nimport numpy as np\n\n#get the data url\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\"\n\n#add a header\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n\n#download data\ndf = pd.read_csv(url, names = headers)\ndf.head(5)\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      ?\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500\n      18\n      22\n      17450\n    \n  \n\n5 rows × 26 columns"
  },
  {
    "objectID": "code/2_preprocessing_of_data.html#dealing-with-missing-values",
    "href": "code/2_preprocessing_of_data.html#dealing-with-missing-values",
    "title": "3  Pre-processing of data in python",
    "section": "3.2 Dealing with missing values",
    "text": "3.2 Dealing with missing values\n\nA missing value condition occurs whenever a data entry is left empty\nCan be represented as: ?, N/A, 0 or a blank cell\n\nTypical options to consider to deal with missing data:\n\nCheck with the collection source to find missing values\nRemove data were missing value is found, here we can do either drop the whole row or column (decide on what has the least amount of impact)\n\ndrop the variable\ndrop the data entry\n\nReplace the missing values\n\nreplace with the average (of similar data points)\nreplace it by frequency\nreplace it based on other functions\n\nLeave it as missing data\n\nNext, let’s first convert ? to NaN\n\n# replace \"?\" to NaN\ndf.replace(\"?\", np.nan, inplace = True)\ndf.head(5)\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      3\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      NaN\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500\n      18\n      22\n      17450\n    \n  \n\n5 rows × 26 columns\n\n\n\n\n3.2.1 Using dropna()\nTo remove data that contains missing values Panda’s library has a built-in method called dropna. Essentially, with the dropna method, you can choose to drop rows or columns that contain missing values like NaN.\n\naxis = 0 –> drop the entire row (default)\naxis = 1 –> drop the entire column\ninpalce = True –> modification is done on the dataset directly\n\ndataframes.dropna()\nIf we want to remove rows based on a specific column\ndataframes.dropna(subset = ['price'], axis = 0, inplace = True)\n\n\n3.2.2 Replace missing values using replace()\ndataframe.replace(missing_value, new_value)\nReplace a value with the mean of a column:\nmean = df['prices'].mean()\ndf['prices'].replace(np.nan, mean)\n\n\n3.2.3 Practical\n\n3.2.3.1 Identify missing data\nNow, let’s find out if we have any missing data.\nThe missing values are converted by default. We use the following functions to identify these missing values. There are two methods to detect missing data:\n\n.isnull()\n.notnull()\n\nThe output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. “True” means the value is a missing value while “False” means the value is not a missing value\n\nmissing_data = df.isnull()\nmissing_data.head(5)\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      1\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n5 rows × 26 columns\n\n\n\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, “True” represents a missing value and “False” means the value is present in the dataset. In the body of the for loop the method “.value_counts()” counts the number of “True” values.\n\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\") \n\nsymboling\nFalse    205\nName: symboling, dtype: int64\n\nnormalized-losses\nFalse    164\nTrue      41\nName: normalized-losses, dtype: int64\n\nmake\nFalse    205\nName: make, dtype: int64\n\nfuel-type\nFalse    205\nName: fuel-type, dtype: int64\n\naspiration\nFalse    205\nName: aspiration, dtype: int64\n\nnum-of-doors\nFalse    203\nTrue       2\nName: num-of-doors, dtype: int64\n\nbody-style\nFalse    205\nName: body-style, dtype: int64\n\ndrive-wheels\nFalse    205\nName: drive-wheels, dtype: int64\n\nengine-location\nFalse    205\nName: engine-location, dtype: int64\n\nwheel-base\nFalse    205\nName: wheel-base, dtype: int64\n\nlength\nFalse    205\nName: length, dtype: int64\n\nwidth\nFalse    205\nName: width, dtype: int64\n\nheight\nFalse    205\nName: height, dtype: int64\n\ncurb-weight\nFalse    205\nName: curb-weight, dtype: int64\n\nengine-type\nFalse    205\nName: engine-type, dtype: int64\n\nnum-of-cylinders\nFalse    205\nName: num-of-cylinders, dtype: int64\n\nengine-size\nFalse    205\nName: engine-size, dtype: int64\n\nfuel-system\nFalse    205\nName: fuel-system, dtype: int64\n\nbore\nFalse    201\nTrue       4\nName: bore, dtype: int64\n\nstroke\nFalse    201\nTrue       4\nName: stroke, dtype: int64\n\ncompression-ratio\nFalse    205\nName: compression-ratio, dtype: int64\n\nhorsepower\nFalse    203\nTrue       2\nName: horsepower, dtype: int64\n\npeak-rpm\nFalse    203\nTrue       2\nName: peak-rpm, dtype: int64\n\ncity-mpg\nFalse    205\nName: city-mpg, dtype: int64\n\nhighway-mpg\nFalse    205\nName: highway-mpg, dtype: int64\n\nprice\nFalse    201\nTrue       4\nName: price, dtype: int64\n\n\n\nBased on the summary above, each column has 205 rows of data and seven of the columns containing missing data:\n“normalized-losses”: 41 missing data “num-of-doors”: 2 missing data “bore”: 4 missing data “stroke” : 4 missing data “horsepower”: 2 missing data “peak-rpm”: 2 missing data “price”: 4 missing data\nNow, we have to decide what to do with this missing data. Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:\nReplace by mean:\n\n“normalized-losses”: 41 missing data, replace them with mean\n“stroke”: 4 missing data, replace them with mean\n“bore”: 4 missing data, replace them with mean\n“horsepower”: 2 missing data, replace them with mean\n“peak-rpm”: 2 missing data, replace them with mean\n\nReplace by frequency:\n\n“num-of-doors”: 2 missing data, replace them with “four”.\n\nReason: 84% sedans is four doors. Since four doors is most frequent, it is most likely to occur\n\n\nDrop the whole row:\n\n“price”: 4 missing data, simply delete the whole row\n\nReason: price is what we want to predict. Any data entry without price data cannot be used for prediction; therefore any row now without price data is not useful to us\n\n\n\n\n3.2.3.2 Deal with the “normalized-losses” column\n\navg_norm_loss = df['normalized-losses'].astype('float').mean(axis = 0)\nprint(\"Average of normalized losses:\", avg_norm_loss)\n\nAverage of normalized losses: 122.0\n\n\n\ndf['normalized-losses'].replace(np.nan, avg_norm_loss, inplace = True)\n\n\n\n3.2.3.3 Deal with the “bore” column¶\n\navg_bore = df['bore'].astype('float').mean(axis = 0)\nprint(\"Average of bore:\", avg_bore)\n\nAverage of bore: 3.3297512437810957\n\n\n\ndf['bore'].replace(np.nan, avg_bore, inplace = True)\n\n\n\n3.2.3.4 Deal with the “stroke” column\n\navg_bore = df['bore'].astype('float').mean(axis = 0)\nprint(\"Average of bore:\", avg_bore)\n\ndf['bore'].replace(np.nan, avg_bore, inplace = True)\n\nAverage of bore: 3.329751243781096\n\n\n\n\n3.2.3.5 Deal with the “horsepower” column\n\navg_horsepower = df['horsepower'].astype('float').mean(axis = 0)\nprint(\"Average of horsepower:\", avg_horsepower)\n\ndf['horsepower'].replace(np.nan, avg_horsepower, inplace = True)\n\nAverage of horsepower: 104.25615763546799\n\n\n\n\n3.2.3.6 Deal with the “peak-rpm” column\n\navg_peakrpm = df['peak-rpm'].astype('float').mean(axis = 0)\nprint(\"Average of peak-rpm:\", avg_peakrpm)\n\ndf['peak-rpm'].replace(np.nan, avg_peakrpm, inplace = True)\n\nAverage of peak-rpm: 5125.369458128079\n\n\n\n\n3.2.3.7 Replace missing door values with frequency\nTo see which values are present in a particular column, we can use the .value_counts() method:\n\ndf['num-of-doors'].value_counts()\n\nfour    114\ntwo      89\nName: num-of-doors, dtype: int64\n\n\nWe can see that four doors are the most common type. We can also use the “.idxmax()” method to calculate the most common type automatically:\n\ndoors_to_replace = df['num-of-doors'].value_counts().idxmax()\ndoors_to_replace\n\n'four'\n\n\nNow we can replace our missing values:\n\ndf['num-of-doors'].replace(np.nan,doors_to_replace, inplace = True )\n\n\n\n3.2.3.8 Drop rows for the price column\n\n#drop nan\ndf.dropna(subset = ['price'], axis = 0 , inplace = True)\n\n#reset index, because we dropped two ros\ndf.reset_index(drop = True, inplace = True)\n\n#view df\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      engine-size\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n    \n  \n  \n    \n      0\n      3\n      122.0\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      13495\n    \n    \n      1\n      3\n      122.0\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      130\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000\n      21\n      27\n      16500\n    \n    \n      2\n      1\n      122.0\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      152\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000\n      19\n      26\n      16500\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      109\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500\n      24\n      30\n      13950\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      136\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500\n      18\n      22\n      17450\n    \n  \n\n5 rows × 26 columns"
  },
  {
    "objectID": "code/2_preprocessing_of_data.html#data-formatting",
    "href": "code/2_preprocessing_of_data.html#data-formatting",
    "title": "3  Pre-processing of data in python",
    "section": "3.3 Data formatting",
    "text": "3.3 Data formatting\nData is usually collected from different places by different people which may be stored in different formats. Data formatting means bringing data into a common standard of expression that allows users to make meaningful comparisons. As a part of dataset cleaning, data formatting ensures the data is consistent and easily understandable.\nFor example, people may use different expressions to represent New York City, such as uppercase N uppercase Y, uppercase N lowercase y, uppercase N uppercase Y and New York.\nFormatted data is:\n\nmore clear\neasier to aggregate\neasier to compare\n\n\n3.3.1 Incorrect data types\nSometimes the wrong data type is assigned to a feature. For example a number might be stored in an object, thus we could not use the mean function, without correcting this.\nTo identify a datatype:\ndf.dtypes()\nTo convert dataframes:\ndf.astype()\n\n\n3.3.2 Practical\n\n3.3.2.1 Deal with incorrect data types\nThe last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other).\nIn Pandas, we use:\n\n.dtype() to check the data type\n.astype() to change the data type\n\nLet’s check the data types for each column:\n\ndf.dtypes\n\nsymboling              int64\nnormalized-losses     object\nmake                  object\nfuel-type             object\naspiration            object\nnum-of-doors          object\nbody-style            object\ndrive-wheels          object\nengine-location       object\nwheel-base           float64\nlength               float64\nwidth                float64\nheight               float64\ncurb-weight            int64\nengine-type           object\nnum-of-cylinders      object\nengine-size            int64\nfuel-system           object\nbore                  object\nstroke                object\ncompression-ratio    float64\nhorsepower            object\npeak-rpm              object\ncity-mpg               int64\nhighway-mpg            int64\nprice                 object\ndtype: object\n\n\nAs we can see above, some columns are not of the correct data type. Numerical variables should have type ‘float’ or ‘int’, and variables with strings such as categories should have type ‘object’.\nFor example, ‘bore’ and ‘stroke’ variables are numerical values that describe the engines, so we should expect them to be of the type ‘float’ or ‘int’; however, they are shown as type ‘object’. We have to convert data types into a proper format for each column using the “astype()” method.\n\ndf[['bore', 'stroke']] = df[['bore', 'stroke']].astype('float')\ndf[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")\ndf[[\"price\"]] = df[[\"price\"]].astype(\"float\")\ndf[[\"peak-rpm\"]] = df[[\"peak-rpm\"]].astype(\"float\")\n\n#check if that worked \ndf.dtypes\n\nsymboling              int64\nnormalized-losses      int64\nmake                  object\nfuel-type             object\naspiration            object\nnum-of-doors          object\nbody-style            object\ndrive-wheels          object\nengine-location       object\nwheel-base           float64\nlength               float64\nwidth                float64\nheight               float64\ncurb-weight            int64\nengine-type           object\nnum-of-cylinders      object\nengine-size            int64\nfuel-system           object\nbore                 float64\nstroke               float64\ncompression-ratio    float64\nhorsepower            object\npeak-rpm             float64\ncity-mpg               int64\nhighway-mpg            int64\nprice                float64\ndtype: object"
  },
  {
    "objectID": "code/2_preprocessing_of_data.html#data-normalization",
    "href": "code/2_preprocessing_of_data.html#data-normalization",
    "title": "3  Pre-processing of data in python",
    "section": "3.4 Data normalization",
    "text": "3.4 Data normalization\n\nUniforms the features value with different ranges: if data are in different ranges, such as age 20-40 and income 10000 to 50000, are hard to compare and income will influence the result more but it might not be more important as a data predictor.\nWe could normalize all values to be in a range from 0-1\n\nThere are several approches:\n\nSimple feature scaling: divides each value by the maximum value for that feature. This makes the new values range between zero and one.\n\ndf['length'] = df['length'] /df['length'].max()\n\nMin-max: each value X_old subtract it from the minimum value of that feature, then divides by the range of that feature\n\ndf['length'] = (df['length'] - df['length'].min())/\n                (df['length'].max()-df['length'].min())\n\nZ-score: for each value you subtract the mu which is the average of the feature, and then divide by the standard deviation sigma.\n\ndf['length'] = (df['length']-df['length'],mean()).df['length'].std()\n\n3.4.1 Practical\n\n3.4.1.1 Data Standardization\nIn our dataset, the fuel consumption columns “city-mpg” and “highway-mpg” are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accepts the fuel consumption with L/100km standard.\nWe will need to apply data transformation to transform mpg into L/100km.\nThe formula for unit conversion is:\nL/100km = 235 / mpg\nWe can do many mathematical operations directly in Pandas.\n\n# Convert mpg to L/100km by mathematical operation (235 divided by mpg)\ndf['city-L/100km'] = 235/df['city-mpg']\n\n#view transformed data\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      fuel-system\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      mpfi\n      3.47\n      2.68\n      9.0\n      111\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      mpfi\n      2.68\n      3.47\n      9.0\n      154\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      mpfi\n      3.19\n      3.40\n      10.0\n      102\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      mpfi\n      3.19\n      3.40\n      8.0\n      115\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n    \n  \n\n5 rows × 27 columns\n\n\n\nNext, transform mpg to L/100km in the column of “highway-mpg” and change the name of column to “highway-L/100km”.\n\n# Convert mpg to L/100km by mathematical operation (235 divided by mpg)\ndf['highway-L/100km'] = 235/df['highway-mpg']\n\n#view transformed data\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      highway-L/100km\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      3.47\n      2.68\n      9.0\n      111\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      8.703704\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      3.47\n      2.68\n      9.0\n      111\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      8.703704\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      2.68\n      3.47\n      9.0\n      154\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      9.038462\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      3.19\n      3.40\n      10.0\n      102\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      7.833333\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      3.19\n      3.40\n      8.0\n      115\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      10.681818\n    \n  \n\n5 rows × 28 columns\n\n\n\n\n\n3.4.1.2 Data normalization\nTo demonstrate normalization, let’s say we want to scale the columns “length”, “width” and “height”.\nTarget: would like to normalize those variables so their value ranges from 0 to 1 Approach: replace original value by (original value)/(maximum value)\n\n# replace (original value) by (original value)/(maximum value)\ndf['length'] = df['length']/df['length'].max()\ndf['width'] = df['width']/df['width'].max()\ndf['height'] = df['height']/df['height'].max()\n\n\ndf[['length','width', 'height']].head()\n\n\n\n\n\n  \n    \n      \n      length\n      width\n      height\n    \n  \n  \n    \n      0\n      0.811148\n      0.890278\n      0.816054\n    \n    \n      1\n      0.811148\n      0.890278\n      0.816054\n    \n    \n      2\n      0.822681\n      0.909722\n      0.876254\n    \n    \n      3\n      0.848630\n      0.919444\n      0.908027\n    \n    \n      4\n      0.848630\n      0.922222\n      0.908027"
  },
  {
    "objectID": "code/2_preprocessing_of_data.html#binning",
    "href": "code/2_preprocessing_of_data.html#binning",
    "title": "3  Pre-processing of data in python",
    "section": "3.5 Binning",
    "text": "3.5 Binning\n\nBinning: Grouping values into bins, i.e. we can bin “age” into [0 to 5], [6 to 10], [11 to 15] and so on\nCan sometimes increase the accuracy of models\nConverts numeric into categorical variables, i.e. the different prices can be grouped into low, medium and high\nGroup a set of numerical values into a set of bins\n\nWe can use the numpy function “linspace” to return the array “bins” that contains 4 equally spaced numbers over the specified interval of the price. We create a list “group_names “ that contains the different bin names. We use the pandas function”cut” to segment and sort the data values into bins.\nbin = np.linspace(min(df['price']),max(df['price']),4)\ngroup_names = ['Low', 'Medium', 'High']\ndf['price_binned'] = pd.cut(df['price'], bins, labels = group_names, include_lowest = True)\nThe distribution of data can be visualized using histograms.\n\n3.5.1 Practical\n\n3.5.1.1 Defining bins\nIn our dataset, “horsepower” is a real valued variable ranging from 48 to 288 and it has 59 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three ‘bins’ to simplify analysis?\n\n#check data format\nprint(df['horsepower'].dtypes)\n\n#convert the data to the correct format\ndf['horsepower'] = df['horsepower'].astype(int, copy=True)\n\nobject\n\n\nLet’s plot the histogram of horsepower to see what the distribution of horsepower looks like.\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#define what we want to plot\nplt.hist(df[\"horsepower\"])\n\n#set axis labels\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\n#plot\nplt.show()\nplt.close()\n\n\n\n\nWe would like 3 bins of equal size bandwidth so we use numpy’s linspace(start_value, end_value, numbers_generated function.\n\nSince we want to include the minimum value of horsepower, we want to set start_value = min(df[“horsepower”]).\nSince we want to include the maximum value of horsepower, we want to set end_value = max(df[“horsepower”]).\nSince we are building 3 bins of equal length, there should be 4 dividers, so numbers_generated = 4.\n\nWe build a bin array with a minimum value to a maximum value by using the bandwidth calculated above. The values will determine when one bin ends and another begins.\n\nbins = np.linspace(min(df['horsepower']), max(df['horsepower']),4)\nbins\n\narray([ 48.        , 119.33333333, 190.66666667, 262.        ])\n\n\nWe set group names:\n\ngroup_names = ['Low' , 'Medium', 'High']\n\nWe apply the function “cut” to determine what each value of df[‘horsepower’] belongs to.\n\ndf['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels = group_names, include_lowest = True)\ndf[['horsepower', 'horsepower-binned']].head(10)\n\n\n\n\n\n  \n    \n      \n      horsepower\n      horsepower-binned\n    \n  \n  \n    \n      0\n      111\n      Low\n    \n    \n      1\n      111\n      Low\n    \n    \n      2\n      154\n      Medium\n    \n    \n      3\n      102\n      Low\n    \n    \n      4\n      115\n      Low\n    \n    \n      5\n      110\n      Low\n    \n    \n      6\n      110\n      Low\n    \n    \n      7\n      110\n      Low\n    \n    \n      8\n      140\n      Medium\n    \n    \n      9\n      101\n      Low\n    \n  \n\n\n\n\nLet’s see the number of vehicles in each bin:\n\ndf['horsepower-binned'].value_counts()\n\nLow       153\nMedium     43\nHigh        5\nName: horsepower-binned, dtype: int64\n\n\nLet’s plot the distribution of each bin:\n\n#define the bargraph\nplt.bar(group_names, df['horsepower-binned'].value_counts())\n\n#set labels\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\n#print\nplt.show()\nplt.close()\n\n\n\n\n\n\n3.5.1.2 Visualizing bins\n\n# draw historgram of attribute \"horsepower\" with bins = 3\nplt.hist(df[\"horsepower\"], bins = 3)\n\n# set x/y labels and plot title\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"count\")\nplt.title(\"horsepower bins\")\n\nplt.show()\nplt.close()"
  },
  {
    "objectID": "code/2_preprocessing_of_data.html#turning-categorical-variables-into-quantitative-variables",
    "href": "code/2_preprocessing_of_data.html#turning-categorical-variables-into-quantitative-variables",
    "title": "3  Pre-processing of data in python",
    "section": "3.6 Turning categorical variables into quantitative variables",
    "text": "3.6 Turning categorical variables into quantitative variables\n\nMost statistical models cannot take in objects or strings as input and for model training only take the numbers as inputs.\nWe encode the values by adding new features corresponding to each unique element in the original feature we would like to encode. I.e if we have two unique values, gas and diesel, we create two new features and assign 0 and 1 to each category. This technique is also called one-hot encoding.\npd.get_dummies() method converts categorical variables to dummy variables, such as 0 and 1\n\nIndicator value: An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called ‘dummies’ because the numbers themselves don’t have inherent meaning.\nWe use indicator variables so we can use categorical variables for regression analysis in the later modules.\npd.get_dummies(df['fuel'])\n\n3.6.1 Practical\nIn this practical we want to answer the question: What is the fuel consumption (L/100k) rate for the diesel car?\nWe see the column “fuel-type” has two unique values: “gas” or “diesel”. Regression doesn’t understand words, only numbers. To use this attribute in regression analysis, we convert “fuel-type” to indicator variables.\nWe will use pandas’ method ‘get_dummies’ to assign numerical values to different categories of fuel type.\n\ndummy_variable_1 = pd.get_dummies(df['fuel-type'])\ndummy_variable_1.head()\n\n\n\n\n\n  \n    \n      \n      diesel\n      gas\n    \n  \n  \n    \n      0\n      0\n      1\n    \n    \n      1\n      0\n      1\n    \n    \n      2\n      0\n      1\n    \n    \n      3\n      0\n      1\n    \n    \n      4\n      0\n      1\n    \n  \n\n\n\n\nChange the column names for clarity\n\ndummy_variable_1.rename(columns={'gas':'fuel-type-gas', 'diesel':'fuel-type-diesel'}, inplace=True)\ndummy_variable_1.head()\n\n\n\n\n\n  \n    \n      \n      fuel-type-diesel\n      fuel-type-gas\n    \n  \n  \n    \n      0\n      0\n      1\n    \n    \n      1\n      0\n      1\n    \n    \n      2\n      0\n      1\n    \n    \n      3\n      0\n      1\n    \n    \n      4\n      0\n      1\n    \n  \n\n\n\n\nReplace the dummy df with our original gas column\n\n# merge data frame \"df\" and \"dummy_variable_1\" \ndf = pd.concat([df, dummy_variable_1], axis = 1)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      fuel-type\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      ...\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      highway-L/100km\n      horsepower-binned\n      fuel-type-diesel\n      fuel-type-gas\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      111\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      8.703704\n      Low\n      0\n      1\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      gas\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      ...\n      111\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      8.703704\n      Low\n      0\n      1\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      gas\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      ...\n      154\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      9.038462\n      Medium\n      0\n      1\n    \n    \n      3\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      ...\n      102\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      7.833333\n      Low\n      0\n      1\n    \n    \n      4\n      2\n      164\n      audi\n      gas\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      ...\n      115\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      10.681818\n      Low\n      0\n      1\n    \n  \n\n5 rows × 31 columns\n\n\n\nDrop original column “fuel-type” from “df”\n\ndf.drop(\"fuel-type\", axis = 1, inplace = True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      length\n      ...\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      highway-L/100km\n      horsepower-binned\n      fuel-type-diesel\n      fuel-type-gas\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      111\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      8.703704\n      Low\n      0\n      1\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      111\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      8.703704\n      Low\n      0\n      1\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      0.822681\n      ...\n      154\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      9.038462\n      Medium\n      0\n      1\n    \n    \n      3\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      0.848630\n      ...\n      102\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      7.833333\n      Low\n      0\n      1\n    \n    \n      4\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      0.848630\n      ...\n      115\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      10.681818\n      Low\n      0\n      1\n    \n  \n\n5 rows × 30 columns\n\n\n\nSimilar to before, create an indicator variable for the column “aspiration”\n\n#create dummy\ndummy_variable_2 = pd.get_dummies(df['aspiration'])\n\n#change the columns\ndummy_variable_2.rename(columns = {'std': 'aspiration-std', 'turbo':'aspiration-turbo'}, inplace = True)\n\nprint(dummy_variable_2.head())\n\n#combine with previous df\ndf = pd.concat([df,dummy_variable_2], axis = 1)\n\n#drop original aspiration column\ndf.drop('aspiration', axis = 1, inplace = True)\n\nprint(df.head())\n\n   aspiration-std  aspiration-turbo\n0               1                 0\n1               1                 0\n2               1                 0\n3               1                 0\n4               1                 0\n   symboling  normalized-losses         make num-of-doors   body-style  \\\n0          3                122  alfa-romero          two  convertible   \n1          3                122  alfa-romero          two  convertible   \n2          1                122  alfa-romero          two    hatchback   \n3          2                164         audi         four        sedan   \n4          2                164         audi         four        sedan   \n\n  drive-wheels engine-location  wheel-base    length     width  ...  city-mpg  \\\n0          rwd           front        88.6  0.811148  0.890278  ...        21   \n1          rwd           front        88.6  0.811148  0.890278  ...        21   \n2          rwd           front        94.5  0.822681  0.909722  ...        19   \n3          fwd           front        99.8  0.848630  0.919444  ...        24   \n4          4wd           front        99.4  0.848630  0.922222  ...        18   \n\n   highway-mpg    price city-L/100km  highway-L/100km horsepower-binned  \\\n0           27  13495.0    11.190476         8.703704               Low   \n1           27  16500.0    11.190476         8.703704               Low   \n2           26  16500.0    12.368421         9.038462            Medium   \n3           30  13950.0     9.791667         7.833333               Low   \n4           22  17450.0    13.055556        10.681818               Low   \n\n   fuel-type-diesel  fuel-type-gas  aspiration-std  aspiration-turbo  \n0                 0              1               1                 0  \n1                 0              1               1                 0  \n2                 0              1               1                 0  \n3                 0              1               1                 0  \n4                 0              1               1                 0  \n\n[5 rows x 31 columns]\n\n\nStore clean df\n\ndf.to_csv(\"../data/clean_df.csv\")"
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html",
    "href": "code/3_exploratory_data_analysis.html",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "",
    "text": "Exploratory data analysis is a preliminary step in data analysis to:\nIn this tutorial we want to answer the key question: What are the main characteristics that have the most impact on the car price?"
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#notebook-setup",
    "href": "code/3_exploratory_data_analysis.html#notebook-setup",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.1 Notebook setup",
    "text": "4.1 Notebook setup\n\n#load libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n#load data\ndf = pd.read_csv(\"../data/automobileEDA.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      length\n      ...\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      horsepower-binned\n      diesel\n      gas\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      Medium\n      0\n      1\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      Medium\n      0\n      1\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      0.822681\n      ...\n      9.0\n      154.0\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      Medium\n      0\n      1\n    \n    \n      3\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      0.848630\n      ...\n      10.0\n      102.0\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      Medium\n      0\n      1\n    \n    \n      4\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      0.848630\n      ...\n      8.0\n      115.0\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      Medium\n      0\n      1\n    \n  \n\n5 rows × 29 columns"
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#analyzing-individual-feature-patterns-using-visualization",
    "href": "code/3_exploratory_data_analysis.html#analyzing-individual-feature-patterns-using-visualization",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.2 Analyzing Individual Feature Patterns Using Visualization",
    "text": "4.2 Analyzing Individual Feature Patterns Using Visualization\nHow to choose the right visualization method?¶\nWhen visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.\n\ndf.dtypes.head()\n\nsymboling             int64\nnormalized-losses     int64\nmake                 object\naspiration           object\nnum-of-doors         object\ndtype: object\n\n\nWe can calculate the correlation between variables of type “int64” or “float64” using the method “corr”. The diagonal elements are always one; we will study correlation more precisely Pearson correlation in-depth at the end of the notebook.\n\ndf.corr()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      wheel-base\n      length\n      width\n      height\n      curb-weight\n      engine-size\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      diesel\n      gas\n    \n  \n  \n    \n      symboling\n      1.000000\n      0.466264\n      -0.535987\n      -0.365404\n      -0.242423\n      -0.550160\n      -0.233118\n      -0.110581\n      -0.140019\n      -0.008245\n      -0.182196\n      0.075819\n      0.279740\n      -0.035527\n      0.036233\n      -0.082391\n      0.066171\n      -0.196735\n      0.196735\n    \n    \n      normalized-losses\n      0.466264\n      1.000000\n      -0.056661\n      0.019424\n      0.086802\n      -0.373737\n      0.099404\n      0.112360\n      -0.029862\n      0.055563\n      -0.114713\n      0.217299\n      0.239543\n      -0.225016\n      -0.181877\n      0.133999\n      0.238567\n      -0.101546\n      0.101546\n    \n    \n      wheel-base\n      -0.535987\n      -0.056661\n      1.000000\n      0.876024\n      0.814507\n      0.590742\n      0.782097\n      0.572027\n      0.493244\n      0.158502\n      0.250313\n      0.371147\n      -0.360305\n      -0.470606\n      -0.543304\n      0.584642\n      0.476153\n      0.307237\n      -0.307237\n    \n    \n      length\n      -0.365404\n      0.019424\n      0.876024\n      1.000000\n      0.857170\n      0.492063\n      0.880665\n      0.685025\n      0.608971\n      0.124139\n      0.159733\n      0.579821\n      -0.285970\n      -0.665192\n      -0.698142\n      0.690628\n      0.657373\n      0.211187\n      -0.211187\n    \n    \n      width\n      -0.242423\n      0.086802\n      0.814507\n      0.857170\n      1.000000\n      0.306002\n      0.866201\n      0.729436\n      0.544885\n      0.188829\n      0.189867\n      0.615077\n      -0.245800\n      -0.633531\n      -0.680635\n      0.751265\n      0.673363\n      0.244356\n      -0.244356\n    \n    \n      height\n      -0.550160\n      -0.373737\n      0.590742\n      0.492063\n      0.306002\n      1.000000\n      0.307581\n      0.074694\n      0.180449\n      -0.062704\n      0.259737\n      -0.087027\n      -0.309974\n      -0.049800\n      -0.104812\n      0.135486\n      0.003811\n      0.281578\n      -0.281578\n    \n    \n      curb-weight\n      -0.233118\n      0.099404\n      0.782097\n      0.880665\n      0.866201\n      0.307581\n      1.000000\n      0.849072\n      0.644060\n      0.167562\n      0.156433\n      0.757976\n      -0.279361\n      -0.749543\n      -0.794889\n      0.834415\n      0.785353\n      0.221046\n      -0.221046\n    \n    \n      engine-size\n      -0.110581\n      0.112360\n      0.572027\n      0.685025\n      0.729436\n      0.074694\n      0.849072\n      1.000000\n      0.572609\n      0.209523\n      0.028889\n      0.822676\n      -0.256733\n      -0.650546\n      -0.679571\n      0.872335\n      0.745059\n      0.070779\n      -0.070779\n    \n    \n      bore\n      -0.140019\n      -0.029862\n      0.493244\n      0.608971\n      0.544885\n      0.180449\n      0.644060\n      0.572609\n      1.000000\n      -0.055390\n      0.001263\n      0.566936\n      -0.267392\n      -0.582027\n      -0.591309\n      0.543155\n      0.554610\n      0.054458\n      -0.054458\n    \n    \n      stroke\n      -0.008245\n      0.055563\n      0.158502\n      0.124139\n      0.188829\n      -0.062704\n      0.167562\n      0.209523\n      -0.055390\n      1.000000\n      0.187923\n      0.098462\n      -0.065713\n      -0.034696\n      -0.035201\n      0.082310\n      0.037300\n      0.241303\n      -0.241303\n    \n    \n      compression-ratio\n      -0.182196\n      -0.114713\n      0.250313\n      0.159733\n      0.189867\n      0.259737\n      0.156433\n      0.028889\n      0.001263\n      0.187923\n      1.000000\n      -0.214514\n      -0.435780\n      0.331425\n      0.268465\n      0.071107\n      -0.299372\n      0.985231\n      -0.985231\n    \n    \n      horsepower\n      0.075819\n      0.217299\n      0.371147\n      0.579821\n      0.615077\n      -0.087027\n      0.757976\n      0.822676\n      0.566936\n      0.098462\n      -0.214514\n      1.000000\n      0.107885\n      -0.822214\n      -0.804575\n      0.809575\n      0.889488\n      -0.169053\n      0.169053\n    \n    \n      peak-rpm\n      0.279740\n      0.239543\n      -0.360305\n      -0.285970\n      -0.245800\n      -0.309974\n      -0.279361\n      -0.256733\n      -0.267392\n      -0.065713\n      -0.435780\n      0.107885\n      1.000000\n      -0.115413\n      -0.058598\n      -0.101616\n      0.115830\n      -0.475812\n      0.475812\n    \n    \n      city-mpg\n      -0.035527\n      -0.225016\n      -0.470606\n      -0.665192\n      -0.633531\n      -0.049800\n      -0.749543\n      -0.650546\n      -0.582027\n      -0.034696\n      0.331425\n      -0.822214\n      -0.115413\n      1.000000\n      0.972044\n      -0.686571\n      -0.949713\n      0.265676\n      -0.265676\n    \n    \n      highway-mpg\n      0.036233\n      -0.181877\n      -0.543304\n      -0.698142\n      -0.680635\n      -0.104812\n      -0.794889\n      -0.679571\n      -0.591309\n      -0.035201\n      0.268465\n      -0.804575\n      -0.058598\n      0.972044\n      1.000000\n      -0.704692\n      -0.930028\n      0.198690\n      -0.198690\n    \n    \n      price\n      -0.082391\n      0.133999\n      0.584642\n      0.690628\n      0.751265\n      0.135486\n      0.834415\n      0.872335\n      0.543155\n      0.082310\n      0.071107\n      0.809575\n      -0.101616\n      -0.686571\n      -0.704692\n      1.000000\n      0.789898\n      0.110326\n      -0.110326\n    \n    \n      city-L/100km\n      0.066171\n      0.238567\n      0.476153\n      0.657373\n      0.673363\n      0.003811\n      0.785353\n      0.745059\n      0.554610\n      0.037300\n      -0.299372\n      0.889488\n      0.115830\n      -0.949713\n      -0.930028\n      0.789898\n      1.000000\n      -0.241282\n      0.241282\n    \n    \n      diesel\n      -0.196735\n      -0.101546\n      0.307237\n      0.211187\n      0.244356\n      0.281578\n      0.221046\n      0.070779\n      0.054458\n      0.241303\n      0.985231\n      -0.169053\n      -0.475812\n      0.265676\n      0.198690\n      0.110326\n      -0.241282\n      1.000000\n      -1.000000\n    \n    \n      gas\n      0.196735\n      0.101546\n      -0.307237\n      -0.211187\n      -0.244356\n      -0.281578\n      -0.221046\n      -0.070779\n      -0.054458\n      -0.241303\n      -0.985231\n      0.169053\n      0.475812\n      -0.265676\n      -0.198690\n      -0.110326\n      0.241282\n      -1.000000\n      1.000000"
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#descriptive-statistics",
    "href": "code/3_exploratory_data_analysis.html#descriptive-statistics",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.3 Descriptive statistics",
    "text": "4.3 Descriptive statistics\n\nDescribe basic features of data\nGive short summaries about the sample and measures of the data\n\nExamples of useful tools in python:\n\n4.3.1 The describe function\nThe describe function automatically computes basic statistics for all continuous variables. Any NaN values are automatically skipped in these statistics.\nThis will show:\n\nthe count of that variable\nthe mean\nthe standard deviation (std)\nthe minimum value\nthe IQR (Interquartile Range: 25%, 50% and 75%)\nthe maximum value\nWe can apply the method “describe” as follows:\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      wheel-base\n      length\n      width\n      height\n      curb-weight\n      engine-size\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      diesel\n      gas\n    \n  \n  \n    \n      count\n      201.000000\n      201.00000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      197.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n      201.000000\n    \n    \n      mean\n      0.840796\n      122.00000\n      98.797015\n      0.837102\n      0.915126\n      53.766667\n      2555.666667\n      126.875622\n      3.330692\n      3.256904\n      10.164279\n      103.405534\n      5117.665368\n      25.179104\n      30.686567\n      13207.129353\n      9.944145\n      0.099502\n      0.900498\n    \n    \n      std\n      1.254802\n      31.99625\n      6.066366\n      0.059213\n      0.029187\n      2.447822\n      517.296727\n      41.546834\n      0.268072\n      0.319256\n      4.004965\n      37.365700\n      478.113805\n      6.423220\n      6.815150\n      7947.066342\n      2.534599\n      0.300083\n      0.300083\n    \n    \n      min\n      -2.000000\n      65.00000\n      86.600000\n      0.678039\n      0.837500\n      47.800000\n      1488.000000\n      61.000000\n      2.540000\n      2.070000\n      7.000000\n      48.000000\n      4150.000000\n      13.000000\n      16.000000\n      5118.000000\n      4.795918\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      101.00000\n      94.500000\n      0.801538\n      0.890278\n      52.000000\n      2169.000000\n      98.000000\n      3.150000\n      3.110000\n      8.600000\n      70.000000\n      4800.000000\n      19.000000\n      25.000000\n      7775.000000\n      7.833333\n      0.000000\n      1.000000\n    \n    \n      50%\n      1.000000\n      122.00000\n      97.000000\n      0.832292\n      0.909722\n      54.100000\n      2414.000000\n      120.000000\n      3.310000\n      3.290000\n      9.000000\n      95.000000\n      5125.369458\n      24.000000\n      30.000000\n      10295.000000\n      9.791667\n      0.000000\n      1.000000\n    \n    \n      75%\n      2.000000\n      137.00000\n      102.400000\n      0.881788\n      0.925000\n      55.500000\n      2926.000000\n      141.000000\n      3.580000\n      3.410000\n      9.400000\n      116.000000\n      5500.000000\n      30.000000\n      34.000000\n      16500.000000\n      12.368421\n      0.000000\n      1.000000\n    \n    \n      max\n      3.000000\n      256.00000\n      120.900000\n      1.000000\n      1.000000\n      59.800000\n      4066.000000\n      326.000000\n      3.940000\n      4.170000\n      23.000000\n      262.000000\n      6600.000000\n      49.000000\n      54.000000\n      45400.000000\n      18.076923\n      1.000000\n      1.000000\n    \n  \n\n\n\n\nThe default setting of “describe” skips variables of type object. We can apply the method “describe” on the variables of type ‘object’ as follows:\n\ndf.describe(include = ['object'])\n\n\n\n\n\n  \n    \n      \n      make\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      engine-type\n      num-of-cylinders\n      fuel-system\n      horsepower-binned\n    \n  \n  \n    \n      count\n      201\n      201\n      201\n      201\n      201\n      201\n      201\n      201\n      201\n      200\n    \n    \n      unique\n      22\n      2\n      2\n      5\n      3\n      2\n      6\n      7\n      8\n      3\n    \n    \n      top\n      toyota\n      std\n      four\n      sedan\n      fwd\n      front\n      ohc\n      four\n      mpfi\n      Low\n    \n    \n      freq\n      32\n      165\n      115\n      94\n      118\n      198\n      145\n      157\n      92\n      115\n    \n  \n\n\n\n\n\n\n4.3.2 The value_counts function\nValue counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the “value_counts” method on the column “drive-wheels”. Don’t forget the method “value_counts” only works on pandas series, not pandas dataframes. As a result, we only include one bracket df[‘drive-wheels’], not two brackets df[[‘drive-wheels’]].\n\ndf['drive-wheels'].value_counts()\n\nfwd    118\nrwd     75\n4wd      8\nName: drive-wheels, dtype: int64\n\n\nWe can convert the series to a dataframe as follows:\n\ndf['drive-wheels'].value_counts().to_frame()\n\n\n\n\n\n  \n    \n      \n      drive-wheels\n    \n  \n  \n    \n      fwd\n      118\n    \n    \n      rwd\n      75\n    \n    \n      4wd\n      8\n    \n  \n\n\n\n\nLet’s repeat the above steps but save the results to the dataframe “drive_wheels_counts” and rename the column ‘drive-wheels’ to ‘value_counts’.\n\ndrive_wheels_counts = df['drive-wheels'].value_counts().to_frame()\ndrive_wheels_counts.rename(columns={'drive-wheels': 'value_counts'}, inplace=True)\ndrive_wheels_counts\n\n\n\n\n\n  \n    \n      \n      value_counts\n    \n  \n  \n    \n      fwd\n      118\n    \n    \n      rwd\n      75\n    \n    \n      4wd\n      8\n    \n  \n\n\n\n\nNow let’s rename the index to ‘drive-wheels’:\n\ndrive_wheels_counts.index.name = \"drive-wheels\"\ndrive_wheels_counts\n\n\n\n\n\n  \n    \n      \n      value_counts\n    \n    \n      drive-wheels\n      \n    \n  \n  \n    \n      fwd\n      118\n    \n    \n      rwd\n      75\n    \n    \n      4wd\n      8\n    \n  \n\n\n\n\nWe can repeat the above process for the variable ‘engine-location’.\n\nengine_loc_counts = df['engine-location'].value_counts().to_frame()\nengine_loc_counts.rename(columns={'engine-location': 'value_counts'}, inplace=True)\nengine_loc_counts.index.name = 'engine-location'\nengine_loc_counts.head(10)\n\n\n\n\n\n  \n    \n      \n      value_counts\n    \n    \n      engine-location\n      \n    \n  \n  \n    \n      front\n      198\n    \n    \n      rear\n      3\n    \n  \n\n\n\n\nAfter examining the value counts of the engine location, we see that engine location would not be a good predictor variable for the price. This is because we only have three cars with a rear engine and 198 with an engine in the front, so this result is skewed. Thus, we are not able to draw any conclusions about the engine location.\n\n\n4.3.3 Boxplots\n\nBoxplot allow to plot the distribution of numerical data and make it easy to compare between groups. The boxplot shows:\n\nThe median: represents where the middle data point is.\nThe upper quartile shows where the 75th percentile is.\nThe lower quartile shows where the 25th percentile is.\nThe data between the upper and lower quartile represents the interquartile range.\nThe lower and upper extremes are calculated as 1.5 times the interquartile range, above the 75th percentile, and as 1.5 times the IQR below the 25th percentile.\nOutliers are individual dots that occur outside the upper and lower extremes.\n\n\nLet’s look at the relationship between “body-style” and “price”.\n\nsns.boxplot(x = \"body-style\", y = \"price\", data = df)\nplt.show()\nplt.close()\n\n\n\n\nWe see that the distributions of price between the different body-style categories have a significant overlap, so body-style would not be a good predictor of price. Let’s examine engine “engine-location” and “price”:\n\nsns.boxplot(x = \"engine-location\", y = \"price\", data = df)\nplt.show()\nplt.close()\n\n\n\n\nHere we see that the distribution of price between these two engine-location categories, front and rear, are distinct enough to take engine-location as a potential good predictor of price.\nLet’s examine “drive-wheels” and “price”.\n\nsns.boxplot(x = \"drive-wheels\", y = \"price\", data = df)\nplt.show()\nplt.close()\n\n\n\n\nHere we see that the distribution of price between the different drive-wheels categories differs. As such, drive-wheels could potentially be a predictor of price.\n\n\n4.3.4 Scatterplots\n\nScatter plots allow the representation of observations as data points and allows to show the relationships between two variables. Could engine size possibly predict the price of a car? Scatterplots show the relationship between two variables:\n\nPredictor/independent variable on the x-axis, i.e. the variable that we are using to predict an outcome, i.e. the engine size\nTarget/dependent variable on the y-axis, i.e. the price\n\n\n\nsns.regplot(x = \"engine-size\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n\n\n\n\nAs the engine-size goes up, the price goes up: this indicates a positive direct correlation between these two variables. Engine size seems like a pretty good predictor of price since the regression line is almost a perfect diagonal line.\nWe can examine the correlation between ‘engine-size’ and ‘price’ and see that it’s approximately 0.87.\n\ndf[[\"engine-size\", \"price\"]].corr()\n\n\n\n\n\n  \n    \n      \n      engine-size\n      price\n    \n  \n  \n    \n      engine-size\n      1.000000\n      0.872335\n    \n    \n      price\n      0.872335\n      1.000000\n    \n  \n\n\n\n\nHighway mpg is a potential predictor variable of price. Let’s find the scatterplot of “highway-mpg” and “price”.\n\nsns.regplot(x = \"highway-mpg\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n\n\n\n\nAs highway-mpg goes up, the price goes down: this indicates an inverse/negative relationship between these two variables. Highway mpg could potentially be a predictor of price.\nWe can examine the correlation between ‘highway-mpg’ and ‘price’ and see it’s approximately -0.704.\n\ndf[[\"highway-mpg\", \"price\"]].corr()\n\n\n\n\n\n  \n    \n      \n      highway-mpg\n      price\n    \n  \n  \n    \n      highway-mpg\n      1.000000\n      -0.704692\n    \n    \n      price\n      -0.704692\n      1.000000\n    \n  \n\n\n\n\nLet’s see if “peak-rpm” is a predictor variable of “price”.\n\nsns.regplot(x = \"peak-rpm\", y = \"price\", data = df)\nplt.ylim\nplt.show()\nplt.close()\n\n\n\n\n\ndf[[\"peak-rpm\", \"price\"]].corr()\n\n\n\n\n\n  \n    \n      \n      peak-rpm\n      price\n    \n  \n  \n    \n      peak-rpm\n      1.000000\n      -0.101616\n    \n    \n      price\n      -0.101616\n      1.000000\n    \n  \n\n\n\n\nPeak rpm does not seem like a good predictor of the price at all since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore, it’s not a reliable variable."
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#groupby-in-python",
    "href": "code/3_exploratory_data_analysis.html#groupby-in-python",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.4 GroupBy in python",
    "text": "4.4 GroupBy in python\nAssume we want to know, is there any relationship between the different types of drive system, forward, rear, and four-wheel drive, and the price of the vehicles?\nIf so, which type of drive system adds the most value to a vehicle? It would be nice if we could group all the data by the different types of drive wheels and compare the results of these different drive wheels against each other.\nTo do this we can use pandas groupby() method, which:\n\ngroups data by different categories\nCan be applied on categorical values\nAllows to group data into categories\nAccepts single multiple variables\n\nFor example, let’s group by the variable “drive-wheels”. We see that there are 3 different categories of drive wheels.\n\ndf['drive-wheels'].unique()\n\narray(['rwd', 'fwd', '4wd'], dtype=object)\n\n\nIf we want to know, on average, which type of drive wheel is most valuable, we can group “drive-wheels” and then average them.\nWe can select the columns ‘drive-wheels’, ‘body-style’ and ‘price’, then assign it to the variable “df_group_one”.\n\ndf_group_one = df[['drive-wheels', 'body-style', 'price']]\n\nWe can then calculate the average price for each of the different categories of data.\n\ndf_group_one = df_group_one.groupby(['drive-wheels'], as_index = False).mean()\ndf_group_one\n\n\n\n\n\n  \n    \n      \n      drive-wheels\n      price\n    \n  \n  \n    \n      0\n      4wd\n      10241.000000\n    \n    \n      1\n      fwd\n      9244.779661\n    \n    \n      2\n      rwd\n      19757.613333\n    \n  \n\n\n\n\nFrom our data, it seems rear-wheel drive vehicles are, on average, the most expensive, while 4-wheel and front-wheel are approximately the same in price.\nYou can also group by multiple variables. For example, let’s group by both ‘drive-wheels’ and ‘body-style’. This groups the dataframe by the unique combination of ‘drive-wheels’ and ‘body-style’. We can store the results in the variable ‘grouped_test1’.\n\ndf_gptest = df[['drive-wheels','body-style','price']]\ngrouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()\ngrouped_test1\n\n\n\n\n\n  \n    \n      \n      drive-wheels\n      body-style\n      price\n    \n  \n  \n    \n      0\n      4wd\n      hatchback\n      7603.000000\n    \n    \n      1\n      4wd\n      sedan\n      12647.333333\n    \n    \n      2\n      4wd\n      wagon\n      9095.750000\n    \n    \n      3\n      fwd\n      convertible\n      11595.000000\n    \n    \n      4\n      fwd\n      hardtop\n      8249.000000\n    \n    \n      5\n      fwd\n      hatchback\n      8396.387755\n    \n    \n      6\n      fwd\n      sedan\n      9811.800000\n    \n    \n      7\n      fwd\n      wagon\n      9997.333333\n    \n    \n      8\n      rwd\n      convertible\n      23949.600000\n    \n    \n      9\n      rwd\n      hardtop\n      24202.714286\n    \n    \n      10\n      rwd\n      hatchback\n      14337.777778\n    \n    \n      11\n      rwd\n      sedan\n      21711.833333\n    \n    \n      12\n      rwd\n      wagon\n      16994.222222\n    \n  \n\n\n\n\n\n4.4.1 Pivot() in python\nTo make the output of the groupby method easier to read, we can transform this table to a pivot table by using the pivot method. A pivot table has one variable displayed along the columns and the other variable displayed along the rows.\nIn our case, we will leave the drive-wheels variable as the rows of the table, and pivot body-style to become the columns of the table:\n\ngrouped_pivot = grouped_test1.pivot(index = \"drive-wheels\", columns = \"body-style\")\ngrouped_pivot\n\n\n\n\n\n  \n    \n      \n      price\n    \n    \n      body-style\n      convertible\n      hardtop\n      hatchback\n      sedan\n      wagon\n    \n    \n      drive-wheels\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      4wd\n      NaN\n      NaN\n      7603.000000\n      12647.333333\n      9095.750000\n    \n    \n      fwd\n      11595.0\n      8249.000000\n      8396.387755\n      9811.800000\n      9997.333333\n    \n    \n      rwd\n      23949.6\n      24202.714286\n      14337.777778\n      21711.833333\n      16994.222222\n    \n  \n\n\n\n\nOften, we won’t have data for some of the pivot cells. We can fill these missing cells with the value 0, but any other value could potentially be used as well. It should be mentioned that missing data is quite a complex subject and is an entire course on its own.\n\ngrouped_pivot = grouped_pivot.fillna(0)\ngrouped_pivot\n\n\n\n\n\n  \n    \n      \n      price\n    \n    \n      body-style\n      convertible\n      hardtop\n      hatchback\n      sedan\n      wagon\n    \n    \n      drive-wheels\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      4wd\n      0.0\n      0.000000\n      7603.000000\n      12647.333333\n      9095.750000\n    \n    \n      fwd\n      11595.0\n      8249.000000\n      8396.387755\n      9811.800000\n      9997.333333\n    \n    \n      rwd\n      23949.6\n      24202.714286\n      14337.777778\n      21711.833333\n      16994.222222\n    \n  \n\n\n\n\n\n\n4.4.2 Heatmap plots\nHeatmap plots:\n\nAnother way to visualize tables and plot the target variable over multiple variables\nTakes a rectangular grid of data and assigns a color intensity based on the data value at the grid points\n\n\n\n4.4.3 Correlation heatmap\nTaking all variables into account, we can now create a heatmap that indicates the correlation between each of the variables with one another. The color scheme indicates the Pearson correlation coefficient, indicating the strength of the correlation between two variables.\nLet’s use a heat map to visualize the relationship between Body Style vs Price.\n\nplt.pcolor(grouped_pivot, cmap = 'RdBu')\nplt.colorbar()\nplt.show()\nplt.close()\n\n\n\n\nThe heatmap plots the target variable (price) proportional to colour with respect to the variables ‘drive-wheel’ and ‘body-style’ on the vertical and horizontal axis, respectively. This allows us to visualize how the price is related to ‘drive-wheel’ and ‘body-style’.\nThe default labels convey no useful information to us. Let’s change that:\n\nfig, ax = plt.subplots()\nim = ax.pcolor(grouped_pivot, cmap='RdBu')\n\n#label names\nrow_labels = grouped_pivot.columns.levels[1]\ncol_labels = grouped_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=90)\n\nfig.colorbar(im)\n\nplt.show()\nplt.close()\n\n\n\n\nVisualization is very important in data science, and Python visualization packages provide great freedom. We will go more in-depth in a separate Python visualizations course.\nThe main question we want to answer in this module is, “What are the main characteristics which have the most impact on the car price?”.\nTo get a better measure of the important characteristics, we look at the correlation of these variables with the car price. In other words: how is the car price dependent on this variable?"
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#correlation",
    "href": "code/3_exploratory_data_analysis.html#correlation",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.5 Correlation",
    "text": "4.5 Correlation\nCorrelation is a statistical metric for measuring to what extent different variables are interdependent.\nIn other words, when we look at two variables over time, if one variable changes how does this affect change in the other variable?\nFor example, there is a correlation between umbrella and rain variables where more precipitation means more people use umbrellas. Also, if it doesn’t rain people would not carry umbrellas. Therefore, we can say that umbrellas and rain are interdependent and by definition they are correlated.\nIt is important to know that correlation doesn’t imply causation.\nCorrelation: a measure of the extent of interdependence between variables. Causation: the relationship between cause and effect between two variables.\nIn fact, we can say that umbrella and rain are correlated but we would not have enough information to say whether the umbrella caused the rain or the rain caused the umbrella.\n\n4.5.0.1 Pearson correlation\nMeasures the strength of the correlation between a continuous numerical variable.\nThis method will give us two values:\n\nCorrelation coefficient, a value:\n\nClose to 1: Large positive relationship\nClose to -1 : Large negative relationship\nClose to 0: No relationship\n\n\nPearson Correlation is the default method of the function “corr”. Like before, we can calculate the Pearson Correlation of the of the ‘int64’ or ‘float64’ variables.\n\ndf.corr().head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      wheel-base\n      length\n      width\n      height\n      curb-weight\n      engine-size\n      bore\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      diesel\n      gas\n    \n  \n  \n    \n      symboling\n      1.000000\n      0.466264\n      -0.535987\n      -0.365404\n      -0.242423\n      -0.550160\n      -0.233118\n      -0.110581\n      -0.140019\n      -0.008245\n      -0.182196\n      0.075819\n      0.279740\n      -0.035527\n      0.036233\n      -0.082391\n      0.066171\n      -0.196735\n      0.196735\n    \n    \n      normalized-losses\n      0.466264\n      1.000000\n      -0.056661\n      0.019424\n      0.086802\n      -0.373737\n      0.099404\n      0.112360\n      -0.029862\n      0.055563\n      -0.114713\n      0.217299\n      0.239543\n      -0.225016\n      -0.181877\n      0.133999\n      0.238567\n      -0.101546\n      0.101546\n    \n    \n      wheel-base\n      -0.535987\n      -0.056661\n      1.000000\n      0.876024\n      0.814507\n      0.590742\n      0.782097\n      0.572027\n      0.493244\n      0.158502\n      0.250313\n      0.371147\n      -0.360305\n      -0.470606\n      -0.543304\n      0.584642\n      0.476153\n      0.307237\n      -0.307237\n    \n    \n      length\n      -0.365404\n      0.019424\n      0.876024\n      1.000000\n      0.857170\n      0.492063\n      0.880665\n      0.685025\n      0.608971\n      0.124139\n      0.159733\n      0.579821\n      -0.285970\n      -0.665192\n      -0.698142\n      0.690628\n      0.657373\n      0.211187\n      -0.211187\n    \n    \n      width\n      -0.242423\n      0.086802\n      0.814507\n      0.857170\n      1.000000\n      0.306002\n      0.866201\n      0.729436\n      0.544885\n      0.188829\n      0.189867\n      0.615077\n      -0.245800\n      -0.633531\n      -0.680635\n      0.751265\n      0.673363\n      0.244356\n      -0.244356\n    \n  \n\n\n\n\nWhat is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\n\nP-value of:\n\n< 0.001 : strong certainty of the result\n< 0.05 : moderate certainty of the result\n< 0.1 : weak certainty of the result\n\n0.1 : no certainty of the result\n\n\n\nLet’s calculate the Pearson Correlation Coefficient and P-value of ‘wheel-base’ and ‘price’.\n\npearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n\nThe Pearson Correlation Coefficient is 0.584641822265508  with a P-value of P = 8.076488270733218e-20\n\n\nSince the p-value is < 0.001, the correlation between wheel-base and price is statistically significant, although the linear relationship isn’t extremely strong (~0.585).\nLet’s calculate the Pearson Correlation Coefficient and P-value of ‘horsepower’ and ‘price’.\n\npearson_coef, p_value = stats.pearsonr(df['horsepower'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n\nThe Pearson Correlation Coefficient is 0.809574567003656  with a P-value of P = 6.369057428259557e-48\n\n\nSince the p-value is < 0.001, the correlation between horsepower and price is statistically significant, and the linear relationship is quite strong (~0.809, close to 1).\nLet’s calculate the Pearson Correlation Coefficient and P-value of ‘length’ and ‘price’.\n\npearson_coef, p_value = stats.pearsonr(df['length'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n\nThe Pearson Correlation Coefficient is 0.690628380448364  with a P-value of P = 8.016477466158986e-30\n\n\nSince the p-value is < 0.001, the correlation between length and price is statistically significant, and the linear relationship is moderately strong (~0.691).\nLet’s calculate the Pearson Correlation Coefficient and P-value of ‘width’ and ‘price’:\n\npearson_coef, p_value = stats.pearsonr(df['width'], df['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  \n\nThe Pearson Correlation Coefficient is 0.7512653440522674  with a P-value of P = 9.200335510481516e-38\n\n\n\n\n4.5.0.2 Anova\nANOVA: Analysis of Variance\nThe Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:\nF-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\nP-value: P-value tells how statistically significant our calculated score value is.\nIf our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to return a sizeable F-test score and a small p-value.\nSince ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.\nTo see if different types of ‘drive-wheels’ impact ‘price’, we group the data.\n\ngrouped_test2=df_gptest[['drive-wheels', 'price']].groupby(['drive-wheels'])\ngrouped_test2.head(2)\n\n\n\n\n\n  \n    \n      \n      drive-wheels\n      price\n    \n  \n  \n    \n      0\n      rwd\n      13495.0\n    \n    \n      1\n      rwd\n      16500.0\n    \n    \n      3\n      fwd\n      13950.0\n    \n    \n      4\n      4wd\n      17450.0\n    \n    \n      5\n      fwd\n      15250.0\n    \n    \n      136\n      4wd\n      7603.0\n    \n  \n\n\n\n\n\ndf_gptest.head(2)\n\n\n\n\n\n  \n    \n      \n      drive-wheels\n      body-style\n      price\n    \n  \n  \n    \n      0\n      rwd\n      convertible\n      13495.0\n    \n    \n      1\n      rwd\n      convertible\n      16500.0\n    \n  \n\n\n\n\nWe can obtain the values of the method group using the method “get_group”.\n\ngrouped_test2.get_group('4wd')['price']\n\n4      17450.0\n136     7603.0\n140     9233.0\n141    11259.0\n144     8013.0\n145    11694.0\n150     7898.0\n151     8778.0\nName: price, dtype: float64\n\n\nWe can use the function ‘f_oneway’ in the module ‘stats’ to obtain the F-test score and P-value.\n\n# ANOVA\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  \n \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   \n\nANOVA results: F= 67.95406500780399 , P = 3.3945443577151245e-23\n\n\nThis is a great result with a large F-test score showing a strong correlation and a P-value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated?\nLet’s examine them separately.\nfwd and rwd\n\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'])  \n \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val )\n\nANOVA results: F= 130.5533160959111 , P = 2.2355306355677845e-23\n\n\n4wd and rwd\n\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('rwd')['price'])  \n   \nprint( \"ANOVA results: F=\", f_val, \", P =\", p_val)   \n\nANOVA results: F= 8.580681368924756 , P = 0.004411492211225333\n\n\n4wd and fwd\n\nf_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('fwd')['price'])  \n \nprint(\"ANOVA results: F=\", f_val, \", P =\", p_val)   \n\nANOVA results: F= 0.665465750252303 , P = 0.41620116697845666\n\n\nWe now have a better idea of what our data looks like and which variables are important to take into account when predicting the car price. We have narrowed it down to the following variables:\nContinuous numerical variables:\nLength Width Curb-weight Engine-size Horsepower City-mpg Highway-mpg Wheel-base Bore Categorical variables:\nDrive-wheels As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model’s prediction performance."
  },
  {
    "objectID": "code/3_exploratory_data_analysis.html#association-between-two-categorical-variables-chi-square",
    "href": "code/3_exploratory_data_analysis.html#association-between-two-categorical-variables-chi-square",
    "title": "4  Exploratory data analysis (EDA) in python",
    "section": "4.6 Association between two categorical variables: Chi-Square",
    "text": "4.6 Association between two categorical variables: Chi-Square\n\nChi-square test for association allows to find out if there is a relationship between two categorical variables.\nIntended to test how likely it is that an observed distribution is due to chance.\nMeasures how well the observed distribution of data fits with the distribution that is expected if the variables are independent = i.e. it tests the null hypothesis that the variables are independent.\nCompares the observed data to the values that the model expects if the data was distributed in different categories by chance. Anytime the observed data doesn’t fit within the model of the expected values, the probability that the variables are dependent becomes stronger, thus proving the null hypothesis incorrect.\nThe chi-square doesn’t tell us what type of relationship exists between the two variables, only that a relationship exists.\nP-value < 0.05: We reject the null hypothesis that the two variables are independent and can conclude that there is evidence for an association between variable A and variable B.\n\n(Row total * Column total) / grand total\ndegree of freedom = (row-1)*(column-1)\nscipy.stats.chi2_contingency(cont_table, correction = True)"
  },
  {
    "objectID": "code/4_model_dev.html",
    "href": "code/4_model_dev.html",
    "title": "5  Model Development in python",
    "section": "",
    "text": "Model : A mathematical equation used to predict a value given one or more values\nRelating one or more independent variables to dependent variables. For example we can input a cars highway-mpg as independent variable (ore feature). The output of the model or the dependent variable is the predicted price.\nThe more relevant data we have, the more accurate a model usually is. For example, we can input multiple independent variables (highway-mpg, engine-size, etc) and therefore our model might predict a more accurate price for a car.\n\nSome questions we want to ask in this module\nDo I know if the dealer is offering fair value for my trade-in? Do I know if I put a fair value on my car?\n\n#setup general libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#setup lib for linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#model vis\nimport seaborn as sns\n\n\n#read in example data\ndf = pd.read_csv(\"../data/automobileEDA.csv.1\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      symboling\n      normalized-losses\n      make\n      aspiration\n      num-of-doors\n      body-style\n      drive-wheels\n      engine-location\n      wheel-base\n      length\n      ...\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      horsepower-binned\n      diesel\n      gas\n    \n  \n  \n    \n      0\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      Medium\n      0\n      1\n    \n    \n      1\n      3\n      122\n      alfa-romero\n      std\n      two\n      convertible\n      rwd\n      front\n      88.6\n      0.811148\n      ...\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      Medium\n      0\n      1\n    \n    \n      2\n      1\n      122\n      alfa-romero\n      std\n      two\n      hatchback\n      rwd\n      front\n      94.5\n      0.822681\n      ...\n      9.0\n      154.0\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      Medium\n      0\n      1\n    \n    \n      3\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      fwd\n      front\n      99.8\n      0.848630\n      ...\n      10.0\n      102.0\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      Medium\n      0\n      1\n    \n    \n      4\n      2\n      164\n      audi\n      std\n      four\n      sedan\n      4wd\n      front\n      99.4\n      0.848630\n      ...\n      8.0\n      115.0\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      Medium\n      0\n      1\n    \n  \n\n5 rows × 29 columns"
  },
  {
    "objectID": "code/4_model_dev.html#linear-regression-and-multiple-linear-regression",
    "href": "code/4_model_dev.html#linear-regression-and-multiple-linear-regression",
    "title": "5  Model Development in python",
    "section": "5.2 Linear Regression and Multiple Linear Regression",
    "text": "5.2 Linear Regression and Multiple Linear Regression\nLinear regression will refer to one independent variable to make a prediction. Multiple linear regression will refer to multiple independent variables to make a prediction.\n\n5.2.1 Simple linear regression (SLR)\n\nA method to help us understand the relationship between two variables:\n\nThe predictor (independent) variable x and\nthe target (dependent) variable y\n\n\nThe result of Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable.\n\\(\\begin{aligned}  y = b_0 + b_1x\\\\ \\end{aligned}\\)\nbo: the intercept of the regression line, in other words: the value of Y when X is 0 b1: the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit\nIf we assume there is a linear relationship between the highway-mpg and the price, we can use this relationship to formulate a model to determine the price of the car. I.e. for a given car for which we know the price, we can check the highway-mpg in the manual. If the highway miles per gallon is 20, we can input this value into the model to obtain a prediction of $22,000.\nIn order to determine the line, we take data points from our data set. We then use these training points to fit our model. The results of the training points are the parameters, bo and b1.\nIn many cases, many factors influence how much people pay for a car. For example, make or how old the car is. In this model, this uncertainty is taken into account by assuming a small random value is added to the point on the line. This is called noise.\nIn short:\n\nWe have a set of training points.\nWe use these training points to fit or train the model and get parameters.\nWe then use these parameters in the model.\nWe now have a model.\nWe use the hat on the y to denote the model is an estimate.\nWe can use this model to predict values that we haven’t seen.\n\n\n#create a linear regression object\nlm = LinearRegression()\nlm\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\nQuestion: How could “highway-mpg” help us predict car price?\nFor this example, we want to look at how highway-mpg can help us predict car price. Using simple linear regression, we will create a linear function with “highway-mpg” as the predictor variable and the “price” as the response variable.\n\n#set the predictor variable (X), highway-mpg , and response variable (Y), price\nX = df[['highway-mpg']]\nY = df[['price']]\n\n#fit the linear model using highway-mpg\nlm.fit(X,Y)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\n#output a prediction\nYhat = lm.predict(X)\nYhat[0:5]\n\narray([[16236.50464347],\n       [16236.50464347],\n       [17058.23802179],\n       [13771.3045085 ],\n       [20345.17153508]])\n\n\n\n#What is the value of the intercept (a)?\nlm.intercept_\n\narray([38423.30585816])\n\n\n\n#What is the value of the slope?\nlm.coef_\n\narray([[-821.73337832]])\n\n\nWith these values we get a final linear model with the structure:\nPrice = 38423.31 -821.73 * highway-mpg\n\n\n5.2.2 Multiple linear regression (MLR)\nWhat if we want to predict car price using more than one variable?\nIf we want to use more variables in our model to predict car price, we can use Multiple Linear Regression. Multiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between:\n\nOne continuous target (Y) variable\nTwo or more predictor (X) variables\n\nIf we would have four predictor variables, we would have something like this\n\\(\\begin{aligned}  y = b_0 + b_1x_1+b_2x_2+b_3x_3+b_4x_4\\\\ \\end{aligned}\\)\nbo: the intercept (x=0)  b1: the coefficient or parameter of x1  b2: the coefficient or parameter of x2 and so on\nFrom the previous section we know that other good predictors of price could be:\n\nHorsepower\nCurb-weight\nEngine-size\nHighway-mpg\n\n\n#use the variables above as predictor variables\nZ = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n\n#fit the linear model\nlm = LinearRegression()\nlm.fit(Z, df[['price']])\n\n#get the intercept\nprint(lm.intercept_)\n\n#get the coefficients\nprint(lm.coef_)\n\n[-15806.62462633]\n[[53.49574423  4.70770099 81.53026382 36.05748882]]\n\n\nThe final model we get with these values is:\nPrice = -15806.62 + 53.50 * horsepower + 4.71 * curb-weight + 81.52 * engine-size + 36.01 * highway-mpg"
  },
  {
    "objectID": "code/4_model_dev.html#model-evaluation-using-visualization",
    "href": "code/4_model_dev.html#model-evaluation-using-visualization",
    "title": "5  Model Development in python",
    "section": "5.3 Model Evaluation using Visualization",
    "text": "5.3 Model Evaluation using Visualization\nNow that we’ve developed some models, how do we evaluate our models and choose the best one? One way to do this is by using a visualization.\n\n5.3.1 Regression plots\nWhen it comes to simple linear regression, an excellent way to visualize the fit of our model is by using regression plots.\nThis plot will show a combination of a scattered data points (a scatterplot), as well as the fitted linear regression line going through the data. This will give us a reasonable estimate of the relationship between the two variables, the strength of the correlation, as well as the direction (positive or negative correlation).\nGive good estimates of:\n\nRelationships between 2 variables\nThe strength of the correlation\nThe direction of the relatioship (positive or negative)\n\nA regression plot shows us:\n\nThe horizontal axis is the independent variable\nThe vertical axis is the dependent variable\nEach point represents a different target point\nThe fitted line represents the predicted value\n\nLet’s visualize highway-mpg as potential predictor variable of price:\n\nwidth = 6\nheight = 5\n\nplt.figure(figsize=(width, height))\nsns.regplot(x = \"highway-mpg\", y = \"price\", data = df)\n#plt.ylim((0,))\n\nplt.show()\nplt.close()\n\n\n\n\nWe can see from this plot that price is negatively correlated to highway-mpg since the regression slope is negative.\nOne thing to keep in mind when looking at a regression plot is to pay attention to how scattered the data points are around the regression line. This will give you a good indication of the variance of the data and whether a linear model would be the best fit or not. If the data is too far off from the line, this linear model might not be the best model for this data.\nLet’s compare this plot to the regression plot of “peak-rpm”.\n\nplt.figure(figsize=(width, height))\nsns.regplot(x=\"peak-rpm\", y=\"price\", data=df)\n#plt.ylim((0,))\n\nplt.show()\nplt.close()\n\n\n\n\nComparing the regression plot of “peak-rpm” and “highway-mpg”, we see that the points for “highway-mpg” are much closer to the generated line and, on average, decrease. The points for “peak-rpm” have more spread around the predicted line and it is much harder to determine if the points are decreasing or increasing as the “peak-rpm” increases.\nGiven the regression plots above, is “peak-rpm” or “highway-mpg” more strongly correlated with “price”?\n\ndf[['price', 'highway-mpg', 'peak-rpm']].corr()\n\n\n\n\n\n  \n    \n      \n      price\n      highway-mpg\n      peak-rpm\n    \n  \n  \n    \n      price\n      1.000000\n      -0.704692\n      -0.101616\n    \n    \n      highway-mpg\n      -0.704692\n      1.000000\n      -0.058598\n    \n    \n      peak-rpm\n      -0.101616\n      -0.058598\n      1.000000\n    \n  \n\n\n\n\nWe see that the variable “highway-mpg” has a stronger correlation with “price”, it is approximate -0.704692 compared to “peak-rpm” which is approximate -0.101616. You can verify it using the following command:\n\n\n5.3.2 Residual plot\nA good way to visualize the variance of the data is to use a residual plot.\nResidual : The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.\nResidual plot: A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.\nWhat to look for: We look at the spread of the residuals –> If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.\n\n5.3.2.1 Single linear regression\n\nwidth = 6\nheight = 5\n\nplt.figure(figsize=(width, height))\nsns.residplot(x = df['highway-mpg'], y = df['price'])\n\nplt.show()\nplt.close()\n\n\n\n\nWe can see from this residual plot that the residuals are not randomly spread around the x-axis, leading us to believe that maybe a non-linear model is more appropriate for this data.\n\n\n\n5.3.3 Distribution plots\nHow do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because you can’t visualize it with regression or residual plot.\nOne way to look at the fit of the model is by looking at the distribution plot. We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.\n\n#make a prediction using 4 variables\nY_hat = lm.predict(Z)\n\n#prepare the plot\nplt.figure(figsize=(width, height))\n\nax1 = sns.distplot(df['price'], hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(Y_hat, hist = False, color = \"b\", label = \"Fitted Values\", ax = ax1)\n\nplt.title(\"Actual vs fitted values for price\")\nplt.xlabel(\"Price (in dollars\")\nplt.ylabel(\"Proportion of cars\")\n\nplt.show()\nplt.close()\n\n\n\n\nWe can see that the fitted values are reasonably close to the actual values since the two distributions overlap a bit. However, there is definitely some room for improvement."
  },
  {
    "objectID": "code/4_model_dev.html#polynomial-regression-and-pipelines",
    "href": "code/4_model_dev.html#polynomial-regression-and-pipelines",
    "title": "5  Model Development in python",
    "section": "5.4 Polynomial Regression and Pipelines",
    "text": "5.4 Polynomial Regression and Pipelines\nWhat do we do when a linear model is not the best fit for our data?\n\n5.4.1 Polynomial Regression\nPolynomial regression is a special case of the general linear regression or multiple linear regression models. We get non-linear relationships by squaring or setting higher-order terms of the predictor variables. This method is beneficial for describing curvilinear relationships.\nCurvilinear relationship: what you get by squaring or setting higher order terms of the predictor variables in the model transforming the data. The model can be quadratic, which means that the predictor variable in the model is squared.\n\nQuadratic - 2nd Order\n\n\\[\nYhat = a + b_1 X +b_2 X^2\n\\]\n\nCubic - 3rd Order\n\n\\[\nYhat = a + b_1 X +b_2 X^2 +b_3 X^3\\\\\\\\\\\\\\\\\\\\\n\\]\n\nHigher-Order:\n\n\\[\nY = a + b_1 X +b_2 X^2 +b_3 X^3 ....\\\\\\\\\n\\]\nWe saw earlier that a linear model did not provide the best fit while using “highway-mpg” as the predictor variable. Let’s see if we can try fitting a polynomial model to the data instead.\nWe will use the following function to plot the data:\n\ndef PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('Price of Cars')\n\n    plt.show()\n    plt.close()\n\n\n#get the variables\nx = df['highway-mpg']\ny = df['price']\n\nLet’s fit the polynomial using the function polyfit, then use the function poly1d to display the polynomial function\n\n#use a polynomal of the 3rd order (cubic)\nf = np.polyfit(x, y, 3)\np = np.poly1d(f)\nprint(p)\n\n        3         2\n-1.557 x + 204.8 x - 8965 x + 1.379e+05\n\n\n\n#plot \nPlotPolly(p, x, y, 'highway-mpg')\n\n\n\n\n\nnp.polyfit(x, y, 3)\n\narray([-1.55663829e+00,  2.04754306e+02, -8.96543312e+03,  1.37923594e+05])\n\n\nWe can already see from plotting that this polynomial model performs better than the linear model. This is because the generated polynomial function “hits” more of the data points.\nThe analytical expression for Multivariate Polynomial function gets complicated. For example, the expression for a second-order (degree=2) polynomial with two variables is given by:\n\\[\nYhat = a + b\\_1 X\\_1 +b\\_2 X\\_2 +b\\_3 X\\_1 X\\_2+b\\_4 X\\_1^2+b\\_5 X\\_2^2\n\\]\nWe can perform a polynomial transform on multiple features.\n\n#import module\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n#We create a PolynomialFeatures object of degree 2\npr=PolynomialFeatures(degree=2)\n\n#transform data\nZ_pr=pr.fit_transform(Z)\n\n#view data before transformation\nprint(Z.shape)\n\n#view data after the transformation\nprint(Z_pr.shape)\n\n(201, 4)\n(201, 15)\n\n\n\n\n5.4.2 Pipelines\nWe can simplify our code by using a pipeline library.\nThere are many steps to getting a prediction. For example, normalization, polynomial transform, and linear regression.\nWe simplify the process using a pipeline, which sequentially performs a series of transformations and were the last step carries out a prediction.\nA pipeline could look as follows:\n\nFirst we import all the modules we need\nthen we import the library pipeline\nWe create a list of tuples, the first element in the tuple contains the name of the estimator model. The second element contains model constructor.\nWe input the list in the pipeline constructor. We now have a pipeline object.\nWe can train the pipeline by applying the train method to the pipeline object.\nWe can also produce a prediction as well.\nThe method normalizes the data, performs a polynomial transform, then outputs a prediction.\n\n\n#import libs\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nWe create the pipeline by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n\nInput=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]\n\nWe input the list as an argument to the pipeline constructor:\n\npipe=Pipeline(Input)\npipe\n\nPipeline(memory=None,\n         steps=[('scale',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('polynomial',\n                 PolynomialFeatures(degree=2, include_bias=False,\n                                    interaction_only=False, order='C')),\n                ('model',\n                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n                                  normalize=False))],\n         verbose=False)\n\n\nFirst, we convert the data type Z to type float to avoid conversion warnings that may appear as a result of StandardScaler taking float inputs.\nThen, we can normalize the data, perform a transform and fit the model simultaneously.\n\nZ = Z.astype(float)\npipe.fit(Z,y)\n\nPipeline(memory=None,\n         steps=[('scale',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('polynomial',\n                 PolynomialFeatures(degree=2, include_bias=False,\n                                    interaction_only=False, order='C')),\n                ('model',\n                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n                                  normalize=False))],\n         verbose=False)\n\n\nSimilarly, we can normalize the data, perform a transform and produce a prediction simultaneously.\n\nypipe=pipe.predict(Z)\nypipe[0:4]\n\narray([13102.74784201, 13102.74784201, 18225.54572197, 10390.29636555])"
  },
  {
    "objectID": "code/4_model_dev.html#measures-for-in-sample-evaluation",
    "href": "code/4_model_dev.html#measures-for-in-sample-evaluation",
    "title": "5  Model Development in python",
    "section": "5.5 Measures for In-Sample Evaluation",
    "text": "5.5 Measures for In-Sample Evaluation\nThese measures are a way to numerically determine how good the model fits on our data.\nTwo important measures that we often use to determine the fit of a model are:\n\nMean Square Error (MSE): To measure the MSE, we find the difference between the actual value y and the predicted value then then square it. In this case, the actual value is 150; the predicted value is 50. Subtracting these points we get 100. We then square the number. We then take the Mean or average of all the errors by adding then all together and dividing by the number of samples.\nR-squared: R-squared is also called the coefficient of determination. It’s a measure to determine how close the data is to the fitted regression line. So how close is our actual data to our estimated model?\n\n\n5.5.1 Example R-square for SLR\n\n#highway_mpg_fit\nlm.fit(X, Y)\n\n# Find the R^2\nprint('The R-square is: ', lm.score(X, Y))\n\nThe R-square is:  0.4965911884339176\n\n\nWe can say that ~49.659% of the variation of the price is explained by this simple linear model “horsepower_fit”.\nLet’s calculate the MSE:\nWe can predict the output i.e., “yhat” using the predict method, where X is the input variable:\n\nYhat=lm.predict(X)\nprint('The output of the first four predicted value is: ', Yhat[0:4])\n\nThe output of the first four predicted value is:  [[16236.50464347]\n [16236.50464347]\n [17058.23802179]\n [13771.3045085 ]]\n\n\nLet’s import the function mean_squared_error from the module metrics:\n\nfrom sklearn.metrics import mean_squared_error\n\n\nmse = mean_squared_error(df['price'], Yhat)\nprint('The mean square error of price and predicted value is: ', mse)\n\nThe mean square error of price and predicted value is:  31635042.944639888\n\n\n\n\n5.5.2 Example R-square for MLR\n\n# fit the model \nlm.fit(Z, df['price'])\n\n# Find the R^2\nprint('The R-square is: ', lm.score(Z, df['price']))\n\nThe R-square is:  0.8093562806577458\n\n\nWe can say that ~80.896 % of the variation of price is explained by this multiple linear regression “multi_fit”.\nLet’s calculate the MSE.\n\n# We produce a prediction: \nY_predict_multifit = lm.predict(Z)\n\n#We compare the predicted results with the actual results:\nprint('The mean square error of price and predicted value using multifit is: ', mean_squared_error(df['price'], Y_predict_multifit))\n\nThe mean square error of price and predicted value using multifit is:  11980366.870726489\n\n\n\n\n5.5.3 Example R-square for polynomial fit\n\n# import the function r2_score from the module metrics as we are using a different function\nfrom sklearn.metrics import r2_score\n\n\n# We apply the function to get the value of R^2:\nr_squared = r2_score(y, p(x))\nprint('The R-square value is: ', r_squared)\n\nThe R-square value is:  0.6741946663906513\n\n\nWe can say that ~67.419 % of the variation of price is explained by this polynomial fit.\nWe can also calculate the MSE:\n\nmean_squared_error(df['price'], p(x))\n\n20474146.42636125"
  },
  {
    "objectID": "code/4_model_dev.html#prediction-and-decision-making",
    "href": "code/4_model_dev.html#prediction-and-decision-making",
    "title": "5  Model Development in python",
    "section": "5.6 Prediction and Decision Making",
    "text": "5.6 Prediction and Decision Making\nHow can we make sure our model is correct?\nTo determine the best fit, we look at a combination of:\n\ndo our model results make sense\nvisualize the data\nuse numerical measures for evaluation\ncompare models\n\n\n5.6.1 Prediction\nIn the previous section, we trained the model using the method fit. Now we will use the method predict to produce a prediction. Lets import pyplot for plotting; we will also be using some functions from numpy.\n\n#create a new input \nnew_input=np.arange(1, 100, 1).reshape(-1, 1)\n\n#fit the model\nlm.fit(X,Y)\nlm\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\n#produce a prediction\nyhat=lm.predict(new_input)\nyhat[0:5]\n\narray([[37601.57247984],\n       [36779.83910151],\n       [35958.10572319],\n       [35136.37234487],\n       [34314.63896655]])\n\n\n\n#plot data\nplt.plot(new_input, yhat)\nplt.show()\nplt.close()\n\n\n\n\n\n\n5.6.2 Decision Making: Determining a Good Model Fit¶\nNow that we have visualized the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?\n\nWhat is a good R-squared value?\n\nWhen comparing models, the model with the higher R-squared value is a better fit for the data.\n\nWhat is a good MSE?\n\nWhen comparing models, the model with the smallest MSE value is a better fit for the data.\nLet’s take a look at the values for the different models.\nSimple Linear Regression: Using Highway-mpg as a Predictor Variable of Price.\n\nR-squared: 0.49659118843391759\nMSE: 3.16 x10^7\n\nMultiple Linear Regression: Using Horsepower, Curb-weight, Engine-size, and Highway-mpg as Predictor Variables of Price.\n\nR-squared: 0.80896354913783497\nMSE: 1.2 x10^7\n\nPolynomial Fit: Using Highway-mpg as a Predictor Variable of Price.\n\nR-squared: 0.6741946663906514\nMSE: 2.05 x 10^7\n\n\n5.6.2.1 Simple Linear Regression Model (SLR) vs Multiple Linear Regression Model (MLR)\nUsually, the more variables you have, the better your model is at predicting, but this is not always true. Sometimes you may not have enough data, you may run into numerical problems, or many of the variables may not be useful and even act as noise. As a result, you should always check the MSE and R^2.\nIn order to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.\n\nMSE: The MSE of SLR is 3.16x10^7 while MLR has an MSE of 1.2 x10^7. The MSE of MLR is much smaller. 0 R-squared: In this case, we can also see that there is a big difference between the R-squared of the SLR and the R-squared of the MLR. The R-squared for the SLR (~0.497) is very small compared to the R-squared for the MLR (~0.809).\n\nThis R-squared in combination with the MSE show that MLR seems like the better model fit in this case compared to SLR.\n\n\n5.6.2.2 Simple Linear Model (SLR) vs. Polynomial Fit\n\nMSE: We can see that Polynomial Fit brought down the MSE, since this MSE is smaller than the one from the SLR.\nR-squared: The R-squared for the Polynomial Fit is larger than the R-squared for the SLR, so the Polynomial Fit also brought up the R-squared quite a bit.\n\nSince the Polynomial Fit resulted in a lower MSE and a higher R-squared, we can conclude that this was a better fit model than the simple linear regression for predicting “price” with “highway-mpg” as a predictor variable.\n\n\n5.6.2.3 Multiple Linear Regression (MLR) vs. Polynomial Fit¶\n\nMSE: The MSE for the MLR is smaller than the MSE for the Polynomial Fit.\nR-squared: The R-squared for the MLR is also much larger than for the Polynomial Fit.\n\n\n\n5.6.2.4 Conclusion\nComparing these three models, we conclude that the MLR model is the best model to be able to predict price from our dataset. This result makes sense since we have 27 variables in total and we know that more than one of those variables are potential predictors of the final car price."
  },
  {
    "objectID": "code/5_model_eval_and_refinement.html",
    "href": "code/5_model_eval_and_refinement.html",
    "title": "6  Model Evaluation and Refinement",
    "section": "",
    "text": "Let’s first prepare our environment:\n\n#load libs\nimport pandas as pd\nimport numpy as np\n\n#libs for plotting\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for model training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n#load data\ndf = pd.read_csv(\"../data/module_5_auto.csv\")\n\n#only use numeric data\ndf=df._get_numeric_data()\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Unnamed: 0.1\n      symboling\n      normalized-losses\n      wheel-base\n      length\n      width\n      height\n      curb-weight\n      engine-size\n      ...\n      stroke\n      compression-ratio\n      horsepower\n      peak-rpm\n      city-mpg\n      highway-mpg\n      price\n      city-L/100km\n      diesel\n      gas\n    \n  \n  \n    \n      0\n      0\n      0\n      3\n      122\n      88.6\n      0.811148\n      0.890278\n      48.8\n      2548\n      130\n      ...\n      2.68\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      13495.0\n      11.190476\n      0\n      1\n    \n    \n      1\n      1\n      1\n      3\n      122\n      88.6\n      0.811148\n      0.890278\n      48.8\n      2548\n      130\n      ...\n      2.68\n      9.0\n      111.0\n      5000.0\n      21\n      27\n      16500.0\n      11.190476\n      0\n      1\n    \n    \n      2\n      2\n      2\n      1\n      122\n      94.5\n      0.822681\n      0.909722\n      52.4\n      2823\n      152\n      ...\n      3.47\n      9.0\n      154.0\n      5000.0\n      19\n      26\n      16500.0\n      12.368421\n      0\n      1\n    \n    \n      3\n      3\n      3\n      2\n      164\n      99.8\n      0.848630\n      0.919444\n      54.3\n      2337\n      109\n      ...\n      3.40\n      10.0\n      102.0\n      5500.0\n      24\n      30\n      13950.0\n      9.791667\n      0\n      1\n    \n    \n      4\n      4\n      4\n      2\n      164\n      99.4\n      0.848630\n      0.922222\n      54.3\n      2824\n      136\n      ...\n      3.40\n      8.0\n      115.0\n      5500.0\n      18\n      22\n      17450.0\n      13.055556\n      0\n      1\n    \n  \n\n5 rows × 21 columns\n\n\n\n\n#define functions for plotting\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 8\n    height = 6\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()\n\n\ndef PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n    width = 8\n    height = 6\n    plt.figure(figsize=(width, height))\n    \n    #training data \n    #testing data \n    # lr:  linear regression object \n    #poly_transform:  polynomial transformation object \n    xmax=max([xtrain.values.max(), xtest.values.max()])\n    xmin=min([xtrain.values.min(), xtest.values.min()])\n    x=np.arange(xmin, xmax, 0.1)\n\n    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n    plt.plot(xtest, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()\n\n\ndef f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)\n\nAn important step in testing your model is to split your data into training and testing data. We will place the target data price in a separate dataframe y_data and Drop price data in dataframe x_data:\n\n#prepare the df\ny_data = df['price']\nx_data=df.drop('price',axis=1)\n\nNow, we randomly split our data into training and testing data using the function train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\n#split data\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n\n#see if this worked\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n\nnumber of test samples : 21\nnumber of training samples: 180\n\n\nWe see that the test_size parameter sets the proportion of data that is split into the testing set. In the above, the testing set is 10% of the total dataset.\nNow, lets prepare the model”\n\n#load libs\nfrom sklearn.linear_model import LinearRegression\n\n#create regression object\nlre = LinearRegression()\n\n#fit the model using the feature `horsepower`\nlre.fit(x_train[['horsepower']], y_train)\n\n#calculate the R2 on the test data\nprint(lre.score(x_test[['horsepower']], y_test))\n\n#compare the R2 of the training data\nprint(lre.score(x_train[['horsepower']], y_train))\n\n0.36358755750788263\n0.6619724197515104\n\n\nWe can see the R^2 is much smaller using the test data compared to the training data."
  },
  {
    "objectID": "code/5_model_eval_and_refinement.html#model-evaluation",
    "href": "code/5_model_eval_and_refinement.html#model-evaluation",
    "title": "6  Model Evaluation and Refinement",
    "section": "6.2 Model evaluation",
    "text": "6.2 Model evaluation\nModel evaluation tells us how our model performs in the real world.\n\nIn-sample evaluation tells us how well our model fits the data already given to train it\nHowever, the problem is that it does not give us an estimate of how well the train model can predict new data\nThe solution is to split our data up, use the in-sample data or training data to train the model. The rest of the data, called Test Data, is used as out-of-sample data. This data is then used to approximate, how the model performs in the real world\nWhen we split a dataset, usually the larger portion of data is used for training and a smaller part is used for testing\nWhen we have completed testing our model, we should use all the data to train the model\n\n\n6.2.1 Generalization performance\n\nGeneralization error is a measure how well our data does at predicting previously unseen data\nThe error we obtain using our testing data is an approximation of this error\nUsing a lot of data for training,gives us an accurate means of determining how well our model will perform in the real world. But the precision of the performance will be low\nIf we use fewer data points to train the model and more to test the model, the accuracy of the generalization performance will be less, but the model will have good precision.\n\nTo overcome this problem, we use cross-validation.\n\n\n6.2.2 Cross validation\n\nOne of the most common out of sample evaluation metrics is cross-validation\nMore effective use of data, as each observation is used for both training and testing\nIn this method, the dataset is split into K equal groups. Each group is referred to as a fold.\nSome of the folds can be used as a training set which we use to train the model and the remaining parts are used as a test set, which we use to test the model. For example, we can use three folds for training, then use one fold for testing\nThis is repeated until each partition is used for both training and testing\nAt the end, we use the average results as the estimate of out-of-sample error\n\n\n#load lib\nfrom sklearn.model_selection import cross_val_score\n\nFor cross-validation we input the object, the feature (“horsepower”), and the target data (y_data). The parameter ‘cv’ determines the number of folds. In this case, it is 4.\n\n#cross validate\nRcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv = 4)\n\nThe default scoring is R^2. Each element in the array has the average R^2 value for the fold:\n\n#view data\nRcross\n\narray([0.7746232 , 0.51716687, 0.74785353, 0.04839605])\n\n\nWe can calculate the average and standard deviation of our estimate:\n\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n\nThe mean of the folds are 0.522009915042119 and the standard deviation is 0.291183944475603\n\n\nWe can use negative squared error as a score by setting the parameter ‘scoring’ metric to ‘neg_mean_squared_error’.\n\n-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n\narray([20254142.84026702, 43745493.2650517 , 12539630.34014931,\n       17561927.72247591])\n\n\nYou can also use the function ‘cross_val_predict’ to predict the output. The function splits up the data into the specified number of folds, with one fold for testing and the other folds are used for training. First, import the function:\nWe input the object, the feature “horsepower”, and the target data y_data. The parameter ‘cv’ determines the number of folds. In this case, it is 4. We can produce an output:\n\n#load lib\nfrom sklearn.model_selection import cross_val_predict\n\n#predict the output\nyhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\nyhat[0:5]\n\narray([14141.63807508, 14141.63807508, 20814.29423473, 12745.03562306,\n       14762.35027598])"
  },
  {
    "objectID": "code/5_model_eval_and_refinement.html#overfitting-underfitting-and-model-selection",
    "href": "code/5_model_eval_and_refinement.html#overfitting-underfitting-and-model-selection",
    "title": "6  Model Evaluation and Refinement",
    "section": "6.3 Overfitting, Underfitting and Model Selection",
    "text": "6.3 Overfitting, Underfitting and Model Selection\nIt turns out that the test data, sometimes referred to as the “out of sample data”, is a much better measure of how well your model performs in the real world. One reason for this is overfitting.\nLet’s go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.\nIn this section, we will discuss how to pick the best polynomial order and problems that arise when selecting the wrong order polynomial.\n\nUnderfitting: the model is too simple to fit the data\nOverfitting : the estimated function oscillates but doesn’t track the function\n\nFor Model selection we can compare the mean square error and the polynomial order:\n\nThe training error decreases with the order of the polynomial\nThe test error is a better means to estimate the error of the polynomial. The error decreases ’till the best order of the polynomial is determined. Then the error begins to increase. We select the order that minimizes the test error.Anything on the left would be considered underfitting. Anything on the right is overfitting\nIf we select the best order of the polynomial, we will still have some errors, i.e. due to random noise or our polynomial assumption may be wrong\n\nLet’s create Multiple Linear Regression objects and train the model using ‘horsepower’, ‘curb-weight’, ‘engine-size’ and ‘highway-mpg’ as features.\n\nlr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\nNext, we run the prediction using training data:\n\nyhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\n\narray([ 7426.6731551 , 28323.75090803, 14213.38819709,  4052.34146983,\n       34500.19124244])\n\n\nNext, we run the prediction using test data:\n\nyhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n\narray([11349.35089149,  5884.11059106, 11208.6928275 ,  6641.07786278,\n       15565.79920282])\n\n\nLet’s perform some model evaluation using our training and testing data separately. First, we import the seaborn and matplotlib library for plotting.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nLet’s examine the distribution of the predicted values of the training data.\n\nTitle = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)\n\n\n\n\nSo far, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.\n\nTitle='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)\n\n\n\n\nComparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different. Let’s see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.\n\n6.3.1 Overfitting\nOverfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let’s create a degree 5 polynomial model.\nLet’s use 55 percent of the data for training and the rest for testing:\n\n#load lib\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#use 55 percent of the data for training and the rest for testing\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\n\n#perform a degree 5 polynomial transformation on the feature 'horsepower'\npr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr\n\nPolynomialFeatures(degree=5, include_bias=True, interaction_only=False,\n                   order='C')\n\n\nNow, let’s create a Linear Regression model “poly” and train it.\n\npoly = LinearRegression()\npoly.fit(x_train_pr, y_train)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\nWe can see the output of our model using the method “predict.” We assign the values to “yhat”.\n\nyhat = poly.predict(x_test_pr)\nyhat[0:5]\n\narray([ 6728.65561887,  7307.98782321, 12213.78770965, 18893.24804015,\n       19995.95195136])\n\n\nLet’s take the first five predicted values and compare it to the actual targets.\n\nprint(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", y_test[0:4].values)\n\nPredicted values: [ 6728.65561887  7307.98782321 12213.78770965 18893.24804015]\nTrue values: [ 6295. 10698. 13860. 13499.]\n\n\nWe will use the function “PollyPlot” that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n\nPollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)\n\n\n\n\nFigure 3: A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction.\nWe see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points.\nR^2 of the training data:\n\npoly.score(x_train_pr, y_train)\n\n0.5567716902120265\n\n\nR^2 of the test data:\n\npoly.score(x_test_pr, y_test)\n\n-29.871340302044135\n\n\nWe see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87. The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting.\nLet’s see how the R^2 changes on the test data for different order polynomials and then plot the results:\n\nRsqu_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    \n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    \n    Rsqu_test.append(lr.score(x_test_pr, y_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')    \nplt.show()\nplt.close()\n\n\n\n\nWe see the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial.\nThe following interface allows you to experiment with different polynomial orders and different amounts of data.\n\ninteract(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))\n\n\n\n\n<function __main__.f(order, test_data)>"
  },
  {
    "objectID": "code/5_model_eval_and_refinement.html#ridge-regression",
    "href": "code/5_model_eval_and_refinement.html#ridge-regression",
    "title": "6  Model Evaluation and Refinement",
    "section": "6.4 Ridge regression",
    "text": "6.4 Ridge regression\nRidge regression is a regression that is employed in a Multiple regression model when Multicollinearity occurs. Multicollinearity is when there is a strong relationship among the independent variables. Ridge regression is very common with polynomial regression and is used to regularize and reduce the standard errors to avoid over-fitting a regression model.\nLet’s perform a degree two polynomial transformation on our data.\n\npr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n\n\n#load lib\nfrom sklearn.linear_model import Ridge\n\n#create a ridge regression object\nRigeModel=Ridge(alpha=1)\n\n#fit the model using the method fit.\nRigeModel.fit(x_train_pr, y_train)\n\n#obtain a prediction\nyhat = RigeModel.predict(x_test_pr)\n\n#Let's compare the first five predicted samples to our test set:\nprint('predicted:', yhat[0:4])\nprint('test set :', y_test[0:4].values)\n\npredicted: [ 6570.82441941  9636.24891471 20949.92322737 19403.60313255]\ntest set : [ 6295. 10698. 13860. 13499.]\n\n\nWe select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n\nfrom tqdm import tqdm\n\nRsqu_test = []\nRsqu_train = []\ndummy1 = []\nAlpha = 10 * np.array(range(0,1000))\npbar = tqdm(Alpha)\n\nfor alpha in pbar:\n    RigeModel = Ridge(alpha=alpha) \n    RigeModel.fit(x_train_pr, y_train)\n    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n    \n    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n\n    Rsqu_test.append(test_score)\n    Rsqu_train.append(train_score)\n\nWe can plot out the value of R^2 for different alphas:\n\nwidth = 5\nheight = 8\nplt.figure(figsize=(width, height))\n\nplt.plot(Alpha,Rsqu_test, label='validation data  ')\nplt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()\n\nplt.show()\nplt.close()\n\n\n\n\nFigure 4: The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha.\nHere the model is built and tested on the same data, so the training and test data are the same.\nThe red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data\nThe blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point."
  },
  {
    "objectID": "code/5_model_eval_and_refinement.html#grid-search",
    "href": "code/5_model_eval_and_refinement.html#grid-search",
    "title": "6  Model Evaluation and Refinement",
    "section": "6.5 Grid Search",
    "text": "6.5 Grid Search\nGrid Search allows us to scan through multiple free parameters with few lines of code.\nParameters like the alpha term in Ridge regression are not part of the fitting or training process. These values are called hyperparameters.\nScikit-learn has a means of automatically iterating over these hyperparameters using cross-validation. This method is called Grid Search. Grid Search takes the model or objects you would like to train and different values of the hyperparameters. It then calculates the mean square error or R-squared for various hyperparameter values, allowing you to choose the best values based on the set of parameters that minimize the error.\nTo select the hyperparameter, we split our dataset into three parts:\n\nThe training set in which we train the model for different hyperparameters.\nWe select the hyperparameter that minimizes the mean squared error or maximizes the R-squared on the validation set.\nFinally, we test our model set using the test set\n\n\n#load lib\nfrom sklearn.model_selection import GridSearchCV\n\n#create a dic of the parameter values\nparameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1\n\n[{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]\n\n\n\n#lCreate a Ridge regression object:\nRR=Ridge()\nRR\n\nRidge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n\n\n\n#Create a ridge grid search object:\nGrid1 = GridSearchCV(RR, parameters1,cv=4)\n\n\n#fit the model\nGrid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n\nGridSearchCV(cv=4, error_score=nan,\n             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n                             max_iter=None, normalize=False, random_state=None,\n                             solver='auto', tol=0.001),\n             iid='deprecated', n_jobs=None,\n             param_grid=[{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000,\n                                    100000]}],\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0)\n\n\nThe object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n\nBestRR=Grid1.best_estimator_\nBestRR\n\nRidge(alpha=10000, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n\n\nWe now test our model on the test data:\n\nBestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)\n\n0.8411649831036149"
  }
]